{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cafe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be8c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trino import dbapi\n",
    "from trino.auth import OAuth2Authentication\n",
    "\n",
    "conn = dbapi.connect(\n",
    "    host=\"smart-cities-trino.pre-prod.cloud.vtti.vt.edu\",\n",
    "    port=443,\n",
    "    http_scheme=\"https\",\n",
    "    auth=OAuth2Authentication(),   # <-- this is the right one\n",
    "    catalog=\"smartcities_iceberg\",  # optional default\n",
    "    # schema=\"...\",                # optional default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366f8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the following URL in browser for the external authentication:\n",
      "https://smart-cities-trino.pre-prod.cloud.vtti.vt.edu/oauth2/token/initiate/2c9e1c25729ab3c719827f2efe20f6c57f4c19bc83283feb7396f162b6374c16\n",
      "[['alexandria'], ['cci'], ['falls-church'], ['information_schema'], ['smart_cities_test'], ['system'], ['tables'], ['vtti']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SHOW SCHEMAS\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835a08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['smartcities_iceberg'], ['system']]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SHOW CATALOGS\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecbca743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alexandria'], ['cci'], ['falls-church'], ['information_schema'], ['smart_cities_test'], ['system'], ['tables'], ['vtti']]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SHOW SCHEMAS FROM smartcities_iceberg\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05016733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alexandria', 'bsm'], ['alexandria', 'psm'], ['alexandria', 'safety-event'], ['alexandria', 'speed-distribution'], ['alexandria', 'vehicle-count'], ['alexandria', 'vru-count'], ['cci', 'bsm'], ['falls-church', 'hiresdata'], ['falls-church', 'maple_washington'], ['falls-church', 'mediantraveltimes'], ['falls-church', 'old_hiresdata'], ['falls-church', 'old_mediantraveltimes'], ['falls-church', 'old_priority_requests'], ['falls-church', 'old_safety_conflicts'], ['falls-church', 'old_safety_pedcompliance'], ['falls-church', 'old_safety_redlightrunners'], ['falls-church', 'old_safety_simpledelay'], ['falls-church', 'old_tmc'], ['falls-church', 'old_tmc_crosswalk'], ['falls-church', 'old_tmc_lanes']]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT table_schema, table_name\n",
    "FROM smartcities_iceberg.information_schema.tables\n",
    "ORDER BY table_schema, table_name\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wye8zdjkqxq",
   "metadata": {},
   "source": [
    "## Best Practices for Querying\n",
    "\n",
    "### 1. Always Verify Column Names First\n",
    "\n",
    "**CRITICAL:** Column names may not be what you expect! Run the schema verification cells in this notebook to see actual field names.\n",
    "\n",
    "```python\n",
    "# See actual columns in a table\n",
    "cur.execute(\"SELECT * FROM smartcities_iceberg.alexandria.bsm LIMIT 1\")\n",
    "actual_columns = [desc[0] for desc in cur.description]\n",
    "print(actual_columns)\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "- ❌ `vehicle_id` may not exist\n",
    "- ❌ `latitude`, `longitude` → Use `lat`, `lon` instead\n",
    "- ✅ Always check the schema verification output first!\n",
    "\n",
    "### 2. Use Cursor Method (Not pd.read_sql)\n",
    "\n",
    "To avoid pandas SQLAlchemy warnings, use the cursor method:\n",
    "\n",
    "```python\n",
    "# Good: Use cursor method\n",
    "cur.execute(\"SELECT * FROM table LIMIT 5\")\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Avoid: pd.read_sql() triggers warning\n",
    "# df = pd.read_sql(\"SELECT * FROM table\", conn)\n",
    "```\n",
    "\n",
    "### 3. Start with SELECT \\*\n",
    "\n",
    "When exploring or unsure about column names, use `SELECT *` to get all columns:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    from_unixtime(publish_timestamp / 1000000) as timestamp,\n",
    "    *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "LIMIT 10\n",
    "```\n",
    "\n",
    "Then filter columns in pandas:\n",
    "\n",
    "```python\n",
    "df_filtered = df[['timestamp', 'lat', 'lon', 'speed']]\n",
    "```\n",
    "\n",
    "### 4. Handle publish_timestamp Correctly\n",
    "\n",
    "`publish_timestamp` is a **bigint** (microseconds since Unix epoch), NOT a timestamp type.\n",
    "\n",
    "**Convert to readable timestamp:**\n",
    "\n",
    "```sql\n",
    "from_unixtime(publish_timestamp / 1000000) as timestamp\n",
    "```\n",
    "\n",
    "**Filter by time:**\n",
    "\n",
    "```sql\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '24' hour) * 100000000\n",
    "```\n",
    "\n",
    "**Use in aggregations:**\n",
    "\n",
    "```sql\n",
    "date_trunc('hour', from_unixtime(publish_timestamp / 1000000))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "s0nvwsudvfk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tables found: 52\n",
      "\n",
      "           Schema                      Table\n",
      "       alexandria                        bsm\n",
      "       alexandria                        psm\n",
      "       alexandria               safety-event\n",
      "       alexandria         speed-distribution\n",
      "       alexandria              vehicle-count\n",
      "       alexandria                  vru-count\n",
      "              cci                        bsm\n",
      "     falls-church                  hiresdata\n",
      "     falls-church           maple_washington\n",
      "     falls-church          mediantraveltimes\n",
      "     falls-church              old_hiresdata\n",
      "     falls-church      old_mediantraveltimes\n",
      "     falls-church      old_priority_requests\n",
      "     falls-church       old_safety_conflicts\n",
      "     falls-church   old_safety_pedcompliance\n",
      "     falls-church old_safety_redlightrunners\n",
      "     falls-church     old_safety_simpledelay\n",
      "     falls-church                    old_tmc\n",
      "     falls-church          old_tmc_crosswalk\n",
      "     falls-church              old_tmc_lanes\n",
      "     falls-church           safety-conflicts\n",
      "     falls-church               safety-event\n",
      "     falls-church              safety-event \n",
      "     falls-church     safety-redlightrunners\n",
      "     falls-church         safety-simpledelay\n",
      "     falls-church         speed-distribution\n",
      "     falls-church        speed-distribution \n",
      "     falls-church                        tmc\n",
      "     falls-church              tmc-crosswalk\n",
      "     falls-church                  tmc-lanes\n",
      "     falls-church              vehicle-count\n",
      "     falls-church             vehicle-count \n",
      "     falls-church                  vru-count\n",
      "     falls-church                 vru-count \n",
      "smart_cities_test                   bsm_data\n",
      "smart_cities_test            bsm_data_struct\n",
      "smart_cities_test              bsm_wide_data\n",
      "smart_cities_test                   psm_data\n",
      "smart_cities_test                   rse_data\n",
      "smart_cities_test               safety-event\n",
      "smart_cities_test                  spat_data\n",
      "smart_cities_test           table_properties\n",
      "           tables                        bsm\n",
      "           tables                        psm\n",
      "             vtti                environment\n",
      "             vtti          environment-alarm\n",
      "             vtti                      light\n",
      "             vtti                light-alarm\n",
      "             vtti                      noise\n",
      "             vtti                noise-alarm\n",
      "             vtti                    traffic\n",
      "             vtti              traffic-alarm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get all tables from all schemas\n",
    "cur.execute(\"\"\"\n",
    "SELECT table_schema, table_name\n",
    "FROM smartcities_iceberg.information_schema.tables\n",
    "WHERE table_schema NOT IN ('information_schema', 'system')\n",
    "ORDER BY table_schema, table_name\n",
    "\"\"\")\n",
    "all_tables = cur.fetchall()\n",
    "\n",
    "tables_df = pd.DataFrame(all_tables, columns=['Schema', 'Table'])\n",
    "print(f\"Total tables found: {len(tables_df)}\\n\")\n",
    "print(tables_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "l1b64brjzm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - BSM Table\n",
      "================================================================================\n",
      "                      Column  Data Type\n",
      "0                       city    varchar\n",
      "1               intersection    varchar\n",
      "2                      table    varchar\n",
      "3          publish_timestamp     bigint\n",
      "4              location_name    varchar\n",
      "5                source_type    varchar\n",
      "6                vendor_name    varchar\n",
      "7             vendor_version    varchar\n",
      "8                       misc    varchar\n",
      "9                    msg_cnt    integer\n",
      "10                        id     bigint\n",
      "11                  sec_mark    integer\n",
      "12                       lat     double\n",
      "13                       lon     double\n",
      "14                      elev       real\n",
      "15       accuracy_semi_major       real\n",
      "16       accuracy_semi_minor       real\n",
      "17      accuracy_orientation       real\n",
      "18              transmission    integer\n",
      "19                     speed       real\n",
      "20                   heading       real\n",
      "21                     angle       real\n",
      "22                 accel_lon       real\n",
      "23                 accel_lat       real\n",
      "24                accel_vert       real\n",
      "25                 accel_yaw       real\n",
      "26      brake_applied_status    integer\n",
      "27   traction_control_status    integer\n",
      "28    anti_lock_brake_status    integer\n",
      "29  stability_control_status    integer\n",
      "30       brake_boost_applied    integer\n",
      "31    auxiliary_brake_status    integer\n",
      "32                size_width    integer\n",
      "33               size_length    integer\n",
      "34     original_content_type    varchar\n",
      "35          original_content  varbinary\n"
     ]
    }
   ],
   "source": [
    "# Function to examine table structure\n",
    "def describe_table(schema, table):\n",
    "    cur.execute(f\"\"\"\n",
    "    SELECT column_name, data_type\n",
    "    FROM smartcities_iceberg.information_schema.columns\n",
    "    WHERE table_schema = '{schema}' AND table_name = '{table}'\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    columns = cur.fetchall()\n",
    "    return pd.DataFrame(columns, columns=['Column', 'Data Type'])\n",
    "\n",
    "\n",
    "# Let's examine some key tables from each schema\n",
    "print(\"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - BSM Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'bsm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058ts384lm93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - PSM Table (Pedestrian Safety Message)\n",
      "================================================================================\n",
      "                  Column  Data Type\n",
      "0                   city    varchar\n",
      "1           intersection    varchar\n",
      "2                  table    varchar\n",
      "3      publish_timestamp     bigint\n",
      "4          location_name    varchar\n",
      "5            source_type    varchar\n",
      "6            vendor_name    varchar\n",
      "7         vendor_version    varchar\n",
      "8                   misc    varchar\n",
      "9                msg_cnt    integer\n",
      "10            basic_type    integer\n",
      "11                    id     bigint\n",
      "12              sec_mark    integer\n",
      "13                   lat     double\n",
      "14                   lon     double\n",
      "15                  elev       real\n",
      "16   accuracy_semi_major       real\n",
      "17   accuracy_semi_minor       real\n",
      "18  accuracy_orientation       real\n",
      "19                 speed       real\n",
      "20               heading       real\n",
      "21             accel_lon       real\n",
      "22             accel_lat       real\n",
      "23            accel_vert       real\n",
      "24             accel_yaw       real\n",
      "25  event_responder_type    integer\n",
      "26         activity_type    integer\n",
      "27     activity_sub_type    integer\n",
      "28      original_content  varbinary\n",
      "\n",
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - Safety Event Table\n",
      "================================================================================\n",
      "               Column Data Type\n",
      "0          event_type   varchar\n",
      "1            event_id   varchar\n",
      "2        time_at_site    bigint\n",
      "3      detection_area   varchar\n",
      "4           camera_id   varchar\n",
      "5           direction   varchar\n",
      "6            movement   varchar\n",
      "7       object1_class   varchar\n",
      "8       object2_class   varchar\n",
      "9                city   varchar\n",
      "10       intersection   varchar\n",
      "11              table   varchar\n",
      "12  publish_timestamp    bigint\n",
      "\n",
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - Vehicle Count Table\n",
      "================================================================================\n",
      "              Column Data Type\n",
      "0              count   integer\n",
      "1               date   varchar\n",
      "2      time_interval   varchar\n",
      "3           approach   varchar\n",
      "4              class   varchar\n",
      "5               city   varchar\n",
      "6       intersection   varchar\n",
      "7              table   varchar\n",
      "8  publish_timestamp    bigint\n",
      "9           movement   varchar\n",
      "\n",
      "================================================================================\n",
      "FALLS CHURCH SCHEMA - HiRes Data Table\n",
      "================================================================================\n",
      "               Column Data Type\n",
      "0                city   varchar\n",
      "1       data_provider   varchar\n",
      "2               table   varchar\n",
      "3     intersection_id   varchar\n",
      "4        intersection   varchar\n",
      "5   intersection_name   varchar\n",
      "6    intersection_lat    double\n",
      "7   intersection_long    double\n",
      "8   publish_timestamp    bigint\n",
      "9     event_timestamp    bigint\n",
      "10         event_code   integer\n",
      "11        event_param   integer\n",
      "12   original_content   varchar\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - PSM Table (Pedestrian Safety Message)\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'psm'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - Safety Event Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'safety-event'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - Vehicle Count Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'vehicle-count'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FALLS CHURCH SCHEMA - HiRes Data Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('falls-church', 'hiresdata'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0s688l7vb3bo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STORING SCHEMAS FOR KEY TABLES\n",
      "================================================================================\n",
      "\n",
      "Alexandria BSM Schema:\n",
      "                  Column Data Type\n",
      "                    city   varchar\n",
      "            intersection   varchar\n",
      "                   table   varchar\n",
      "       publish_timestamp    bigint\n",
      "           location_name   varchar\n",
      "             source_type   varchar\n",
      "             vendor_name   varchar\n",
      "          vendor_version   varchar\n",
      "                    misc   varchar\n",
      "                 msg_cnt   integer\n",
      "                      id    bigint\n",
      "                sec_mark   integer\n",
      "                     lat    double\n",
      "                     lon    double\n",
      "                    elev      real\n",
      "     accuracy_semi_major      real\n",
      "     accuracy_semi_minor      real\n",
      "    accuracy_orientation      real\n",
      "            transmission   integer\n",
      "                   speed      real\n",
      "                 heading      real\n",
      "                   angle      real\n",
      "               accel_lon      real\n",
      "               accel_lat      real\n",
      "              accel_vert      real\n",
      "               accel_yaw      real\n",
      "    brake_applied_status   integer\n",
      " traction_control_status   integer\n",
      "  anti_lock_brake_status   integer\n",
      "stability_control_status   integer\n",
      "     brake_boost_applied   integer\n",
      "  auxiliary_brake_status   integer\n",
      "              size_width   integer\n",
      "             size_length   integer\n",
      "   original_content_type   varchar\n",
      "        original_content varbinary\n",
      "\n",
      "\n",
      "Alexandria PSM Schema:\n",
      "              Column Data Type\n",
      "                city   varchar\n",
      "        intersection   varchar\n",
      "               table   varchar\n",
      "   publish_timestamp    bigint\n",
      "       location_name   varchar\n",
      "         source_type   varchar\n",
      "         vendor_name   varchar\n",
      "      vendor_version   varchar\n",
      "                misc   varchar\n",
      "             msg_cnt   integer\n",
      "          basic_type   integer\n",
      "                  id    bigint\n",
      "            sec_mark   integer\n",
      "                 lat    double\n",
      "                 lon    double\n",
      "                elev      real\n",
      " accuracy_semi_major      real\n",
      " accuracy_semi_minor      real\n",
      "accuracy_orientation      real\n",
      "               speed      real\n",
      "             heading      real\n",
      "           accel_lon      real\n",
      "           accel_lat      real\n",
      "          accel_vert      real\n",
      "           accel_yaw      real\n",
      "event_responder_type   integer\n",
      "       activity_type   integer\n",
      "   activity_sub_type   integer\n",
      "    original_content varbinary\n",
      "\n",
      "\n",
      "Alexandria Safety Event Schema:\n",
      "           Column Data Type\n",
      "       event_type   varchar\n",
      "         event_id   varchar\n",
      "     time_at_site    bigint\n",
      "   detection_area   varchar\n",
      "        camera_id   varchar\n",
      "        direction   varchar\n",
      "         movement   varchar\n",
      "    object1_class   varchar\n",
      "    object2_class   varchar\n",
      "             city   varchar\n",
      "     intersection   varchar\n",
      "            table   varchar\n",
      "publish_timestamp    bigint\n",
      "\n",
      "\n",
      "Alexandria Vehicle Count Schema:\n",
      "           Column Data Type\n",
      "            count   integer\n",
      "             date   varchar\n",
      "    time_interval   varchar\n",
      "         approach   varchar\n",
      "            class   varchar\n",
      "             city   varchar\n",
      "     intersection   varchar\n",
      "            table   varchar\n",
      "publish_timestamp    bigint\n",
      "         movement   varchar\n",
      "\n",
      "\n",
      "Falls Church HiRes Data Schema:\n",
      "           Column Data Type\n",
      "             city   varchar\n",
      "    data_provider   varchar\n",
      "            table   varchar\n",
      "  intersection_id   varchar\n",
      "     intersection   varchar\n",
      "intersection_name   varchar\n",
      " intersection_lat    double\n",
      "intersection_long    double\n",
      "publish_timestamp    bigint\n",
      "  event_timestamp    bigint\n",
      "       event_code   integer\n",
      "      event_param   integer\n",
      " original_content   varchar\n",
      "\n",
      "================================================================================\n",
      "Schemas stored! Use these column lists to ensure correct field names in queries.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Store schemas in variables for later use to ensure correct field names\n",
    "print(\"=\" * 80)\n",
    "print(\"STORING SCHEMAS FOR KEY TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get and store BSM schema\n",
    "bsm_schema = describe_table('alexandria', 'bsm')\n",
    "print(\"\\nAlexandria BSM Schema:\")\n",
    "print(bsm_schema.to_string(index=False))\n",
    "bsm_columns = bsm_schema['Column'].tolist()\n",
    "\n",
    "# Get and store PSM schema\n",
    "psm_schema = describe_table('alexandria', 'psm')\n",
    "print(\"\\n\\nAlexandria PSM Schema:\")\n",
    "print(psm_schema.to_string(index=False))\n",
    "psm_columns = psm_schema['Column'].tolist()\n",
    "\n",
    "# Get and store Safety Event schema\n",
    "safety_event_schema = describe_table('alexandria', 'safety-event')\n",
    "print(\"\\n\\nAlexandria Safety Event Schema:\")\n",
    "print(safety_event_schema.to_string(index=False))\n",
    "safety_event_columns = safety_event_schema['Column'].tolist()\n",
    "\n",
    "# Get and store Vehicle Count schema\n",
    "vehicle_count_schema = describe_table('alexandria', 'vehicle-count')\n",
    "print(\"\\n\\nAlexandria Vehicle Count Schema:\")\n",
    "print(vehicle_count_schema.to_string(index=False))\n",
    "vehicle_count_columns = vehicle_count_schema['Column'].tolist()\n",
    "\n",
    "# Get and store Falls Church HiRes Data schema\n",
    "hiresdata_schema = describe_table('falls-church', 'hiresdata')\n",
    "print(\"\\n\\nFalls Church HiRes Data Schema:\")\n",
    "print(hiresdata_schema.to_string(index=False))\n",
    "hiresdata_columns = hiresdata_schema['Column'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Schemas stored! Use these column lists to ensure correct field names in queries.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tkmtv24xpwd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Checking if columns exist in BSM table:\n",
      "WARNING: The following columns do not exist in alexandria.bsm:\n",
      "  Missing: ['vehicle_id']\n",
      "  Available columns: ['city', 'intersection', 'table', 'publish_timestamp', 'location_name', 'source_type', 'vendor_name', 'vendor_version', 'misc', 'msg_cnt', 'id', 'sec_mark', 'lat', 'lon', 'elev', 'accuracy_semi_major', 'accuracy_semi_minor', 'accuracy_orientation', 'transmission', 'speed', 'heading', 'angle', 'accel_lon', 'accel_lat', 'accel_vert', 'accel_yaw', 'brake_applied_status', 'traction_control_status', 'anti_lock_brake_status', 'stability_control_status', 'brake_boost_applied', 'auxiliary_brake_status', 'size_width', 'size_length', 'original_content_type', 'original_content']\n",
      "\n",
      "================================================================================\n",
      "COMMON FIELD NAME CORRECTIONS:\n",
      "================================================================================\n",
      "BSM Table:\n",
      "  ✗ latitude  → ✓ lat\n",
      "  ✗ longitude → ✓ lon\n",
      "\n",
      "Safety Event Table:\n",
      "  ✗ latitude  → ✓ lat\n",
      "  ✗ longitude → ✓ lon\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if columns exist in a table\n",
    "def check_columns(columns_to_check, available_columns, table_name):\n",
    "    \"\"\"\n",
    "    Verify that requested columns exist in the table schema\n",
    "\n",
    "    Args:\n",
    "        columns_to_check: List of column names you want to use\n",
    "        available_columns: List of actual columns from the schema\n",
    "        table_name: Name of the table (for error messages)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (all_valid: bool, missing_columns: list)\n",
    "    \"\"\"\n",
    "    missing = [col for col in columns_to_check if col not in available_columns]\n",
    "\n",
    "    if missing:\n",
    "        print(f\"WARNING: The following columns do not exist in {table_name}:\")\n",
    "        print(f\"  Missing: {missing}\")\n",
    "        print(f\"  Available columns: {available_columns}\")\n",
    "        return False, missing\n",
    "    else:\n",
    "        print(f\"✓ All columns exist in {table_name}\")\n",
    "        return True, []\n",
    "\n",
    "\n",
    "# Example: Verify BSM columns before using them\n",
    "print(\"Example: Checking if columns exist in BSM table:\")\n",
    "columns_i_want = ['publish_timestamp',\n",
    "                  'vehicle_id', 'lat', 'lon', 'speed', 'heading']\n",
    "check_columns(columns_i_want, bsm_columns, 'alexandria.bsm')\n",
    "\n",
    "# Show common column name corrections\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMMON FIELD NAME CORRECTIONS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"BSM Table:\")\n",
    "print(\"  ✗ latitude  → ✓ lat\")\n",
    "print(\"  ✗ longitude → ✓ lon\")\n",
    "print(\"\\nSafety Event Table:\")\n",
    "print(\"  ✗ latitude  → ✓ lat\")\n",
    "print(\"  ✗ longitude → ✓ lon\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t31t48pbqz",
   "metadata": {},
   "source": [
    "## Using Stored Schemas\n",
    "\n",
    "The schemas for key tables are now stored in variables:\n",
    "\n",
    "- `bsm_columns` - List of columns in alexandria.bsm\n",
    "- `psm_columns` - List of columns in alexandria.psm\n",
    "- `safety_event_columns` - List of columns in alexandria.safety-event\n",
    "- `vehicle_count_columns` - List of columns in alexandria.vehicle-count\n",
    "- `hiresdata_columns` - List of columns in falls-church.hiresdata\n",
    "\n",
    "**Always verify column names before writing queries** to avoid errors!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30hy3yg34xj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a query for BSM data:\n",
      "WARNING: The following columns do not exist in alexandria.bsm:\n",
      "  Missing: ['vehicle_id']\n",
      "  Available columns: ['city', 'intersection', 'table', 'publish_timestamp', 'location_name', 'source_type', 'vendor_name', 'vendor_version', 'misc', 'msg_cnt', 'id', 'sec_mark', 'lat', 'lon', 'elev', 'accuracy_semi_major', 'accuracy_semi_minor', 'accuracy_orientation', 'transmission', 'speed', 'heading', 'angle', 'accel_lon', 'accel_lat', 'accel_vert', 'accel_yaw', 'brake_applied_status', 'traction_control_status', 'anti_lock_brake_status', 'stability_control_status', 'brake_boost_applied', 'auxiliary_brake_status', 'size_width', 'size_length', 'original_content_type', 'original_content']\n",
      "Cannot build query - missing columns: ['vehicle_id']\n"
     ]
    }
   ],
   "source": [
    "# Example: Dynamically build a query using stored schema\n",
    "def build_select_query(table_schema, table_name, columns_to_select, available_columns, limit=10):\n",
    "    \"\"\"\n",
    "    Build a SELECT query dynamically after verifying columns exist\n",
    "\n",
    "    Args:\n",
    "        table_schema: Schema name (e.g., 'alexandria')\n",
    "        table_name: Table name (e.g., 'bsm')\n",
    "        columns_to_select: List of columns to select\n",
    "        available_columns: List of available columns from schema\n",
    "        limit: Number of rows to return\n",
    "\n",
    "    Returns:\n",
    "        str: SQL query string or None if columns are invalid\n",
    "    \"\"\"\n",
    "    # Verify all columns exist\n",
    "    is_valid, missing = check_columns(\n",
    "        columns_to_select, available_columns, f\"{table_schema}.{table_name}\")\n",
    "\n",
    "    if not is_valid:\n",
    "        print(f\"Cannot build query - missing columns: {missing}\")\n",
    "        return None\n",
    "\n",
    "    # Build column list\n",
    "    column_str = \", \".join(columns_to_select)\n",
    "\n",
    "    # Build query\n",
    "    query = f\"\"\"\n",
    "    SELECT {column_str}\n",
    "    FROM smartcities_iceberg.{table_schema}.\"{table_name}\"\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "# Example: Build a safe BSM query\n",
    "print(\"Building a query for BSM data:\")\n",
    "columns_needed = ['publish_timestamp', 'vehicle_id', 'lat', 'lon', 'speed']\n",
    "query = build_select_query(\n",
    "    'alexandria', 'bsm', columns_needed, bsm_columns, limit=5)\n",
    "\n",
    "if query:\n",
    "    print(\"\\nGenerated Query:\")\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aolrfk0z9xu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFYING ACTUAL COLUMN NAMES IN BSM TABLE\n",
      "================================================================================\n",
      "\n",
      "Actual columns in BSM table (36 total):\n",
      "   1. city\n",
      "   2. intersection\n",
      "   3. table\n",
      "   4. publish_timestamp\n",
      "   5. location_name\n",
      "   6. source_type\n",
      "   7. vendor_name\n",
      "   8. vendor_version\n",
      "   9. misc\n",
      "  10. msg_cnt\n",
      "  11. id\n",
      "  12. sec_mark\n",
      "  13. lat\n",
      "  14. lon\n",
      "  15. elev\n",
      "  16. accuracy_semi_major\n",
      "  17. accuracy_semi_minor\n",
      "  18. accuracy_orientation\n",
      "  19. transmission\n",
      "  20. speed\n",
      "  21. heading\n",
      "  22. angle\n",
      "  23. accel_lon\n",
      "  24. accel_lat\n",
      "  25. accel_vert\n",
      "  26. accel_yaw\n",
      "  27. brake_applied_status\n",
      "  28. traction_control_status\n",
      "  29. anti_lock_brake_status\n",
      "  30. stability_control_status\n",
      "  31. brake_boost_applied\n",
      "  32. auxiliary_brake_status\n",
      "  33. size_width\n",
      "  34. size_length\n",
      "  35. original_content_type\n",
      "  36. original_content\n",
      "\n",
      "Columns stored from schema query: 36 total\n",
      "✓ Schema matches actual columns\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the ACTUAL column names are by querying sample data\n",
    "print(\"=\" * 80)\n",
    "print(\"VERIFYING ACTUAL COLUMN NAMES IN BSM TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a sample row to see actual column names\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# Get actual column names from the query result\n",
    "actual_bsm_columns = [desc[0] for desc in cur.description]\n",
    "print(f\"\\nActual columns in BSM table ({len(actual_bsm_columns)} total):\")\n",
    "for i, col in enumerate(actual_bsm_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Store for comparison\n",
    "print(f\"\\nColumns stored from schema query: {len(bsm_columns)} total\")\n",
    "if set(actual_bsm_columns) != set(bsm_columns):\n",
    "    print(\"WARNING: Mismatch between schema query and actual columns!\")\n",
    "else:\n",
    "    print(\"✓ Schema matches actual columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9qoqv9q8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING ACTUAL COLUMN NAMES IN SAFETY-EVENT TABLE\n",
      "================================================================================\n",
      "\n",
      "Actual columns in Safety-Event table (13 total):\n",
      "   1. event_type\n",
      "   2. event_id\n",
      "   3. time_at_site\n",
      "   4. detection_area\n",
      "   5. camera_id\n",
      "   6. direction\n",
      "   7. movement\n",
      "   8. object1_class\n",
      "   9. object2_class\n",
      "  10. city\n",
      "  11. intersection\n",
      "  12. table\n",
      "  13. publish_timestamp\n",
      "\n",
      "================================================================================\n",
      "STORE THESE VERIFIED COLUMNS FOR USE IN QUERIES\n",
      "================================================================================\n",
      "\n",
      "bsm_columns_verified = ['city', 'intersection', 'table', 'publish_timestamp', 'location_name', 'source_type', 'vendor_name', 'vendor_version', 'misc', 'msg_cnt', 'id', 'sec_mark', 'lat', 'lon', 'elev', 'accuracy_semi_major', 'accuracy_semi_minor', 'accuracy_orientation', 'transmission', 'speed', 'heading', 'angle', 'accel_lon', 'accel_lat', 'accel_vert', 'accel_yaw', 'brake_applied_status', 'traction_control_status', 'anti_lock_brake_status', 'stability_control_status', 'brake_boost_applied', 'auxiliary_brake_status', 'size_width', 'size_length', 'original_content_type', 'original_content']\n",
      "\n",
      "safety_event_columns_verified = ['event_type', 'event_id', 'time_at_site', 'detection_area', 'camera_id', 'direction', 'movement', 'object1_class', 'object2_class', 'city', 'intersection', 'table', 'publish_timestamp']\n"
     ]
    }
   ],
   "source": [
    "# Check Safety Event columns too\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFYING ACTUAL COLUMN NAMES IN SAFETY-EVENT TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.\"safety-event\"\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "actual_safety_columns = [desc[0] for desc in cur.description]\n",
    "print(\n",
    "    f\"\\nActual columns in Safety-Event table ({len(actual_safety_columns)} total):\")\n",
    "for i, col in enumerate(actual_safety_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STORE THESE VERIFIED COLUMNS FOR USE IN QUERIES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nbsm_columns_verified = {actual_bsm_columns}\")\n",
    "print(f\"\\nsafety_event_columns_verified = {actual_safety_columns}\")\n",
    "\n",
    "# Update the stored columns with verified ones\n",
    "bsm_columns = actual_bsm_columns\n",
    "safety_event_columns = actual_safety_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csnfcywl3x",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Workflow for Using This Notebook\n",
    "\n",
    "**Before running the example queries below, you MUST:**\n",
    "\n",
    "1. **Run ALL cells in order** from the beginning of the notebook\n",
    "2. **Pay special attention to the schema verification cells** (above) which show you the ACTUAL column names\n",
    "3. **Note the actual column names** displayed in the verification output\n",
    "4. **Update queries** to use the correct column names based on what you see\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "- ❌ Assuming field names like `vehicle_id`, `latitude`, `longitude`\n",
    "- ✅ Use the ACTUAL field names shown in the verification cells above\n",
    "- ❌ Using `timestamp` instead of `publish_timestamp` for filtering\n",
    "- ✅ `publish_timestamp` is a **bigint** (microseconds) - use conversion functions\n",
    "\n",
    "### Safe Approach:\n",
    "\n",
    "All example queries below now use `SELECT *` to get ALL columns. After seeing what's available, you can filter to specific columns in your DataFrame:\n",
    "\n",
    "```python\n",
    "# After running a query with SELECT *\n",
    "df_filtered = df[['column1', 'column2', 'column3']]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "argzlf026c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA AVAILABILITY AND TIME RANGES\n",
      "================================================================================\n",
      "\n",
      "Checking alexandria.bsm...\n",
      "  Error: Could not convert '+57641-08-14 10:06:55.000 America/New_York' into the associated python type\n",
      "\n",
      "Checking alexandria.psm...\n",
      "  Error: Could not convert '+57644-02-14 14:15:42.000 America/New_York' into the associated python type\n",
      "\n",
      "Checking alexandria.safety-event...\n",
      "  Error: Could not convert '+57527-01-15 15:25:49.000 America/New_York' into the associated python type\n",
      "\n",
      "Checking alexandria.vehicle-count...\n",
      "  Error: Could not convert '+57527-02-26 06:51:24.000 America/New_York' into the associated python type\n",
      "\n",
      "Checking falls-church.hiresdata...\n",
      "  Error: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 6:39: mismatched input '-'. Expecting: ',', '.', 'AS', 'CROSS', 'EXCEPT', 'FETCH', 'FOR', 'FULL', 'GROUP', 'HAVING', 'INNER', 'INTERSECT', 'JOIN', 'LEFT', 'LIMIT', 'MATCH_RECOGNIZE', 'NATURAL', 'OFFSET', 'ORDER', 'RIGHT', 'TABLESAMPLE', 'UNION', 'WHERE', 'WINDOW', <EOF>, <identifier>\", query_id=20251111_173415_00042_tsjew)\n"
     ]
    }
   ],
   "source": [
    "# Function to check data availability and time range\n",
    "def check_data_range(schema, table, timestamp_col='publish_timestamp'):\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as record_count,\n",
    "            from_unixtime(MIN({timestamp_col}) / 1000) as earliest_record,\n",
    "            from_unixtime(MAX({timestamp_col}) / 1000) as latest_record\n",
    "        FROM smartcities_iceberg.{schema}.\"{table}\"\n",
    "        \"\"\"\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "        return {\n",
    "            'Schema': schema,\n",
    "            'Table': table,\n",
    "            'Record Count': result[0] if result else 0,\n",
    "            'Earliest': result[1] if result else None,\n",
    "            'Latest': result[2] if result else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'Schema': schema,\n",
    "            'Table': table,\n",
    "            'Error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Check data ranges for key tables\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA AVAILABILITY AND TIME RANGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ranges = []\n",
    "tables_to_check = [\n",
    "    ('alexandria', 'bsm'),\n",
    "    ('alexandria', 'psm'),\n",
    "    ('alexandria', 'safety-event'),\n",
    "    ('alexandria', 'vehicle-count'),\n",
    "    ('falls-church', 'hiresdata'),\n",
    "]\n",
    "\n",
    "for schema, table in tables_to_check:\n",
    "    print(f\"\\nChecking {schema}.{table}...\")\n",
    "    range_info = check_data_range(schema, table)\n",
    "    ranges.append(range_info)\n",
    "    if 'Error' not in range_info:\n",
    "        print(f\"  Records: {range_info['Record Count']:,}\")\n",
    "        print(\n",
    "            f\"  Time range: {range_info['Earliest']} to {range_info['Latest']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {range_info['Error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blqomcv0go",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE DATA: Alexandria BSM (Basic Safety Message)\n",
      "================================================================================\n",
      "         city   intersection           table  publish_timestamp  \\\n",
      "0  alexandria  glebe-potomac  alexandria.bsm   1762879458287000   \n",
      "1  alexandria  glebe-potomac  alexandria.bsm   1762879458283000   \n",
      "2  alexandria  glebe-potomac  alexandria.bsm   1762879458287000   \n",
      "3  alexandria  glebe-potomac  alexandria.bsm   1762879458287000   \n",
      "4  alexandria  glebe-potomac  alexandria.bsm   1762879458283000   \n",
      "\n",
      "     location_name source_type vendor_name vendor_version  misc  msg_cnt  ...  \\\n",
      "0  Glebe & Potomac        ittf        derq           rev1  V2.2       97  ...   \n",
      "1  Glebe & Potomac        ittf        derq           rev1  V2.2       45  ...   \n",
      "2  Glebe & Potomac        ittf        derq           rev1  V2.2       82  ...   \n",
      "3  Glebe & Potomac        ittf        derq           rev1  V2.2       81  ...   \n",
      "4  Glebe & Potomac        ittf        derq           rev1  V2.2       61  ...   \n",
      "\n",
      "   brake_applied_status  traction_control_status  anti_lock_brake_status  \\\n",
      "0                     0                     None                    None   \n",
      "1                     0                     None                    None   \n",
      "2                     0                     None                    None   \n",
      "3                     0                     None                    None   \n",
      "4                     0                     None                    None   \n",
      "\n",
      "   stability_control_status  brake_boost_applied  auxiliary_brake_status  \\\n",
      "0                      None                 None                    None   \n",
      "1                      None                 None                    None   \n",
      "2                      None                 None                    None   \n",
      "3                      None                 None                    None   \n",
      "4                      None                 None                    None   \n",
      "\n",
      "   size_width  size_length original_content_type  \\\n",
      "0         190          450      application/json   \n",
      "1         190          450      application/json   \n",
      "2         190          450      application/json   \n",
      "3         190          450      application/json   \n",
      "4         190          450      application/json   \n",
      "\n",
      "                                    original_content  \n",
      "0  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "1  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "2  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "3  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "4  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get sample data from BSM table (using cursor to avoid pandas warning)\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE DATA: Alexandria BSM (Basic Safety Message)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_bsm = pd.DataFrame(results, columns=columns)\n",
    "print(df_bsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "p6jyxrlg2x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE DATA: Alexandria Safety Events\n",
      "================================================================================\n",
      "  event_type                  event_id      time_at_site       detection_area  \\\n",
      "0        LCV  6910d6e6b53ea10012dac887  1762693270494000            South Leg   \n",
      "1         IC  6910dcc386da330012d41285  1762694771847000  Intersection Center   \n",
      "2     NM-VRU  6910e2894f51570013a8f6dc  1762696246031000            South Leg   \n",
      "3         IC  69110db164697100125b2daa  1762707297102000  Intersection Center   \n",
      "4         IC  6911024ae03ddb00119e6a23  1762704377490000  Intersection Center   \n",
      "\n",
      "  camera_id direction movement      object1_class object2_class        city  \\\n",
      "0         0      None     None               None          None  alexandria   \n",
      "1         0      None     None               None          None  alexandria   \n",
      "2         0      None     None  Passenger Vehicle    Pedestrian  alexandria   \n",
      "3         0      None     None               None          None  alexandria   \n",
      "4         0      None     None               None          None  alexandria   \n",
      "\n",
      "    intersection                    table  publish_timestamp  \n",
      "0  glebe-potomac  alexandria.safety-event   1762714807465000  \n",
      "1  glebe-potomac  alexandria.safety-event   1762714807465000  \n",
      "2  glebe-potomac  alexandria.safety-event   1762714807465000  \n",
      "3  glebe-potomac  alexandria.safety-event   1762725605146000  \n",
      "4  glebe-potomac  alexandria.safety-event   1762725605146000  \n"
     ]
    }
   ],
   "source": [
    "# Get sample data from Safety Event table (using cursor to avoid pandas warning)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA: Alexandria Safety Events\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.\"safety-event\"\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_safety = pd.DataFrame(results, columns=columns)\n",
    "print(df_safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "v68sy8dltf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE DATA: Falls Church - Median Travel Times\n",
      "================================================================================\n",
      "           city data_provider                           table  \\\n",
      "0  falls-church     MioVision  falls-church.mediantraveltimes   \n",
      "1  falls-church     MioVision  falls-church.mediantraveltimes   \n",
      "2  falls-church     MioVision  falls-church.mediantraveltimes   \n",
      "3  falls-church     MioVision  falls-church.mediantraveltimes   \n",
      "4  falls-church     MioVision  falls-church.mediantraveltimes   \n",
      "\n",
      "                    src_intersection_id  src_intersection  \\\n",
      "0  0f45e46c-efc1-498a-91c3-f61cfad78ea8  broad-washington   \n",
      "1  0f45e46c-efc1-498a-91c3-f61cfad78ea8  broad-washington   \n",
      "2  0f45e46c-efc1-498a-91c3-f61cfad78ea8  broad-washington   \n",
      "3  0f45e46c-efc1-498a-91c3-f61cfad78ea8  broad-washington   \n",
      "4  0f45e46c-efc1-498a-91c3-f61cfad78ea8  broad-washington   \n",
      "\n",
      "                           src_intersection_name  src_intersection_lat  \\\n",
      "0  East Broad Street and North Washington Street             38.882233   \n",
      "1  East Broad Street and North Washington Street             38.882233   \n",
      "2  East Broad Street and North Washington Street             38.882233   \n",
      "3  East Broad Street and North Washington Street             38.882233   \n",
      "4  East Broad Street and North Washington Street             38.882233   \n",
      "\n",
      "   src_intersection_long                  dest_intersection_id  \\\n",
      "0             -77.171086  1909c589-3cfa-4e8b-b759-792c667baa96   \n",
      "1             -77.171086  1909c589-3cfa-4e8b-b759-792c667baa96   \n",
      "2             -77.171086  1909c589-3cfa-4e8b-b759-792c667baa96   \n",
      "3             -77.171086  1909c589-3cfa-4e8b-b759-792c667baa96   \n",
      "4             -77.171086  1909c589-3cfa-4e8b-b759-792c667baa96   \n",
      "\n",
      "  dest_intersection              dest_intersection_name  \\\n",
      "0       birch-broad  Birch Street and West Broad Street   \n",
      "1       birch-broad  Birch Street and West Broad Street   \n",
      "2       birch-broad  Birch Street and West Broad Street   \n",
      "3       birch-broad  Birch Street and West Broad Street   \n",
      "4       birch-broad  Birch Street and West Broad Street   \n",
      "\n",
      "   dest_intersection_lat  dest_intersection_long  publish_timestamp  \\\n",
      "0              38.893546              -77.188643   1762444037234000   \n",
      "1              38.893546              -77.188643   1762444037235000   \n",
      "2              38.893546              -77.188643   1762444037235000   \n",
      "3              38.893546              -77.188643   1762444037235000   \n",
      "4              38.893546              -77.188643   1762444037235000   \n",
      "\n",
      "   travel_start_time   travel_end_time  travel_median data_confidence  \\\n",
      "0   1760792743511000  1760792803511000              0            HIGH   \n",
      "1   1760792923511000  1760792983511000              0            HIGH   \n",
      "2   1760792983511000  1760793043511000              0            HIGH   \n",
      "3   1760792803511000  1760792863511000              0            HIGH   \n",
      "4   1760792863511000  1760792923511000              0            HIGH   \n",
      "\n",
      "                                    original_content  \n",
      "0  {startTime=2025-10-18T13:05:43.511Z, endTime=2...  \n",
      "1  {startTime=2025-10-18T13:08:43.511Z, endTime=2...  \n",
      "2  {startTime=2025-10-18T13:09:43.511Z, endTime=2...  \n",
      "3  {startTime=2025-10-18T13:06:43.511Z, endTime=2...  \n",
      "4  {startTime=2025-10-18T13:07:43.511Z, endTime=2...  \n"
     ]
    }
   ],
   "source": [
    "# Get sample data from Falls Church median travel times (using cursor to avoid pandas warning)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA: Falls Church - Median Travel Times\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.\"falls-church\".mediantraveltimes\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_travel = pd.DataFrame(results, columns=columns)\n",
    "print(df_travel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xopxgvk0zdc",
   "metadata": {},
   "source": [
    "## API Summary\n",
    "\n",
    "This smart-cities API provides access to connected vehicle and traffic management data from multiple cities:\n",
    "\n",
    "### Available Data Types:\n",
    "\n",
    "1. **BSM (Basic Safety Message)**: Real-time vehicle position, speed, heading from connected vehicles\n",
    "2. **PSM (Pedestrian Safety Message)**: Pedestrian detection and safety data\n",
    "3. **Safety Events**: Detected safety conflicts and incidents\n",
    "4. **Vehicle/VRU Counts**: Traffic volume data\n",
    "5. **Speed Distribution**: Speed profiles across locations\n",
    "6. **High-Resolution Traffic Data**: Detailed signal performance metrics\n",
    "7. **Travel Times**: Corridor travel time measurements\n",
    "\n",
    "### Geographic Coverage:\n",
    "\n",
    "- Alexandria, VA\n",
    "- Falls Church, VA\n",
    "- CCI (Center for Connected Infrastructure)\n",
    "- VTTI (Virginia Tech Transportation Institute)\n",
    "\n",
    "### Data Collection Approaches:\n",
    "\n",
    "Run the cells below to see examples of collecting data over time periods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q8l685650w",
   "metadata": {},
   "source": [
    "## Example 1: Collecting BSM Data Over a Time Period\n",
    "\n",
    "This example shows how to collect vehicle trajectory data (BSM) for a specific time range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "otvff2r5z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 BSM records from the last 24 hours\n",
      "\n",
      "Columns available: ['timestamp', 'city', 'intersection', 'table', 'publish_timestamp', 'location_name', 'source_type', 'vendor_name', 'vendor_version', 'misc', 'msg_cnt', 'id', 'sec_mark', 'lat', 'lon', 'elev', 'accuracy_semi_major', 'accuracy_semi_minor', 'accuracy_orientation', 'transmission', 'speed', 'heading', 'angle', 'accel_lon', 'accel_lat', 'accel_vert', 'accel_yaw', 'brake_applied_status', 'traction_control_status', 'anti_lock_brake_status', 'stability_control_status', 'brake_boost_applied', 'auxiliary_brake_status', 'size_width', 'size_length', 'original_content_type', 'original_content']\n",
      "\n",
      "First few rows:\n",
      "                  timestamp        city   intersection           table  \\\n",
      "0 2025-11-11 12:44:31-05:00  alexandria  glebe-potomac  alexandria.bsm   \n",
      "1 2025-11-11 12:44:31-05:00  alexandria  glebe-potomac  alexandria.bsm   \n",
      "2 2025-11-11 12:44:31-05:00  alexandria  glebe-potomac  alexandria.bsm   \n",
      "3 2025-11-11 12:44:31-05:00  alexandria  glebe-potomac  alexandria.bsm   \n",
      "4 2025-11-11 12:44:31-05:00  alexandria  glebe-potomac  alexandria.bsm   \n",
      "\n",
      "   publish_timestamp    location_name source_type vendor_name vendor_version  \\\n",
      "0   1762883071654000  Glebe & Potomac        ittf        derq           rev1   \n",
      "1   1762883071654000  Glebe & Potomac        ittf        derq           rev1   \n",
      "2   1762883071654000  Glebe & Potomac        ittf        derq           rev1   \n",
      "3   1762883071654000  Glebe & Potomac        ittf        derq           rev1   \n",
      "4   1762883071654000  Glebe & Potomac        ittf        derq           rev1   \n",
      "\n",
      "   misc  ...  brake_applied_status  traction_control_status  \\\n",
      "0  V2.2  ...                     0                     None   \n",
      "1  V2.2  ...                     0                     None   \n",
      "2  V2.2  ...                     0                     None   \n",
      "3  V2.2  ...                     0                     None   \n",
      "4  V2.2  ...                     0                     None   \n",
      "\n",
      "   anti_lock_brake_status  stability_control_status  brake_boost_applied  \\\n",
      "0                    None                      None                 None   \n",
      "1                    None                      None                 None   \n",
      "2                    None                      None                 None   \n",
      "3                    None                      None                 None   \n",
      "4                    None                      None                 None   \n",
      "\n",
      "   auxiliary_brake_status  size_width  size_length  original_content_type  \\\n",
      "0                    None         190          450       application/json   \n",
      "1                    None         190          450       application/json   \n",
      "2                    None         190          450       application/json   \n",
      "3                    None         190          450       application/json   \n",
      "4                    None         190          450       application/json   \n",
      "\n",
      "                                    original_content  \n",
      "0  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "1  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "2  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "3  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "4  b'{sourceType=ittf, locationName=Glebe & Potom...  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Collect BSM data for the last 24 hours\n",
    "# Using SELECT * to get all columns - customize after verifying column names above\n",
    "# Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT \n",
    "    from_unixtime(publish_timestamp / 1000000) as timestamp,\n",
    "    *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '24' hour) * 1000000\n",
    "ORDER BY publish_timestamp DESC\n",
    "LIMIT 1000\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_bsm_daily = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Collected {len(df_bsm_daily)} BSM records from the last 24 hours\")\n",
    "print(f\"\\nColumns available: {list(df_bsm_daily.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_bsm_daily.head())\n",
    "\n",
    "# After seeing the columns, you can select specific ones like this:\n",
    "# columns_i_want = ['timestamp', 'publish_timestamp', 'lat', 'lon', 'speed', 'heading']\n",
    "# df_bsm_daily_filtered = df_bsm_daily[columns_i_want]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vgetkelhpbi",
   "metadata": {},
   "source": [
    "## Example 2: Collecting Safety Events Over a Date Range\n",
    "\n",
    "This example shows how to collect safety event data between specific dates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ytyxhosfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 safety events\n",
      "\n",
      "Columns available: ['timestamp', 'event_type', 'event_id', 'time_at_site', 'detection_area', 'camera_id', 'direction', 'movement', 'object1_class', 'object2_class', 'city', 'intersection', 'table', 'publish_timestamp']\n"
     ]
    }
   ],
   "source": [
    "# Collect safety events for a specific date range\n",
    "# Using SELECT * to get all columns - customize after verifying column names above\n",
    "# Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT \n",
    "    from_unixtime(publish_timestamp / 1000000) as timestamp,\n",
    "    *\n",
    "FROM smartcities_iceberg.alexandria.\"safety-event\"\n",
    "WHERE publish_timestamp BETWEEN \n",
    "    to_unixtime(timestamp '2024-01-01 00:00:00') * 1000000 AND \n",
    "    to_unixtime(timestamp '2024-12-31 23:59:59') * 1000000\n",
    "ORDER BY publish_timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_safety_events = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Collected {len(df_safety_events)} safety events\")\n",
    "print(f\"\\nColumns available: {list(df_safety_events.columns)}\")\n",
    "\n",
    "if len(df_safety_events) > 0:\n",
    "    # Check if 'event_type' column exists before using it\n",
    "    if 'event_type' in df_safety_events.columns:\n",
    "        print(\"\\nEvent type breakdown:\")\n",
    "        print(df_safety_events['event_type'].value_counts())\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_safety_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kojtnd5jhkj",
   "metadata": {},
   "source": [
    "## Example 3: Aggregated Traffic Data by Hour\n",
    "\n",
    "This example shows how to aggregate traffic metrics over time periods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "xhonfeub11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 169 hourly aggregates\n",
      "\n",
      "Columns: ['hour', 'vehicle_count', 'avg_speed', 'max_speed', 'min_speed']\n",
      "                       hour  vehicle_count  avg_speed  max_speed  min_speed\n",
      "0 2025-11-11 12:00:00-05:00         125273        NaN        NaN        NaN\n",
      "1 2025-11-11 11:00:00-05:00         153049        NaN        NaN        NaN\n",
      "2 2025-11-11 10:00:00-05:00         151151        NaN        NaN        NaN\n",
      "3 2025-11-11 09:00:00-05:00         154407        NaN        NaN        NaN\n",
      "4 2025-11-11 08:00:00-05:00         125744        NaN        NaN        NaN\n",
      "5 2025-11-11 07:00:00-05:00          93338        NaN        NaN        NaN\n",
      "6 2025-11-11 06:00:00-05:00          58090        NaN        NaN        NaN\n",
      "7 2025-11-11 05:00:00-05:00          22832        NaN        NaN        NaN\n",
      "8 2025-11-11 04:00:00-05:00          10789        NaN        NaN        NaN\n",
      "9 2025-11-11 03:00:00-05:00           4330        NaN        NaN        NaN\n",
      "\n",
      "Install matplotlib to visualize the data: %pip install matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Aggregate vehicle counts by hour for the last 7 days\n",
    "# Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "# Note: Adjust aggregation fields based on actual columns available\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', from_unixtime(publish_timestamp / 1000000)) as hour,\n",
    "    COUNT(*) as vehicle_count,\n",
    "    AVG(speed) as avg_speed,\n",
    "    MAX(speed) as max_speed,\n",
    "    MIN(speed) as min_speed\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '7' day) * 1000000\n",
    "GROUP BY date_trunc('hour', from_unixtime(publish_timestamp / 1000000))\n",
    "ORDER BY hour DESC\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_hourly_traffic = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Collected {len(df_hourly_traffic)} hourly aggregates\")\n",
    "print(f\"\\nColumns: {list(df_hourly_traffic.columns)}\")\n",
    "print(df_hourly_traffic.head(10))\n",
    "\n",
    "# Plot if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if len(df_hourly_traffic) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "        ax1.plot(df_hourly_traffic['hour'], df_hourly_traffic['vehicle_count'])\n",
    "        ax1.set_ylabel('Vehicle Count')\n",
    "        ax1.set_title('Hourly Vehicle Counts - Last 7 Days')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(df_hourly_traffic['hour'], df_hourly_traffic['avg_speed'])\n",
    "        ax2.set_ylabel('Average Speed')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_title('Average Speed by Hour')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except ImportError:\n",
    "    print(\"\\nInstall matplotlib to visualize the data: %pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810as7jw2",
   "metadata": {},
   "source": [
    "## Example 4: Batch Collection Over Multiple Days\n",
    "\n",
    "This example shows how to collect data in batches over a longer time period:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cwtlmu2957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Collecting safety events in daily batches...\n",
      "================================================================================\n",
      "Collecting data for 2024-01-01...\n",
      "  Found 0 records\n",
      "Collecting data for 2024-01-02...\n",
      "  Found 0 records\n",
      "Collecting data for 2024-01-03...\n",
      "  Found 0 records\n",
      "Collecting data for 2024-01-04...\n",
      "  Found 0 records\n",
      "Collecting data for 2024-01-05...\n",
      "  Found 0 records\n",
      "Collecting data for 2024-01-06...\n",
      "  Found 0 records\n",
      "Collecting data for 2024-01-07...\n",
      "  Found 0 records\n",
      "\n",
      "No data found in this date range\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to collect data in daily batches (using cursor to avoid pandas warning)\n",
    "\n",
    "\n",
    "def collect_data_batches(start_date, end_date, table_schema, table_name, columns_to_select='*'):\n",
    "    \"\"\"\n",
    "    Collect data in daily batches to avoid memory issues with large datasets\n",
    "\n",
    "    Args:\n",
    "        start_date: Start date (string or datetime)\n",
    "        end_date: End date (string or datetime)\n",
    "        table_schema: Database schema name\n",
    "        table_name: Table name\n",
    "        columns_to_select: Columns to select (default '*' for all columns)\n",
    "\n",
    "    Returns:\n",
    "        List of dataframes, one per day\n",
    "    \"\"\"\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    if isinstance(end_date, str):\n",
    "        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    batches = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        next_date = current_date + timedelta(days=1)\n",
    "\n",
    "        # Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "        # Using SELECT * to get all columns, or specify columns_to_select\n",
    "        if columns_to_select == '*':\n",
    "            select_clause = \"from_unixtime(publish_timestamp / 1000000) as timestamp, *\"\n",
    "        else:\n",
    "            select_clause = f\"from_unixtime(publish_timestamp / 1000000) as timestamp, {columns_to_select}\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT {select_clause}\n",
    "        FROM smartcities_iceberg.{table_schema}.\"{table_name}\"\n",
    "        WHERE publish_timestamp >= to_unixtime(timestamp '{current_date.strftime('%Y-%m-%d 00:00:00')}') * 100000000\n",
    "          AND publish_timestamp < to_unixtime(timestamp '{next_date.strftime('%Y-%m-%d 00:00:00')}') * 100000000\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Collecting data for {current_date.strftime('%Y-%m-%d')}...\")\n",
    "\n",
    "        # Use cursor instead of pd.read_sql to avoid pandas warning\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df_batch = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        print(f\"  Found {len(df_batch)} records\")\n",
    "\n",
    "        if len(df_batch) > 0:\n",
    "            batches.append(df_batch)\n",
    "\n",
    "        current_date = next_date\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "# Example: Collect safety events for a week in daily batches\n",
    "print(\"=\" * 80)\n",
    "print(\"Collecting safety events in daily batches...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Note: Using SELECT * to get all columns - adjust dates based on actual data availability\n",
    "batches = collect_data_batches(\n",
    "    start_date='2024-01-01',\n",
    "    end_date='2024-01-07',\n",
    "    table_schema='alexandria',\n",
    "    table_name='safety-event',\n",
    "    columns_to_select='*'  # Get all columns\n",
    ")\n",
    "\n",
    "if batches:\n",
    "    # Combine all batches\n",
    "    df_all = pd.concat(batches, ignore_index=True)\n",
    "    print(f\"\\nTotal records collected: {len(df_all)}\")\n",
    "    print(f\"\\nColumns available: {list(df_all.columns)}\")\n",
    "    print(\n",
    "        f\"\\nDate range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_all.head())\n",
    "else:\n",
    "    print(\"\\nNo data found in this date range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1mjqsfi2sge",
   "metadata": {},
   "source": [
    "## Tips for Data Collection\n",
    "\n",
    "### STEP 1: Always Verify Column Names First!\n",
    "\n",
    "**Run the schema verification cells at the beginning of this notebook to see actual column names.**\n",
    "\n",
    "Don't assume field names! Common mistakes:\n",
    "\n",
    "- ❌ `vehicle_id` - may not exist\n",
    "- ❌ `latitude`, `longitude` - actual names are `lat`, `lon`\n",
    "- ✅ Check the verification output to see real column names\n",
    "\n",
    "### STEP 2: Understand the Data Types\n",
    "\n",
    "**publish_timestamp is a BIGINT (Unix epoch in milliseconds)**\n",
    "\n",
    "Since `publish_timestamp` is stored as a bigint (microseconds since Unix epoch), you need to convert it:\n",
    "\n",
    "**For display/conversion to readable timestamp:**\n",
    "\n",
    "```sql\n",
    "from_unixtime(publish_timestamp / 1000000) as timestamp\n",
    "```\n",
    "\n",
    "**For time-based filtering:**\n",
    "\n",
    "```sql\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '24' hour) * 100000000\n",
    "```\n",
    "\n",
    "**For date range filtering:**\n",
    "\n",
    "```sql\n",
    "WHERE publish_timestamp BETWEEN\n",
    "    to_unixtime(timestamp '2024-01-01 00:00:00') * 100000000 AND\n",
    "    to_unixtime(timestamp '2024-12-31 23:59:59') * 1000000\n",
    "```\n",
    "\n",
    "**For aggregation:**\n",
    "\n",
    "```sql\n",
    "date_trunc('hour', from_unixtime(publish_timestamp / 1000000))\n",
    "```\n",
    "\n",
    "### STEP 3: Start with SELECT \\*\n",
    "\n",
    "When exploring a new table, always start with `SELECT *` to see all available columns:\n",
    "\n",
    "```python\n",
    "cur.execute(\"SELECT * FROM smartcities_iceberg.alexandria.bsm LIMIT 5\")\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "print(df.columns.tolist())  # See what columns are actually available\n",
    "```\n",
    "\n",
    "Then filter in pandas after seeing the data:\n",
    "\n",
    "```python\n",
    "df_filtered = df[['column1', 'column2', 'column3']]\n",
    "```\n",
    "\n",
    "### Performance Tips:\n",
    "\n",
    "1. **Verify column names first** - Run schema verification cells\n",
    "2. **Start with SELECT \\*** - See what's available before selecting specific columns\n",
    "3. **Add LIMIT**: Always test queries with LIMIT first to check structure\n",
    "4. **Use WHERE clauses**: Filter by publish_timestamp to reduce data volume\n",
    "5. **Aggregate when possible**: Use GROUP BY and aggregation functions (COUNT, AVG, etc.)\n",
    "6. **Batch large requests**: Use the batch collection function for multi-day/multi-week requests\n",
    "\n",
    "### Common Time Intervals:\n",
    "\n",
    "- Last hour: `to_unixtime(current_timestamp - interval '1' hour) * 1000000`\n",
    "- Last 24 hours: `to_unixtime(current_timestamp - interval '24' hour) * 100000000`\n",
    "- Last week: `to_unixtime(current_timestamp - interval '7' day) * 1000000`\n",
    "- Last month: `to_unixtime(current_timestamp - interval '30' day) * 1000000`\n",
    "\n",
    "### Key Tables for Traffic Safety Analysis:\n",
    "\n",
    "- **alexandria.bsm**: Vehicle trajectories (position, speed, acceleration)\n",
    "- **alexandria.safety-event**: Detected safety conflicts\n",
    "- **alexandria.vehicle-count**: Traffic volumes\n",
    "- **alexandria.speed-distribution**: Speed profiles\n",
    "- **falls-church.hiresdata**: High-resolution signal data\n",
    "- **falls-church.mediantraveltimes**: Travel time measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9sjupzoy53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1.2: INTERSECTION DATA OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "Error analyzing intersection coverage: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 5:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175216_00059_tsjew)\n",
      "Proceeding with available data...\n"
     ]
    }
   ],
   "source": [
    "# Phase 1.2: Identify intersections with sufficient data overlap\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1.2: INTERSECTION DATA OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find intersections that have data in all key tables\n",
    "query_intersections = \"\"\"\n",
    "WITH bsm_intersections AS (\n",
    "    SELECT DISTINCT intersection\n",
    "    FROM alexandria.bsm\n",
    "    WHERE publish_timestamp > 0 AND publish_timestamp < 9999999999999999999\n",
    "    LIMIT 100\n",
    "),\n",
    "psm_intersections AS (\n",
    "    SELECT DISTINCT intersection  \n",
    "    FROM alexandria.psm\n",
    "    WHERE publish_timestamp > 0 AND publish_timestamp < 9999999999999999999\n",
    "    LIMIT 100\n",
    "),\n",
    "event_intersections AS (\n",
    "    SELECT DISTINCT intersection\n",
    "    FROM alexandria.\"safety-event\"\n",
    "    WHERE time_at_site > 0 AND time_at_site < 9999999999999999999\n",
    "    LIMIT 100\n",
    "),\n",
    "count_intersections AS (\n",
    "    SELECT DISTINCT intersection\n",
    "    FROM alexandria.\"vehicle-count\"\n",
    "    WHERE publish_timestamp > 0 AND publish_timestamp < 9999999999999999999\n",
    "    LIMIT 100\n",
    ")\n",
    "SELECT \n",
    "    b.intersection,\n",
    "    CASE WHEN p.intersection IS NOT NULL THEN 'Y' ELSE 'N' END as has_psm,\n",
    "    CASE WHEN e.intersection IS NOT NULL THEN 'Y' ELSE 'N' END as has_events,\n",
    "    CASE WHEN c.intersection IS NOT NULL THEN 'Y' ELSE 'N' END as has_counts\n",
    "FROM bsm_intersections b\n",
    "LEFT JOIN psm_intersections p ON b.intersection = p.intersection\n",
    "LEFT JOIN event_intersections e ON b.intersection = e.intersection\n",
    "LEFT JOIN count_intersections c ON b.intersection = c.intersection\n",
    "ORDER BY b.intersection\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cur.execute(query_intersections)\n",
    "    columns = [desc[0] for desc in cur.description]\n",
    "    results = cur.fetchall()\n",
    "    intersection_coverage = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    print(f\"\\nFound {len(intersection_coverage)} intersections with BSM data\")\n",
    "    print(\"\\nData availability by intersection:\")\n",
    "    print(intersection_coverage.to_string(index=False))\n",
    "\n",
    "    # Find intersections with complete data\n",
    "    complete_data = intersection_coverage[\n",
    "        (intersection_coverage['has_psm'] == 'Y') &\n",
    "        (intersection_coverage['has_events'] == 'Y') &\n",
    "        (intersection_coverage['has_counts'] == 'Y')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\n",
    "        f\"Intersections with COMPLETE data (BSM + PSM + Events + Counts): {len(complete_data)}\")\n",
    "    if len(complete_data) > 0:\n",
    "        print(\"Recommended intersections for analysis:\")\n",
    "        print(complete_data['intersection'].tolist())\n",
    "    else:\n",
    "        print(\"⚠ WARNING: No intersections have complete data across all tables\")\n",
    "        print(\"Consider using intersections with partial data and handling missing values\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing intersection coverage: {e}\")\n",
    "    print(\"Proceeding with available data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "tj7oe82pp6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2.1: SEVERITY-WEIGHTED BASELINE CONSTRUCTION\n",
      "================================================================================\n",
      "\n",
      "Collecting events from 2025-10-12 to 2025-11-11...\n",
      "Error collecting baseline events: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 14:47: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00060_tsjew)\n",
      "⚠ No events found in the specified date range\n",
      "Try adjusting the date range or checking data availability\n"
     ]
    }
   ],
   "source": [
    "# Phase 2.1: Collect severity-weighted safety events for baseline\n",
    "from datetime import datetime, timedelta\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2.1: SEVERITY-WEIGHTED BASELINE CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Severity weights from checkpoint document\n",
    "SEVERITY_WEIGHTS = {\n",
    "    'FATAL': 10,\n",
    "    'INJURY': 3,\n",
    "    'PDO': 1,  # Property Damage Only\n",
    "    'DEFAULT': 1  # For events without explicit severity\n",
    "}\n",
    "\n",
    "# Define function that will eventually go in FastAPI backend\n",
    "\n",
    "\n",
    "def collect_baseline_events(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Collect and weight safety events for baseline risk calculation.\n",
    "    This will become an API backend function.\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period (datetime or timestamp)\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with weighted events by intersection, TOD, DOW\n",
    "    \"\"\"\n",
    "\n",
    "    # Build query with optional filters\n",
    "    where_clauses = [\"time_at_site > 0\", \"time_at_site < 9999999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        # Convert to milliseconds timestamp if needed\n",
    "        where_clauses.append(\n",
    "            f\"time_at_site >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"time_at_site <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        event_type,\n",
    "        event_id,\n",
    "        from_unixtime(time_at_site / 1000000) as event_time,\n",
    "        time_at_site,\n",
    "        HOUR(from_unixtime(time_at_site / 1000000)) as hour_of_day,\n",
    "        DAY_OF_WEEK(from_unixtime(time_at_site / 1000000)) as day_of_week,\n",
    "        object1_class,\n",
    "        object2_class,\n",
    "        detection_area\n",
    "    FROM alexandria.\"safety-event\"\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, time_at_site\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        if len(df) > 0:\n",
    "            # Apply severity weighting\n",
    "            # Note: We'll need to map event types to severity levels\n",
    "            # For now, use a simple heuristic\n",
    "            def assign_severity_weight(row):\n",
    "                event_type = str(row['event_type']).upper()\n",
    "\n",
    "                # VRU-involved events get higher weight\n",
    "                is_vru = ('PEDESTRIAN' in str(row['object1_class']).upper() or\n",
    "                          'CYCLIST' in str(row['object1_class']).upper() or\n",
    "                          'PEDESTRIAN' in str(row['object2_class']).upper() or\n",
    "                          'CYCLIST' in str(row['object2_class']).upper())\n",
    "\n",
    "                if 'FATAL' in event_type or 'DEATH' in event_type:\n",
    "                    return SEVERITY_WEIGHTS['FATAL']\n",
    "                elif 'INJURY' in event_type or is_vru:\n",
    "                    return SEVERITY_WEIGHTS['INJURY']\n",
    "                else:\n",
    "                    return SEVERITY_WEIGHTS['PDO']\n",
    "\n",
    "            df['severity_weight'] = df.apply(assign_severity_weight, axis=1)\n",
    "            df['is_vru_involved'] = df.apply(\n",
    "                lambda row: 1 if ('PEDESTRIAN' in str(row['object1_class']).upper() or\n",
    "                                  'CYCLIST' in str(row['object1_class']).upper() or\n",
    "                                  'PEDESTRIAN' in str(row['object2_class']).upper() or\n",
    "                                  'CYCLIST' in str(row['object2_class']).upper()) else 0,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting baseline events: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Test the function - collect last 30 days of data\n",
    "\n",
    "end_dt = datetime.now()\n",
    "start_dt = end_dt - timedelta(days=30)\n",
    "\n",
    "print(f\"\\nCollecting events from {start_dt.date()} to {end_dt.date()}...\")\n",
    "baseline_events = collect_baseline_events(start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(baseline_events) > 0:\n",
    "    print(f\"\\n✓ Collected {len(baseline_events)} events\")\n",
    "    print(f\"\\nEvent type distribution:\")\n",
    "    print(baseline_events['event_type'].value_counts())\n",
    "    print(f\"\\nVRU-involved events: {baseline_events['is_vru_involved'].sum()}\")\n",
    "    print(f\"\\nSeverity weight distribution:\")\n",
    "    print(baseline_events['severity_weight'].value_counts().sort_index())\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(baseline_events[['intersection', 'event_time',\n",
    "          'event_type', 'severity_weight', 'is_vru_involved']].head(10))\n",
    "else:\n",
    "    print(\"⚠ No events found in the specified date range\")\n",
    "    print(\"Try adjusting the date range or checking data availability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "foavg978bkb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2.2: EXPOSURE METRICS CONSTRUCTION\n",
      "================================================================================\n",
      "\n",
      "Collecting exposure metrics from 2025-10-12 to 2025-11-11...\n",
      "Error collecting exposure metrics: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 13:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00061_tsjew)\n",
      "\n",
      "⚠ No vehicle count data found\n",
      "\n",
      "⚠ No VRU count data found\n"
     ]
    }
   ],
   "source": [
    "# Phase 2.2: Build exposure metrics from vehicle and VRU count data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2.2: EXPOSURE METRICS CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def collect_exposure_metrics(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Collect vehicle and VRU counts for exposure normalization.\n",
    "    This will become an API backend function.\n",
    "\n",
    "    Returns:\n",
    "        Two DataFrames: vehicle_counts, vru_counts\n",
    "    \"\"\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_clauses = [\"publish_timestamp > 0\",\n",
    "                     \"publish_timestamp < 9999999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    # Collect vehicle counts\n",
    "    vehicle_query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        approach,\n",
    "        movement,\n",
    "        class as vehicle_class,\n",
    "        count as vehicle_count,\n",
    "        HOUR(from_unixtime(publish_timestamp / 1000000)) as hour_of_day,\n",
    "        DAY_OF_WEEK(from_unixtime(publish_timestamp / 1000000)) as day_of_week\n",
    "    FROM alexandria.\"vehicle-count\"\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect VRU counts\n",
    "    vru_query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        approach,\n",
    "        count as vru_count,\n",
    "        HOUR(from_unixtime(publish_timestamp / 1000000)) as hour_of_day,\n",
    "        DAY_OF_WEEK(from_unixtime(publish_timestamp / 1000000)) as day_of_week\n",
    "    FROM alexandria.\"vru-count\"\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Get vehicle counts\n",
    "        cur.execute(vehicle_query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        vehicle_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        # Get VRU counts\n",
    "        cur.execute(vru_query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        vru_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        return vehicle_df, vru_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting exposure metrics: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\n",
    "    f\"\\nCollecting exposure metrics from {start_dt.date()} to {end_dt.date()}...\")\n",
    "vehicle_counts, vru_counts = collect_exposure_metrics(\n",
    "    start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    print(f\"\\n✓ Vehicle Counts: {len(vehicle_counts)} records\")\n",
    "    print(f\"  Intersections: {vehicle_counts['intersection'].nunique()}\")\n",
    "    print(f\"  Total vehicles: {vehicle_counts['vehicle_count'].sum():,}\")\n",
    "    print(f\"\\n  Sample vehicle counts:\")\n",
    "    print(vehicle_counts[['intersection', 'time',\n",
    "          'approach', 'movement', 'vehicle_count']].head(5))\n",
    "else:\n",
    "    print(\"\\n⚠ No vehicle count data found\")\n",
    "\n",
    "if len(vru_counts) > 0:\n",
    "    print(f\"\\n✓ VRU Counts: {len(vru_counts)} records\")\n",
    "    print(f\"  Intersections: {vru_counts['intersection'].nunique()}\")\n",
    "    print(f\"  Total VRUs: {vru_counts['vru_count'].sum():,}\")\n",
    "    print(f\"\\n  Sample VRU counts:\")\n",
    "    print(vru_counts[['intersection', 'time',\n",
    "          'approach', 'vru_count']].head(5))\n",
    "else:\n",
    "    print(\"\\n⚠ No VRU count data found\")\n",
    "\n",
    "# Calculate normalization constants (V_max, N_VRU_max) for index formulas\n",
    "if len(vehicle_counts) > 0 and len(vru_counts) > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NORMALIZATION CONSTANTS FOR INDEX COMPUTATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Aggregate to 15-minute intervals to match index granularity\n",
    "    vehicle_counts['time_15min'] = pd.to_datetime(\n",
    "        vehicle_counts['time']).dt.floor('15min')\n",
    "    vru_counts['time_15min'] = pd.to_datetime(\n",
    "        vru_counts['time']).dt.floor('15min')\n",
    "\n",
    "    vehicle_15min = vehicle_counts.groupby(['intersection', 'time_15min'])[\n",
    "        'vehicle_count'].sum().reset_index()\n",
    "    vru_15min = vru_counts.groupby(['intersection', 'time_15min'])[\n",
    "        'vru_count'].sum().reset_index()\n",
    "\n",
    "    V_max = vehicle_15min['vehicle_count'].max()\n",
    "    N_VRU_max = vru_15min['vru_count'].max()\n",
    "\n",
    "    print(f\"\\nV_max (max vehicle volume per 15-min): {V_max}\")\n",
    "    print(f\"N_VRU_max (max VRU count per 15-min): {N_VRU_max}\")\n",
    "    print(f\"\\nThese constants will be used to normalize the Safety Index formulas.\")\n",
    "\n",
    "    # Store for later use\n",
    "    NORMALIZATION_CONSTANTS = {\n",
    "        'V_max': V_max,\n",
    "        'N_VRU_max': N_VRU_max\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "734le8n83k8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3.1: BSM-DERIVED VEHICLE FEATURES\n",
      "================================================================================\n",
      "\n",
      "Collecting BSM features from 2025-10-12 to 2025-11-11...\n",
      "Error collecting BSM features: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 15:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00062_tsjew)\n",
      "⚠ No BSM features generated - check data availability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\djjay\\AppData\\Local\\Temp\\ipykernel_161612\\2768779151.py\", line 64, in collect_bsm_features\n",
      "    cur.execute(query)\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\dbapi.py\", line 640, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 909, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "                         ^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 929, in fetch\n",
      "    status = self._request.process(response)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 698, in process\n",
      "    raise self._process_error(response[\"error\"], response.get(\"id\"))\n",
      "trino.exceptions.TrinoUserError: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 15:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00062_tsjew)\n"
     ]
    }
   ],
   "source": [
    "# Phase 3.1: BSM-derived vehicle features (15-minute aggregates)\n",
    "import numpy as np\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3.1: BSM-DERIVED VEHICLE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def collect_bsm_features(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Extract vehicle behavior features from BSM data aggregated to 15-minute intervals.\n",
    "\n",
    "    Features computed:\n",
    "    - Vehicle count (exposure metric V)\n",
    "    - Average speed (S)\n",
    "    - Speed variance (σ_S) \n",
    "    - Hard braking frequency\n",
    "    - Heading change rate (proxy for weaving/erratic behavior)\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 15-minute aggregated features\n",
    "    \"\"\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_clauses = [\"publish_timestamp > 0\",\n",
    "                     \"publish_timestamp < 9999999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    # Query BSM data with relevant fields\n",
    "    # Note: Using verified column names (lat, lon, id, speed, heading, brake_applied_status)\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        id as vehicle_id,\n",
    "        lat,\n",
    "        lon,\n",
    "        speed,\n",
    "        heading,\n",
    "        brake_applied_status,\n",
    "        accel_lon,\n",
    "        accel_lat\n",
    "    FROM alexandria.bsm\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(\"⚠ No BSM data found for specified criteria\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"✓ Retrieved {len(df):,} BSM records\")\n",
    "\n",
    "        # Convert time to datetime and create 15-minute bins\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df['time_15min'] = df['time'].dt.floor('15min')\n",
    "\n",
    "        # Identify hard braking events (brake_applied_status & 0x04 for hard braking)\n",
    "        # Brake status is a bitmask: bit 2 (0x04) indicates hard braking\n",
    "        df['hard_braking'] = df['brake_applied_status'].apply(\n",
    "            lambda x: 1 if pd.notna(x) and (int(x) & 0x04) else 0\n",
    "        )\n",
    "\n",
    "        # Calculate features by 15-minute interval and intersection\n",
    "        print(\"Computing aggregated features...\")\n",
    "\n",
    "        # Group by intersection and 15-minute interval\n",
    "        grouped = df.groupby(['intersection', 'time_15min'])\n",
    "\n",
    "        features = grouped.agg({\n",
    "            'vehicle_id': 'nunique',  # Unique vehicle count\n",
    "            'speed': ['mean', 'std'],  # Average speed and speed variance\n",
    "            'hard_braking': 'sum',  # Count of hard braking events\n",
    "            # Heading variance (weaving proxy)\n",
    "            'heading': lambda x: calculate_heading_change_rate(x),\n",
    "            'accel_lon': 'std',  # Longitudinal acceleration variance\n",
    "            'accel_lat': 'std'  # Lateral acceleration variance\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        features.columns = [\n",
    "            'intersection',\n",
    "            'time_15min',\n",
    "            'vehicle_count',\n",
    "            'avg_speed',\n",
    "            'speed_variance',\n",
    "            'hard_braking_count',\n",
    "            'heading_change_rate',\n",
    "            'accel_lon_variance',\n",
    "            'accel_lat_variance'\n",
    "        ]\n",
    "\n",
    "        # Add time-of-day and day-of-week for contextual analysis\n",
    "        features['hour_of_day'] = features['time_15min'].dt.hour\n",
    "        features['day_of_week'] = features['time_15min'].dt.dayofweek\n",
    "\n",
    "        print(f\"✓ Generated {len(features)} 15-minute feature records\")\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting BSM features: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def calculate_heading_change_rate(heading_series):\n",
    "    \"\"\"\n",
    "    Calculate rate of heading changes as a proxy for weaving behavior.\n",
    "    Returns standard deviation of heading changes.\n",
    "    \"\"\"\n",
    "    if len(heading_series) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate absolute differences in heading\n",
    "    # Note: Need to handle circular nature of heading (0-360 degrees)\n",
    "    headings = heading_series.dropna().values\n",
    "    if len(headings) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate angular differences\n",
    "    diffs = []\n",
    "    for i in range(1, len(headings)):\n",
    "        diff = abs(headings[i] - headings[i-1])\n",
    "        # Handle wrap-around (e.g., 359° to 1° is a 2° change, not 358°)\n",
    "        if diff > 180:\n",
    "            diff = 360 - diff\n",
    "        diffs.append(diff)\n",
    "\n",
    "    return np.std(diffs) if len(diffs) > 0 else 0.0\n",
    "\n",
    "\n",
    "# Test the function\n",
    "\n",
    "print(\n",
    "    f\"\\nCollecting BSM features from {start_dt.date()} to {end_dt.date()}...\")\n",
    "bsm_features = collect_bsm_features(start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(bsm_features) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BSM FEATURE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total 15-minute intervals: {len(bsm_features)}\")\n",
    "    print(f\"Intersections covered: {bsm_features['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"Date range: {bsm_features['time_15min'].min()} to {bsm_features['time_15min'].max()}\")\n",
    "\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(bsm_features[['vehicle_count', 'avg_speed', 'speed_variance',\n",
    "          'hard_braking_count', 'heading_change_rate']].describe())\n",
    "\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(bsm_features[['intersection', 'time_15min', 'vehicle_count', 'avg_speed',\n",
    "          'speed_variance', 'hard_braking_count']].head(10).to_string(index=False))\n",
    "\n",
    "    # Store normalization constants\n",
    "    if 'NORMALIZATION_CONSTANTS' not in dir():\n",
    "        NORMALIZATION_CONSTANTS = {}\n",
    "\n",
    "    NORMALIZATION_CONSTANTS['speed_ref'] = bsm_features['avg_speed'].quantile(\n",
    "        0.85)  # 85th percentile speed\n",
    "    NORMALIZATION_CONSTANTS['speed_variance_max'] = bsm_features['speed_variance'].max(\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"UPDATED NORMALIZATION CONSTANTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"S_ref (reference speed, 85th percentile): {NORMALIZATION_CONSTANTS['speed_ref']:.2f} m/s\")\n",
    "    print(\n",
    "        f\"σ_max (max speed variance): {NORMALIZATION_CONSTANTS['speed_variance_max']:.2f}\")\n",
    "else:\n",
    "    print(\"⚠ No BSM features generated - check data availability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "zdewx17ivq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3.2: PSM-DERIVED VRU FEATURES\n",
      "================================================================================\n",
      "\n",
      "Collecting PSM features from 2025-10-12 to 2025-11-11...\n",
      "Error collecting PSM features: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 16:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00063_tsjew)\n",
      "⚠ No PSM features generated\n",
      "   PSM data may be limited or unavailable for this time period\n",
      "   The Safety Index can still be computed with reduced VRU component accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\djjay\\AppData\\Local\\Temp\\ipykernel_161612\\3809972926.py\", line 62, in collect_psm_features\n",
      "    cur.execute(query)\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\dbapi.py\", line 640, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 909, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "                         ^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 929, in fetch\n",
      "    status = self._request.process(response)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 698, in process\n",
      "    raise self._process_error(response[\"error\"], response.get(\"id\"))\n",
      "trino.exceptions.TrinoUserError: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 16:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00063_tsjew)\n"
     ]
    }
   ],
   "source": [
    "# Phase 3.2: PSM-derived VRU features (15-minute aggregates)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3.2: PSM-DERIVED VRU FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def collect_psm_features(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Extract VRU (Vulnerable Road User) behavior features from PSM data.\n",
    "\n",
    "    Features computed:\n",
    "    - VRU count (pedestrians, cyclists)\n",
    "    - Average VRU speed\n",
    "    - VRU activity type distribution\n",
    "    - Emergency responder presence\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 15-minute aggregated VRU features\n",
    "    \"\"\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_clauses = [\"publish_timestamp > 0\",\n",
    "                     \"publish_timestamp < 9999999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    # Query PSM data with relevant fields\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        id as vru_id,\n",
    "        lat,\n",
    "        lon,\n",
    "        speed,\n",
    "        heading,\n",
    "        basic_type,\n",
    "        activity_type,\n",
    "        activity_sub_type,\n",
    "        event_responder_type\n",
    "    FROM alexandria.psm\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(\"⚠ No PSM data found for specified criteria\")\n",
    "            print(\"   Note: PSM data may be sparse due to limited VRU device adoption\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"✓ Retrieved {len(df):,} PSM records\")\n",
    "\n",
    "        # Convert time to datetime and create 15-minute bins\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df['time_15min'] = df['time'].dt.floor('15min')\n",
    "\n",
    "        # Classify VRU types based on basic_type field\n",
    "        # PSM basic_type values (from J2735 standard):\n",
    "        # 0 = unknown, 1 = pedestrian, 2 = cyclist, 3 = wheelchair, 4 = animal\n",
    "        df['is_pedestrian'] = df['basic_type'].apply(\n",
    "            lambda x: 1 if x == 1 else 0)\n",
    "        df['is_cyclist'] = df['basic_type'].apply(lambda x: 1 if x == 2 else 0)\n",
    "        df['is_emergency_responder'] = df['event_responder_type'].apply(\n",
    "            lambda x: 1 if pd.notna(x) and x > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Calculate features by 15-minute interval and intersection\n",
    "        print(\"Computing aggregated VRU features...\")\n",
    "\n",
    "        grouped = df.groupby(['intersection', 'time_15min'])\n",
    "\n",
    "        features = grouped.agg({\n",
    "            'vru_id': 'nunique',  # Unique VRU count\n",
    "            'speed': 'mean',  # Average VRU speed\n",
    "            'is_pedestrian': 'sum',  # Pedestrian count\n",
    "            'is_cyclist': 'sum',  # Cyclist count\n",
    "            'is_emergency_responder': 'sum'  # Emergency responder count\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        features.columns = [\n",
    "            'intersection',\n",
    "            'time_15min',\n",
    "            'vru_count',\n",
    "            'avg_vru_speed',\n",
    "            'pedestrian_count',\n",
    "            'cyclist_count',\n",
    "            'emergency_responder_count'\n",
    "        ]\n",
    "\n",
    "        # Add time context\n",
    "        features['hour_of_day'] = features['time_15min'].dt.hour\n",
    "        features['day_of_week'] = features['time_15min'].dt.dayofweek\n",
    "\n",
    "        print(f\"✓ Generated {len(features)} 15-minute VRU feature records\")\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting PSM features: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\n",
    "    f\"\\nCollecting PSM features from {start_dt.date()} to {end_dt.date()}...\")\n",
    "psm_features = collect_psm_features(start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(psm_features) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PSM FEATURE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total 15-minute intervals: {len(psm_features)}\")\n",
    "    print(f\"Intersections covered: {psm_features['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"Date range: {psm_features['time_15min'].min()} to {psm_features['time_15min'].max()}\")\n",
    "\n",
    "    print(f\"\\nVRU statistics:\")\n",
    "    print(f\"  Total VRU observations: {psm_features['vru_count'].sum():.0f}\")\n",
    "    print(f\"  Pedestrians: {psm_features['pedestrian_count'].sum():.0f}\")\n",
    "    print(f\"  Cyclists: {psm_features['cyclist_count'].sum():.0f}\")\n",
    "    print(\n",
    "        f\"  Emergency responders: {psm_features['emergency_responder_count'].sum():.0f}\")\n",
    "\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(psm_features[['vru_count', 'avg_vru_speed',\n",
    "          'pedestrian_count', 'cyclist_count']].describe())\n",
    "\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(psm_features[['intersection', 'time_15min', 'vru_count',\n",
    "          'pedestrian_count', 'cyclist_count']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"⚠ No PSM features generated\")\n",
    "    print(\"   PSM data may be limited or unavailable for this time period\")\n",
    "    print(\"   The Safety Index can still be computed with reduced VRU component accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "zdxbtsvnfg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3.3: SAFETY EVENT AGGREGATION\n",
      "================================================================================\n",
      "\n",
      "Aggregating safety events from 2025-10-12 to 2025-11-11...\n",
      "Error collecting baseline events: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 14:47: Invalid numeric literal: 9999999999999999999\", query_id=20251111_175217_00064_tsjew)\n",
      "⚠ No safety events to aggregate\n",
      "⚠ No aggregated events generated\n"
     ]
    }
   ],
   "source": [
    "# Phase 3.3: Aggregate safety events by 15-minute intervals\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3.3: SAFETY EVENT AGGREGATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def aggregate_safety_events(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Aggregate safety events to 15-minute intervals with severity weighting.\n",
    "\n",
    "    Features computed:\n",
    "    - Total event count\n",
    "    - VRU-involved event count (I_VRU)\n",
    "    - Vehicle-only event count\n",
    "    - Severity-weighted event score\n",
    "    - Event type distribution\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 15-minute aggregated safety events\n",
    "    \"\"\"\n",
    "\n",
    "    # Reuse the baseline_events collection function but aggregate differently\n",
    "    events_df = collect_baseline_events(intersection, start_date, end_date)\n",
    "\n",
    "    if len(events_df) == 0:\n",
    "        print(\"⚠ No safety events to aggregate\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"✓ Retrieved {len(events_df)} safety events\")\n",
    "    print(\"Aggregating to 15-minute intervals...\")\n",
    "\n",
    "    # Convert event_time to datetime and create 15-minute bins\n",
    "    events_df['event_time'] = pd.to_datetime(events_df['event_time'])\n",
    "    events_df['time_15min'] = events_df['event_time'].dt.floor('15min')\n",
    "\n",
    "    # Group by intersection and 15-minute interval\n",
    "    grouped = events_df.groupby(['intersection', 'time_15min'])\n",
    "\n",
    "    aggregated = grouped.agg({\n",
    "        'event_id': 'count',  # Total event count\n",
    "        'is_vru_involved': 'sum',  # VRU-involved events (I_VRU)\n",
    "        'severity_weight': 'sum',  # Severity-weighted score\n",
    "        # Most common event type\n",
    "        'event_type': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    }).reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    aggregated.columns = [\n",
    "        'intersection',\n",
    "        'time_15min',\n",
    "        'total_event_count',\n",
    "        'vru_event_count',\n",
    "        'severity_weighted_score',\n",
    "        'dominant_event_type'\n",
    "    ]\n",
    "\n",
    "    # Calculate vehicle-only events\n",
    "    aggregated['vehicle_event_count'] = aggregated['total_event_count'] - \\\n",
    "        aggregated['vru_event_count']\n",
    "\n",
    "    # Add time context\n",
    "    aggregated['hour_of_day'] = aggregated['time_15min'].dt.hour\n",
    "    aggregated['day_of_week'] = aggregated['time_15min'].dt.dayofweek\n",
    "\n",
    "    # Calculate I_VRU (VRU conflict intensity) per interval\n",
    "    # This is the key metric for VRU Safety Index\n",
    "    aggregated['I_VRU'] = aggregated['vru_event_count']\n",
    "\n",
    "    print(f\"✓ Generated {len(aggregated)} 15-minute event aggregates\")\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\n",
    "    f\"\\nAggregating safety events from {start_dt.date()} to {end_dt.date()}...\")\n",
    "aggregated_events = aggregate_safety_events(\n",
    "    start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(aggregated_events) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAFETY EVENT AGGREGATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total 15-minute intervals with events: {len(aggregated_events)}\")\n",
    "    print(\n",
    "        f\"Intersections covered: {aggregated_events['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"Date range: {aggregated_events['time_15min'].min()} to {aggregated_events['time_15min'].max()}\")\n",
    "\n",
    "    print(f\"\\nEvent statistics:\")\n",
    "    print(\n",
    "        f\"  Total events: {aggregated_events['total_event_count'].sum():.0f}\")\n",
    "    print(\n",
    "        f\"  VRU-involved: {aggregated_events['vru_event_count'].sum():.0f} ({100*aggregated_events['vru_event_count'].sum()/aggregated_events['total_event_count'].sum():.1f}%)\")\n",
    "    print(\n",
    "        f\"  Vehicle-only: {aggregated_events['vehicle_event_count'].sum():.0f}\")\n",
    "    print(\n",
    "        f\"  Total severity-weighted score: {aggregated_events['severity_weighted_score'].sum():.0f}\")\n",
    "\n",
    "    print(f\"\\nI_VRU (VRU conflict intensity) statistics:\")\n",
    "    print(aggregated_events['I_VRU'].describe())\n",
    "\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(aggregated_events[['intersection', 'time_15min', 'total_event_count',\n",
    "          'vru_event_count', 'I_VRU', 'severity_weighted_score']].head(10).to_string(index=False))\n",
    "\n",
    "    # Store I_max for normalization\n",
    "    if 'NORMALIZATION_CONSTANTS' not in dir():\n",
    "        NORMALIZATION_CONSTANTS = {}\n",
    "\n",
    "    NORMALIZATION_CONSTANTS['I_max'] = aggregated_events['I_VRU'].max()\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"UPDATED NORMALIZATION CONSTANTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"I_max (max VRU conflict intensity): {NORMALIZATION_CONSTANTS['I_max']:.0f}\")\n",
    "else:\n",
    "    print(\"⚠ No aggregated events generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "qe9f5x2bmvi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 4: MASTER FEATURE TABLE CONSTRUCTION\n",
      "================================================================================\n",
      "Creating master feature table from collected data...\n",
      "\n",
      "Data sources availability:\n",
      "  ✗ bsm_features\n",
      "  ✗ psm_features\n",
      "  ✗ aggregated_events\n",
      "  ✗ vehicle_counts\n",
      "  ✗ vru_counts\n",
      "\n",
      "⚠ ERROR: Cannot create master feature table - BSM features are required\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Create master feature table by joining all data sources\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: MASTER FEATURE TABLE CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def create_master_feature_table(bsm_features, psm_features, aggregated_events,\n",
    "                                vehicle_counts, vru_counts):\n",
    "    \"\"\"\n",
    "    Join all feature sources into a unified feature table.\n",
    "\n",
    "    Inputs:\n",
    "    - bsm_features: Vehicle behavior features (speed, variance, braking)\n",
    "    - psm_features: VRU features (pedestrian/cyclist counts, speed)\n",
    "    - aggregated_events: Safety events aggregated to 15-min intervals\n",
    "    - vehicle_counts: Vehicle exposure metrics\n",
    "    - vru_counts: VRU exposure metrics\n",
    "\n",
    "    Returns:\n",
    "        Comprehensive feature table ready for index computation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Building master feature table...\")\n",
    "\n",
    "    # Start with BSM features as the base (most comprehensive)\n",
    "    if len(bsm_features) == 0:\n",
    "        print(\"⚠ ERROR: No BSM features available - cannot build master table\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    master = bsm_features.copy()\n",
    "    print(f\"  Base table (BSM features): {len(master)} records\")\n",
    "\n",
    "    # Aggregate vehicle counts to 15-minute intervals\n",
    "    if len(vehicle_counts) > 0:\n",
    "        vehicle_counts['time_15min'] = pd.to_datetime(\n",
    "            vehicle_counts['time']).dt.floor('15min')\n",
    "        vehicle_agg = vehicle_counts.groupby(['intersection', 'time_15min'])[\n",
    "            'vehicle_count'].sum().reset_index()\n",
    "        vehicle_agg.rename(\n",
    "            columns={'vehicle_count': 'vehicle_volume'}, inplace=True)\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            vehicle_agg, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + Vehicle counts: {len(vehicle_agg)} records merged\")\n",
    "    else:\n",
    "        master['vehicle_volume'] = 0\n",
    "        print(\"  ⚠ No vehicle count data available\")\n",
    "\n",
    "    # Aggregate VRU counts to 15-minute intervals\n",
    "    if len(vru_counts) > 0:\n",
    "        vru_counts['time_15min'] = pd.to_datetime(\n",
    "            vru_counts['time']).dt.floor('15min')\n",
    "        vru_agg = vru_counts.groupby(['intersection', 'time_15min'])[\n",
    "            'vru_count'].sum().reset_index()\n",
    "        vru_agg.rename(columns={'vru_count': 'vru_volume'}, inplace=True)\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            vru_agg, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + VRU counts: {len(vru_agg)} records merged\")\n",
    "    else:\n",
    "        master['vru_volume'] = 0\n",
    "        print(\"  ⚠ No VRU count data available\")\n",
    "\n",
    "    # Join PSM features\n",
    "    if len(psm_features) > 0:\n",
    "        psm_cols = ['intersection', 'time_15min', 'vru_count', 'avg_vru_speed',\n",
    "                    'pedestrian_count', 'cyclist_count']\n",
    "        psm_subset = psm_features[psm_cols].copy()\n",
    "        psm_subset.rename(columns={'vru_count': 'psm_vru_count'}, inplace=True)\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            psm_subset, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + PSM features: {len(psm_features)} records merged\")\n",
    "    else:\n",
    "        master['psm_vru_count'] = 0\n",
    "        master['avg_vru_speed'] = 0\n",
    "        master['pedestrian_count'] = 0\n",
    "        master['cyclist_count'] = 0\n",
    "        print(\"  ⚠ No PSM data available\")\n",
    "\n",
    "    # Join aggregated safety events\n",
    "    if len(aggregated_events) > 0:\n",
    "        event_cols = ['intersection', 'time_15min', 'total_event_count', 'vru_event_count',\n",
    "                      'vehicle_event_count', 'severity_weighted_score', 'I_VRU']\n",
    "        event_subset = aggregated_events[event_cols].copy()\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            event_subset, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + Safety events: {len(aggregated_events)} records merged\")\n",
    "    else:\n",
    "        master['total_event_count'] = 0\n",
    "        master['vru_event_count'] = 0\n",
    "        master['vehicle_event_count'] = 0\n",
    "        master['severity_weighted_score'] = 0\n",
    "        master['I_VRU'] = 0\n",
    "        print(\"  ⚠ No safety event data available\")\n",
    "\n",
    "    # Fill NaN values with 0 for count/intensity metrics\n",
    "    count_cols = ['vehicle_volume', 'vru_volume', 'psm_vru_count', 'pedestrian_count',\n",
    "                  'cyclist_count', 'total_event_count', 'vru_event_count',\n",
    "                  'vehicle_event_count', 'severity_weighted_score', 'I_VRU']\n",
    "\n",
    "    for col in count_cols:\n",
    "        if col in master.columns:\n",
    "            master[col] = master[col].fillna(0)\n",
    "\n",
    "    # Fill NaN values for continuous features with median\n",
    "    continuous_cols = ['avg_vru_speed']\n",
    "    for col in continuous_cols:\n",
    "        if col in master.columns and master[col].notna().any():\n",
    "            master[col] = master[col].fillna(master[col].median())\n",
    "\n",
    "    print(f\"\\n✓ Master feature table created: {len(master)} records\")\n",
    "    print(f\"  Columns: {len(master.columns)}\")\n",
    "    print(f\"  Intersections: {master['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"  Date range: {master['time_15min'].min()} to {master['time_15min'].max()}\")\n",
    "\n",
    "    return master\n",
    "\n",
    "\n",
    "# Create the master feature table\n",
    "print(\"Creating master feature table from collected data...\")\n",
    "\n",
    "# Check which data sources we have\n",
    "data_sources_available = {\n",
    "    'bsm_features': 'bsm_features' in dir() and len(bsm_features) > 0,\n",
    "    'psm_features': 'psm_features' in dir() and len(psm_features) > 0,\n",
    "    'aggregated_events': 'aggregated_events' in dir() and len(aggregated_events) > 0,\n",
    "    'vehicle_counts': 'vehicle_counts' in dir() and len(vehicle_counts) > 0,\n",
    "    'vru_counts': 'vru_counts' in dir() and len(vru_counts) > 0\n",
    "}\n",
    "\n",
    "print(\"\\nData sources availability:\")\n",
    "for source, available in data_sources_available.items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    print(f\"  {status} {source}\")\n",
    "\n",
    "if data_sources_available['bsm_features']:\n",
    "    master_features = create_master_feature_table(\n",
    "        bsm_features=bsm_features,\n",
    "        psm_features=psm_features if data_sources_available['psm_features'] else pd.DataFrame(\n",
    "        ),\n",
    "        aggregated_events=aggregated_events if data_sources_available['aggregated_events'] else pd.DataFrame(\n",
    "        ),\n",
    "        vehicle_counts=vehicle_counts if data_sources_available['vehicle_counts'] else pd.DataFrame(\n",
    "        ),\n",
    "        vru_counts=vru_counts if data_sources_available['vru_counts'] else pd.DataFrame(\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if len(master_features) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"MASTER FEATURE TABLE SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nShape: {master_features.shape}\")\n",
    "        print(f\"\\nColumn names:\")\n",
    "        for i, col in enumerate(master_features.columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "        print(f\"\\nKey metrics summary:\")\n",
    "        key_metrics = ['vehicle_count', 'vehicle_volume', 'avg_speed', 'speed_variance',\n",
    "                       'hard_braking_count', 'vru_volume', 'I_VRU', 'total_event_count']\n",
    "        available_metrics = [\n",
    "            m for m in key_metrics if m in master_features.columns]\n",
    "        print(master_features[available_metrics].describe())\n",
    "\n",
    "        print(f\"\\nSample records:\")\n",
    "        display_cols = ['intersection', 'time_15min', 'vehicle_count', 'avg_speed',\n",
    "                        'I_VRU', 'total_event_count', 'vru_volume']\n",
    "        display_cols = [\n",
    "            c for c in display_cols if c in master_features.columns]\n",
    "        print(master_features[display_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n⚠ ERROR: Cannot create master feature table - BSM features are required\")\n",
    "    master_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "786qw0rygee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 5: NORMALIZATION CONSTANTS\n",
      "================================================================================\n",
      "⚠ ERROR: Master feature table not available - cannot compute constants\n"
     ]
    }
   ],
   "source": [
    "# Phase 5: Compute all normalization constants\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 5: NORMALIZATION CONSTANTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def compute_normalization_constants(master_features):\n",
    "    \"\"\"\n",
    "    Compute all normalization constants from the master feature table.\n",
    "\n",
    "    Constants computed:\n",
    "    - I_max: Maximum VRU conflict intensity (events per 15-min)\n",
    "    - V_max: Maximum vehicle volume per 15-min interval\n",
    "    - σ_max: Maximum speed variance\n",
    "    - S_ref: Reference speed (85th percentile of average speeds)\n",
    "    - N_VRU_max: Maximum VRU count per 15-min interval\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of normalization constants\n",
    "    \"\"\"\n",
    "\n",
    "    if len(master_features) == 0:\n",
    "        print(\"⚠ ERROR: No master features available\")\n",
    "        return {}\n",
    "\n",
    "    constants = {}\n",
    "\n",
    "    # I_max: Maximum VRU conflict intensity\n",
    "    if 'I_VRU' in master_features.columns:\n",
    "        constants['I_max'] = float(master_features['I_VRU'].max())\n",
    "        if constants['I_max'] == 0:\n",
    "            constants['I_max'] = 1.0  # Avoid division by zero\n",
    "            print(\"  ⚠ Warning: I_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "    else:\n",
    "        constants['I_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: I_VRU not found, setting I_max to 1.0\")\n",
    "\n",
    "    # V_max: Maximum vehicle volume (use both BSM vehicle_count and detector vehicle_volume)\n",
    "    v_sources = []\n",
    "    if 'vehicle_count' in master_features.columns:\n",
    "        v_sources.append(master_features['vehicle_count'].max())\n",
    "    if 'vehicle_volume' in master_features.columns:\n",
    "        v_sources.append(master_features['vehicle_volume'].max())\n",
    "\n",
    "    constants['V_max'] = float(max(v_sources)) if v_sources else 1.0\n",
    "    if constants['V_max'] == 0:\n",
    "        constants['V_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: V_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "\n",
    "    # σ_max: Maximum speed variance\n",
    "    if 'speed_variance' in master_features.columns:\n",
    "        constants['sigma_max'] = float(master_features['speed_variance'].max())\n",
    "        if constants['sigma_max'] == 0:\n",
    "            constants['sigma_max'] = 1.0\n",
    "            print(\"  ⚠ Warning: sigma_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "    else:\n",
    "        constants['sigma_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: speed_variance not found, setting sigma_max to 1.0\")\n",
    "\n",
    "    # S_ref: Reference speed (85th percentile)\n",
    "    if 'avg_speed' in master_features.columns:\n",
    "        # Filter out zeros and NaNs for realistic speed reference\n",
    "        valid_speeds = master_features['avg_speed'][master_features['avg_speed'] > 0]\n",
    "        if len(valid_speeds) > 0:\n",
    "            constants['S_ref'] = float(valid_speeds.quantile(0.85))\n",
    "            if constants['S_ref'] == 0:\n",
    "                constants['S_ref'] = 1.0\n",
    "                print(\"  ⚠ Warning: S_ref is 0, setting to 1.0\")\n",
    "        else:\n",
    "            constants['S_ref'] = 1.0\n",
    "            print(\"  ⚠ Warning: No valid speeds found, setting S_ref to 1.0\")\n",
    "    else:\n",
    "        constants['S_ref'] = 1.0\n",
    "        print(\"  ⚠ Warning: avg_speed not found, setting S_ref to 1.0\")\n",
    "\n",
    "    # N_VRU_max: Maximum VRU count (use both PSM and detector counts)\n",
    "    vru_sources = []\n",
    "    if 'psm_vru_count' in master_features.columns:\n",
    "        vru_sources.append(master_features['psm_vru_count'].max())\n",
    "    if 'vru_volume' in master_features.columns:\n",
    "        vru_sources.append(master_features['vru_volume'].max())\n",
    "    if 'pedestrian_count' in master_features.columns:\n",
    "        vru_sources.append(master_features['pedestrian_count'].max())\n",
    "    if 'cyclist_count' in master_features.columns:\n",
    "        vru_sources.append(master_features['cyclist_count'].max())\n",
    "\n",
    "    constants['N_VRU_max'] = float(max(vru_sources)) if vru_sources else 1.0\n",
    "    if constants['N_VRU_max'] == 0:\n",
    "        constants['N_VRU_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: N_VRU_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "\n",
    "    # Additional constants for Vehicle Index\n",
    "    # Hard braking rate normalization\n",
    "    if 'hard_braking_count' in master_features.columns:\n",
    "        constants['hard_braking_max'] = float(\n",
    "            master_features['hard_braking_count'].max())\n",
    "        if constants['hard_braking_max'] == 0:\n",
    "            constants['hard_braking_max'] = 1.0\n",
    "    else:\n",
    "        constants['hard_braking_max'] = 1.0\n",
    "\n",
    "    # Heading change rate normalization\n",
    "    if 'heading_change_rate' in master_features.columns:\n",
    "        constants['heading_change_max'] = float(\n",
    "            master_features['heading_change_rate'].max())\n",
    "        if constants['heading_change_max'] == 0:\n",
    "            constants['heading_change_max'] = 1.0\n",
    "    else:\n",
    "        constants['heading_change_max'] = 1.0\n",
    "\n",
    "    return constants\n",
    "\n",
    "\n",
    "# Compute normalization constants from master feature table\n",
    "if 'master_features' in dir() and len(master_features) > 0:\n",
    "    NORMALIZATION_CONSTANTS = compute_normalization_constants(master_features)\n",
    "\n",
    "    print(\"\\n✓ Normalization constants computed:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"  I_max (max VRU conflict intensity):    {NORMALIZATION_CONSTANTS['I_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  V_max (max vehicle volume):            {NORMALIZATION_CONSTANTS['V_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  σ_max (max speed variance):            {NORMALIZATION_CONSTANTS['sigma_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  S_ref (reference speed, 85th pct):     {NORMALIZATION_CONSTANTS['S_ref']:.2f} m/s\")\n",
    "    print(\n",
    "        f\"  N_VRU_max (max VRU count):             {NORMALIZATION_CONSTANTS['N_VRU_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  hard_braking_max:                      {NORMALIZATION_CONSTANTS['hard_braking_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  heading_change_max:                    {NORMALIZATION_CONSTANTS['heading_change_max']:.2f}\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"These constants will be used to normalize features in the Safety Index formulas.\")\n",
    "    print(\"In production, these should be periodically recalibrated (e.g., monthly).\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ ERROR: Master feature table not available - cannot compute constants\")\n",
    "    NORMALIZATION_CONSTANTS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qcz5en3no6",
   "metadata": {},
   "source": [
    "## Phase 6: Safety Index Computation\n",
    "\n",
    "Implement the VRU, Vehicle, and Combined Safety Index formulas from the checkpoint document.\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "- **VRU Index** = 100 × [0.4×(I_VRU/I_max) + 0.2×(V/V_max) + 0.2×(S/S_ref) + 0.2×(σ_S/σ_max)]\n",
    "- **Vehicle Index** = 100 × [0.3×(I_vehicle/I_max) + 0.3×(V/V_max) + 0.2×(σ_S/σ_max) + 0.2×(hard_braking_rate)]\n",
    "- **Combined Index** = 0.6×VRU_Index + 0.4×Vehicle_Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "lfeaaup8jup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 6: SAFETY INDEX COMPUTATION\n",
      "================================================================================\n",
      "⚠ ERROR: Cannot compute safety indices - missing master features or normalization constants\n"
     ]
    }
   ],
   "source": [
    "# Phase 6: Implement Safety Index formulas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 6: SAFETY INDEX COMPUTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def compute_safety_indices(master_features, norm_constants):\n",
    "    \"\"\"\n",
    "    Compute VRU, Vehicle, and Combined Safety Indices for each 15-minute interval.\n",
    "\n",
    "    Formulas from checkpoint document:\n",
    "    - VRU Index = 100 × [0.4×(I_VRU/I_max) + 0.2×(V/V_max) + 0.2×(S/S_ref) + 0.2×(σ_S/σ_max)]\n",
    "    - Vehicle Index = 100 × [0.3×(I_vehicle/I_max) + 0.3×(V/V_max) + 0.2×(σ_S/σ_max) + 0.2×(hard_braking)]\n",
    "    - Combined Index = 0.6×VRU_Index + 0.4×Vehicle_Index\n",
    "\n",
    "    Args:\n",
    "        master_features: Master feature table with all metrics\n",
    "        norm_constants: Dictionary of normalization constants\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with computed indices added\n",
    "    \"\"\"\n",
    "\n",
    "    if len(master_features) == 0:\n",
    "        print(\"⚠ ERROR: No master features available\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not norm_constants:\n",
    "        print(\"⚠ ERROR: No normalization constants available\")\n",
    "        return master_features\n",
    "\n",
    "    df = master_features.copy()\n",
    "\n",
    "    # Extract normalization constants\n",
    "    I_max = norm_constants.get('I_max', 1.0)\n",
    "    V_max = norm_constants.get('V_max', 1.0)\n",
    "    sigma_max = norm_constants.get('sigma_max', 1.0)\n",
    "    S_ref = norm_constants.get('S_ref', 1.0)\n",
    "    N_VRU_max = norm_constants.get('N_VRU_max', 1.0)\n",
    "    hard_braking_max = norm_constants.get('hard_braking_max', 1.0)\n",
    "\n",
    "    print(\"Computing normalized components...\")\n",
    "\n",
    "    # ========== VRU Safety Index Components ==========\n",
    "\n",
    "    # Component 1: VRU conflict intensity (I_VRU/I_max)\n",
    "    df['I_VRU_norm'] = df['I_VRU'] / I_max if I_max > 0 else 0\n",
    "\n",
    "    # Component 2: Vehicle volume exposure (V/V_max)\n",
    "    # Use vehicle_count from BSM as primary, fall back to vehicle_volume from detectors\n",
    "    df['V'] = df['vehicle_count'].fillna(0)\n",
    "    if 'vehicle_volume' in df.columns:\n",
    "        df['V'] = df['V'].combine_first(df['vehicle_volume'])\n",
    "    df['V_norm'] = df['V'] / V_max if V_max > 0 else 0\n",
    "\n",
    "    # Component 3: Speed factor (S/S_ref)\n",
    "    df['S_norm'] = df['avg_speed'] / S_ref if S_ref > 0 else 0\n",
    "\n",
    "    # Component 4: Speed variance (σ_S/σ_max)\n",
    "    df['sigma_norm'] = df['speed_variance'] / sigma_max if sigma_max > 0 else 0\n",
    "\n",
    "    # Compute VRU Safety Index\n",
    "    df['VRU_Index'] = 100 * (\n",
    "        0.4 * df['I_VRU_norm'] +\n",
    "        0.2 * df['V_norm'] +\n",
    "        0.2 * df['S_norm'] +\n",
    "        0.2 * df['sigma_norm']\n",
    "    )\n",
    "\n",
    "    # Cap at 100\n",
    "    df['VRU_Index'] = df['VRU_Index'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ VRU Safety Index computed\")\n",
    "\n",
    "    # ========== Vehicle Safety Index Components ==========\n",
    "\n",
    "    # Component 1: Vehicle-vehicle conflict intensity\n",
    "    if 'vehicle_event_count' in df.columns:\n",
    "        df['I_vehicle'] = df['vehicle_event_count']\n",
    "    else:\n",
    "        # Fall back to total events minus VRU events\n",
    "        df['I_vehicle'] = df.get('total_event_count', 0) - \\\n",
    "            df.get('vru_event_count', 0)\n",
    "\n",
    "    df['I_vehicle_norm'] = df['I_vehicle'] / I_max if I_max > 0 else 0\n",
    "\n",
    "    # Component 2: Vehicle volume (same as VRU index)\n",
    "    # Already computed as V_norm\n",
    "\n",
    "    # Component 3: Speed variance (same as VRU index)\n",
    "    # Already computed as sigma_norm\n",
    "\n",
    "    # Component 4: Hard braking rate\n",
    "    if 'hard_braking_count' in df.columns:\n",
    "        df['hard_braking_norm'] = df['hard_braking_count'] / \\\n",
    "            hard_braking_max if hard_braking_max > 0 else 0\n",
    "    else:\n",
    "        df['hard_braking_norm'] = 0\n",
    "\n",
    "    # Compute Vehicle Safety Index\n",
    "    df['Vehicle_Index'] = 100 * (\n",
    "        0.3 * df['I_vehicle_norm'] +\n",
    "        0.3 * df['V_norm'] +\n",
    "        0.2 * df['sigma_norm'] +\n",
    "        0.2 * df['hard_braking_norm']\n",
    "    )\n",
    "\n",
    "    # Cap at 100\n",
    "    df['Vehicle_Index'] = df['Vehicle_Index'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ Vehicle Safety Index computed\")\n",
    "\n",
    "    # ========== Combined Safety Index ==========\n",
    "\n",
    "    df['Combined_Index'] = (\n",
    "        0.6 * df['VRU_Index'] +\n",
    "        0.4 * df['Vehicle_Index']\n",
    "    )\n",
    "\n",
    "    # Cap at 100\n",
    "    df['Combined_Index'] = df['Combined_Index'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ Combined Safety Index computed\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Compute safety indices\n",
    "if 'master_features' in dir() and len(master_features) > 0 and NORMALIZATION_CONSTANTS:\n",
    "    print(\"Computing safety indices for all 15-minute intervals...\")\n",
    "\n",
    "    indices_df = compute_safety_indices(\n",
    "        master_features, NORMALIZATION_CONSTANTS)\n",
    "\n",
    "    if len(indices_df) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SAFETY INDEX SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(f\"\\nVRU Safety Index statistics:\")\n",
    "        print(indices_df['VRU_Index'].describe())\n",
    "\n",
    "        print(f\"\\nVehicle Safety Index statistics:\")\n",
    "        print(indices_df['Vehicle_Index'].describe())\n",
    "\n",
    "        print(f\"\\nCombined Safety Index statistics:\")\n",
    "        print(indices_df['Combined_Index'].describe())\n",
    "\n",
    "        # Show distribution by intersection\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AVERAGE SAFETY INDICES BY INTERSECTION\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        intersection_summary = indices_df.groupby('intersection').agg({\n",
    "            'VRU_Index': ['mean', 'std', 'max'],\n",
    "            'Vehicle_Index': ['mean', 'std', 'max'],\n",
    "            'Combined_Index': ['mean', 'std', 'max'],\n",
    "            'time_15min': 'count'\n",
    "        }).round(2)\n",
    "\n",
    "        intersection_summary.columns = [\n",
    "            '_'.join(col).strip() for col in intersection_summary.columns.values]\n",
    "        intersection_summary.rename(\n",
    "            columns={'time_15min_count': 'num_intervals'}, inplace=True)\n",
    "\n",
    "        print(intersection_summary.to_string())\n",
    "\n",
    "        # Show highest risk intervals\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TOP 10 HIGHEST RISK INTERVALS (Combined Index)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        top_risk = indices_df.nlargest(10, 'Combined_Index')[\n",
    "            ['intersection', 'time_15min', 'VRU_Index', 'Vehicle_Index', 'Combined_Index',\n",
    "             'I_VRU', 'vehicle_count', 'avg_speed', 'speed_variance']\n",
    "        ]\n",
    "        print(top_risk.to_string(index=False))\n",
    "\n",
    "        # Show safest intervals\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TOP 10 SAFEST INTERVALS (Combined Index)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        safest = indices_df.nsmallest(10, 'Combined_Index')[\n",
    "            ['intersection', 'time_15min', 'VRU_Index',\n",
    "                'Vehicle_Index', 'Combined_Index']\n",
    "        ]\n",
    "        print(safest.to_string(index=False))\n",
    "\n",
    "        # Time-of-day analysis\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AVERAGE SAFETY INDICES BY HOUR OF DAY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        hourly_summary = indices_df.groupby('hour_of_day').agg({\n",
    "            'VRU_Index': 'mean',\n",
    "            'Vehicle_Index': 'mean',\n",
    "            'Combined_Index': 'mean'\n",
    "        }).round(2)\n",
    "\n",
    "        print(hourly_summary.to_string())\n",
    "\n",
    "else:\n",
    "    print(\"⚠ ERROR: Cannot compute safety indices - missing master features or normalization constants\")\n",
    "    indices_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "xv37m8scn2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 7: EMPIRICAL BAYES STABILIZATION\n",
      "================================================================================\n",
      "⚠ ERROR: No indices available for Empirical Bayes adjustment\n"
     ]
    }
   ],
   "source": [
    "# Phase 7: Empirical Bayes stabilization\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 7: EMPIRICAL BAYES STABILIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def apply_empirical_bayes(indices_df, baseline_events, lambda_param=None):\n",
    "    \"\"\"\n",
    "    Apply Empirical Bayes adjustment to safety indices.\n",
    "\n",
    "    Adjusts raw indices based on:\n",
    "    1. Historical baseline risk at the intersection\n",
    "    2. Reliability of the current estimate (based on sample size)\n",
    "\n",
    "    Formula: Adjusted_Index = λ × Raw_Index + (1-λ) × Baseline_Index\n",
    "\n",
    "    Where λ = N / (N + k), with:\n",
    "    - N = number of observations in current period\n",
    "    - k = tuning parameter (default: 50 for 15-min intervals)\n",
    "\n",
    "    Args:\n",
    "        indices_df: DataFrame with computed raw indices\n",
    "        baseline_events: Historical safety events for baseline calculation\n",
    "        lambda_param: Fixed lambda value (0-1), or None for adaptive lambda\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with EB-adjusted indices added\n",
    "    \"\"\"\n",
    "\n",
    "    if len(indices_df) == 0:\n",
    "        print(\"⚠ No indices to adjust\")\n",
    "        return indices_df\n",
    "\n",
    "    df = indices_df.copy()\n",
    "\n",
    "    # Calculate historical baseline indices by intersection\n",
    "    print(\"Computing baseline risk scores from historical data...\")\n",
    "\n",
    "    if len(baseline_events) > 0:\n",
    "        # Calculate baseline severity-weighted event rate by intersection and time-of-day\n",
    "        baseline_events['hour_of_day'] = baseline_events['hour_of_day'].astype(\n",
    "            int)\n",
    "\n",
    "        baseline_summary = baseline_events.groupby(['intersection', 'hour_of_day']).agg({\n",
    "            'severity_weight': 'sum',\n",
    "            'event_id': 'count',\n",
    "            'is_vru_involved': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        baseline_summary.rename(columns={\n",
    "            'severity_weight': 'baseline_severity',\n",
    "            'event_id': 'baseline_event_count',\n",
    "            'is_vru_involved': 'baseline_vru_count'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Merge baseline with current data\n",
    "        df['hour_of_day'] = df['hour_of_day'].astype(int)\n",
    "        df = df.merge(baseline_summary, on=[\n",
    "                      'intersection', 'hour_of_day'], how='left')\n",
    "\n",
    "        # Fill missing baselines with intersection average\n",
    "        intersection_avg = baseline_summary.groupby('intersection').agg({\n",
    "            'baseline_severity': 'mean',\n",
    "            'baseline_event_count': 'mean',\n",
    "            'baseline_vru_count': 'mean'\n",
    "        })\n",
    "\n",
    "        for col in ['baseline_severity', 'baseline_event_count', 'baseline_vru_count']:\n",
    "            df[col] = df.apply(\n",
    "                lambda row: intersection_avg.loc[row['intersection'], col]\n",
    "                if pd.isna(row[col]) and row['intersection'] in intersection_avg.index\n",
    "                else row[col],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Fill any remaining NaNs with global mean\n",
    "        df['baseline_severity'] = df['baseline_severity'].fillna(\n",
    "            baseline_summary['baseline_severity'].mean())\n",
    "        df['baseline_event_count'] = df['baseline_event_count'].fillna(\n",
    "            baseline_summary['baseline_event_count'].mean())\n",
    "        df['baseline_vru_count'] = df['baseline_vru_count'].fillna(\n",
    "            baseline_summary['baseline_vru_count'].mean())\n",
    "\n",
    "        print(\n",
    "            f\"  ✓ Baseline computed for {len(baseline_summary)} intersection-hour combinations\")\n",
    "    else:\n",
    "        print(\"  ⚠ No baseline data available - using global mean\")\n",
    "        df['baseline_severity'] = df['severity_weighted_score'].mean(\n",
    "        ) if 'severity_weighted_score' in df.columns else 0\n",
    "        df['baseline_event_count'] = df['total_event_count'].mean(\n",
    "        ) if 'total_event_count' in df.columns else 0\n",
    "        df['baseline_vru_count'] = df['vru_event_count'].mean(\n",
    "        ) if 'vru_event_count' in df.columns else 0\n",
    "\n",
    "    # Calculate lambda (weight for raw vs baseline)\n",
    "    print(\"\\nCalculating Empirical Bayes weights...\")\n",
    "\n",
    "    if lambda_param is not None:\n",
    "        # Use fixed lambda\n",
    "        df['lambda'] = lambda_param\n",
    "        print(f\"  Using fixed lambda = {lambda_param}\")\n",
    "    else:\n",
    "        # Adaptive lambda based on sample size\n",
    "        # Lambda = N / (N + k), where k is a tuning parameter\n",
    "        k = 50  # Tuning parameter: higher k = more shrinkage toward baseline\n",
    "\n",
    "        # Use vehicle count as proxy for sample size (more vehicles = more reliable estimate)\n",
    "        df['N'] = df['vehicle_count'].fillna(\n",
    "            0) + 1  # Add 1 to avoid division by zero\n",
    "        df['lambda'] = df['N'] / (df['N'] + k)\n",
    "\n",
    "        print(f\"  Using adaptive lambda with k={k}\")\n",
    "        print(\n",
    "            f\"  Lambda range: {df['lambda'].min():.3f} to {df['lambda'].max():.3f}\")\n",
    "        print(f\"  Mean lambda: {df['lambda'].mean():.3f}\")\n",
    "\n",
    "    # Convert baseline to index scale (approximate)\n",
    "    # Use baseline event count relative to maximum as a simple baseline index\n",
    "    max_baseline = df['baseline_event_count'].max()\n",
    "    df['baseline_index'] = (df['baseline_event_count'] /\n",
    "                            max_baseline * 100) if max_baseline > 0 else 20.0\n",
    "    df['baseline_index'] = df['baseline_index'].fillna(\n",
    "        20.0)  # Default baseline = 20 (low-medium risk)\n",
    "\n",
    "    # Apply Empirical Bayes adjustment\n",
    "    print(\"\\nApplying Empirical Bayes adjustment...\")\n",
    "\n",
    "    df['VRU_Index_EB'] = df['lambda'] * df['VRU_Index'] + \\\n",
    "        (1 - df['lambda']) * df['baseline_index']\n",
    "    df['Vehicle_Index_EB'] = df['lambda'] * df['Vehicle_Index'] + \\\n",
    "        (1 - df['lambda']) * df['baseline_index']\n",
    "    df['Combined_Index_EB'] = df['lambda'] * df['Combined_Index'] + \\\n",
    "        (1 - df['lambda']) * df['baseline_index']\n",
    "\n",
    "    # Cap at 100\n",
    "    df['VRU_Index_EB'] = df['VRU_Index_EB'].clip(0, 100)\n",
    "    df['Vehicle_Index_EB'] = df['Vehicle_Index_EB'].clip(0, 100)\n",
    "    df['Combined_Index_EB'] = df['Combined_Index_EB'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ Empirical Bayes adjustment applied\")\n",
    "\n",
    "    # Calculate adjustment magnitude\n",
    "    df['EB_adjustment'] = df['Combined_Index'] - df['Combined_Index_EB']\n",
    "\n",
    "    print(f\"\\n  Adjustment statistics:\")\n",
    "    print(f\"    Mean adjustment: {df['EB_adjustment'].mean():.2f}\")\n",
    "    print(f\"    Std adjustment: {df['EB_adjustment'].std():.2f}\")\n",
    "    print(f\"    Max upward adjustment: {df['EB_adjustment'].min():.2f}\")\n",
    "    print(f\"    Max downward adjustment: {df['EB_adjustment'].max():.2f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply Empirical Bayes stabilization\n",
    "if 'indices_df' in dir() and len(indices_df) > 0:\n",
    "    print(\"Applying Empirical Bayes stabilization to safety indices...\")\n",
    "\n",
    "    baseline_data = baseline_events if 'baseline_events' in dir() and len(\n",
    "        baseline_events) > 0 else pd.DataFrame()\n",
    "\n",
    "    indices_df_eb = apply_empirical_bayes(\n",
    "        indices_df, baseline_data, lambda_param=None)\n",
    "\n",
    "    if len(indices_df_eb) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EMPIRICAL BAYES ADJUSTED INDICES SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(f\"\\nVRU Safety Index (EB-adjusted):\")\n",
    "        print(indices_df_eb['VRU_Index_EB'].describe())\n",
    "\n",
    "        print(f\"\\nVehicle Safety Index (EB-adjusted):\")\n",
    "        print(indices_df_eb['Vehicle_Index_EB'].describe())\n",
    "\n",
    "        print(f\"\\nCombined Safety Index (EB-adjusted):\")\n",
    "        print(indices_df_eb['Combined_Index_EB'].describe())\n",
    "\n",
    "        # Compare raw vs EB-adjusted\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPARISON: RAW vs EB-ADJUSTED INDICES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        comparison = pd.DataFrame({\n",
    "            'Metric': ['VRU Index', 'Vehicle Index', 'Combined Index'],\n",
    "            'Raw Mean': [\n",
    "                indices_df_eb['VRU_Index'].mean(),\n",
    "                indices_df_eb['Vehicle_Index'].mean(),\n",
    "                indices_df_eb['Combined_Index'].mean()\n",
    "            ],\n",
    "            'EB Mean': [\n",
    "                indices_df_eb['VRU_Index_EB'].mean(),\n",
    "                indices_df_eb['Vehicle_Index_EB'].mean(),\n",
    "                indices_df_eb['Combined_Index_EB'].mean()\n",
    "            ],\n",
    "            'Raw Std': [\n",
    "                indices_df_eb['VRU_Index'].std(),\n",
    "                indices_df_eb['Vehicle_Index'].std(),\n",
    "                indices_df_eb['Combined_Index'].std()\n",
    "            ],\n",
    "            'EB Std': [\n",
    "                indices_df_eb['VRU_Index_EB'].std(),\n",
    "                indices_df_eb['Vehicle_Index_EB'].std(),\n",
    "                indices_df_eb['Combined_Index_EB'].std()\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        print(comparison.round(2).to_string(index=False))\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"IMPACT OF EMPIRICAL BAYES ADJUSTMENT\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Note: EB adjustment typically:\")\n",
    "        print(\"  • Reduces variance (more stable estimates)\")\n",
    "        print(\"  • Shrinks extreme values toward baseline\")\n",
    "        print(\"  • Improves prediction accuracy for low-sample situations\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\"⚠ ERROR: No indices available for Empirical Bayes adjustment\")\n",
    "    indices_df_eb = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkta9g8m0m",
   "metadata": {},
   "source": [
    "## Phase 8: Validation & Next Steps\n",
    "\n",
    "### Validation Approaches\n",
    "\n",
    "**1. Predictive Validation**\n",
    "\n",
    "- Split data into training (historical) and testing (recent) periods\n",
    "- Test if high-index intervals predict future safety events\n",
    "- Metrics: ROC-AUC, precision-recall curves\n",
    "\n",
    "**2. Sensitivity Analysis**\n",
    "\n",
    "- Vary formula weights (e.g., VRU vs Vehicle Index contributions)\n",
    "- Test different Empirical Bayes k parameters\n",
    "- Assess robustness to normalization constant changes\n",
    "\n",
    "**3. Expert Review**\n",
    "\n",
    "- Present high-risk intervals to traffic safety experts\n",
    "- Validate against known problematic intersections\n",
    "- Incorporate domain knowledge for weight calibration\n",
    "\n",
    "**4. Comparison with Baseline Methods**\n",
    "\n",
    "- Compare against simple crash rate\n",
    "- Benchmark against national safety metrics (e.g., USDOT FAST tool)\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "**Backend (FastAPI)**\n",
    "\n",
    "- [ ] Refactor data collection functions into API endpoints\n",
    "- [ ] Implement PostgreSQL schema for feature storage\n",
    "- [ ] Set up Cloud Scheduler for 15-minute triggers\n",
    "- [ ] Add caching layer (Redis) for normalization constants\n",
    "- [ ] Implement error handling and data quality checks\n",
    "- [ ] Add logging and monitoring (Cloud Logging)\n",
    "\n",
    "**Frontend (Streamlit)**\n",
    "\n",
    "- [ ] Create interactive map visualization (Folium/Pydeck)\n",
    "- [ ] Build real-time dashboard with index charts\n",
    "- [ ] Add intersection comparison tool\n",
    "- [ ] Implement time-series plots and trend analysis\n",
    "- [ ] Add download functionality for reports\n",
    "\n",
    "**Infrastructure**\n",
    "\n",
    "- [ ] Deploy to Google Cloud Run (auto-scaling)\n",
    "- [ ] Set up Cloud SQL PostgreSQL with PostGIS\n",
    "- [ ] Configure VPC for secure Trino access\n",
    "- [ ] Implement CI/CD pipeline (GitHub Actions)\n",
    "- [ ] Set up monitoring and alerting\n",
    "\n",
    "### Calibration & Tuning\n",
    "\n",
    "**Normalization Constants**: Recalibrate monthly using rolling 3-6 month window\n",
    "\n",
    "**Empirical Bayes k parameter**: Tune via cross-validation\n",
    "\n",
    "- Lower k (e.g., 20) → more responsive to recent data\n",
    "- Higher k (e.g., 100) → more stable, less noisy\n",
    "\n",
    "**Formula Weights**: Consider intersection-specific tuning\n",
    "\n",
    "- High-speed corridors → increase speed variance weight\n",
    "- School zones → increase VRU component weight\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "1. **Predictive Modeling**: Train ML model to forecast next-interval risk\n",
    "2. **Weather Integration**: Incorporate precipitation, visibility data\n",
    "3. **Event Detection**: Real-time alerting for extreme index values\n",
    "4. **Mobile App**: Push notifications for high-risk conditions\n",
    "5. **Policy Evaluation**: Before/after analysis for interventions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wby6llsrwk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VIRGINIA TRANSPORTATION SAFETY INDEX - END-TO-END WORKFLOW\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Date range: 2025-11-04 to 2025-11-11 (7 days)\n",
      "  Target intersection: ALL\n",
      "  Export results: True\n",
      "  Create visualizations: True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[1/7] Assessing data quality...\n",
      "Open the following URL in browser for the external authentication:\n",
      "https://smart-cities-trino.pre-prod.cloud.vtti.vt.edu/oauth2/token/initiate/b692cf04b024145445523d09438f246245a77ff5f4d1e52978f7e7edd69c8bc1\n",
      "  ✓ Quality assessment complete\n",
      "\n",
      "[2/7] Collecting historical baseline data...\n",
      "Error collecting baseline events: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 14:47: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234113_00132_tsjew)\n",
      "Error collecting exposure metrics: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 13:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234113_00133_tsjew)\n",
      "  ✓ Baseline: 0 events, 0 vehicle counts, 0 VRU counts\n",
      "\n",
      "[3/7] Engineering features from BSM, PSM, and safety events...\n",
      "Error collecting BSM features: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 15:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234114_00134_tsjew)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\djjay\\AppData\\Local\\Temp\\ipykernel_161612\\2768779151.py\", line 64, in collect_bsm_features\n",
      "    cur.execute(query)\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\dbapi.py\", line 640, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 909, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "                         ^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 929, in fetch\n",
      "    status = self._request.process(response)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 698, in process\n",
      "    raise self._process_error(response[\"error\"], response.get(\"id\"))\n",
      "trino.exceptions.TrinoUserError: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 15:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234114_00134_tsjew)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error collecting PSM features: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 16:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234114_00135_tsjew)\n",
      "Error collecting baseline events: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 14:47: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234114_00136_tsjew)\n",
      "⚠ No safety events to aggregate\n",
      "  ✓ Features: 0 BSM intervals, 0 PSM intervals, 0 event intervals\n",
      "\n",
      "[4/7] Creating master feature table...\n",
      "Building master feature table...\n",
      "⚠ ERROR: No BSM features available - cannot build master table\n",
      "  ✗ Error: Master table creation failed: 'intersection'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\djjay\\AppData\\Local\\Temp\\ipykernel_161612\\3809972926.py\", line 62, in collect_psm_features\n",
      "    cur.execute(query)\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\dbapi.py\", line 640, in execute\n",
      "    self._iterator = iter(self._query.execute())\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 909, in execute\n",
      "    self._result.rows += self.fetch()\n",
      "                         ^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 929, in fetch\n",
      "    status = self._request.process(response)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py\", line 698, in process\n",
      "    raise self._process_error(response[\"error\"], response.get(\"id\"))\n",
      "trino.exceptions.TrinoUserError: TrinoUserError(type=USER_ERROR, name=SYNTAX_ERROR, message=\"line 16:57: Invalid numeric literal: 9999999999999999999\", query_id=20251111_234114_00135_tsjew)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'intersection'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    126\u001b[39m     master_features_wf = create_master_feature_table(\n\u001b[32m    127\u001b[39m         bsm_features=bsm_features_wf,\n\u001b[32m    128\u001b[39m         psm_features=psm_features_wf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m         vru_counts=vru_counts_wf\n\u001b[32m    132\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Master table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(master_features_wf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 15-minute intervals across \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmaster_features_wf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mintersection\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m intersections\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✗ Error: Master table creation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'intersection'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE END-TO-END WORKFLOW: Safety Index Computation\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATE_RANGE_DAYS = 7              # Number of days to analyze\n",
    "# None = all intersections, or specify like \"glebe-potomac\"\n",
    "TARGET_INTERSECTION = None\n",
    "EXPORT_RESULTS = True            # Set to False to skip CSV export\n",
    "EXPORT_PATH = None               # Auto-generate if None\n",
    "CREATE_VISUALIZATIONS = True     # Set to False to skip plots\n",
    "\n",
    "# Time range\n",
    "end_dt = datetime.now()\n",
    "start_dt = end_dt - timedelta(days=DATE_RANGE_DAYS)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VIRGINIA TRANSPORTATION SAFETY INDEX - END-TO-END WORKFLOW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(\n",
    "    f\"  Date range: {start_dt.date()} to {end_dt.date()} ({DATE_RANGE_DAYS} days)\")\n",
    "print(f\"  Target intersection: {TARGET_INTERSECTION or 'ALL'}\")\n",
    "print(f\"  Export results: {EXPORT_RESULTS}\")\n",
    "print(f\"  Create visualizations: {CREATE_VISUALIZATIONS}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: DATA QUALITY ASSESSMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/7] Assessing data quality...\")\n",
    "\n",
    "try:\n",
    "    tables_to_check = [\n",
    "        ('alexandria', 'bsm', 'publish_timestamp'),\n",
    "        ('alexandria', 'psm', 'publish_timestamp'),\n",
    "        ('alexandria', 'safety-event', 'time_at_site'),\n",
    "        ('alexandria', 'vehicle-count', 'publish_timestamp'),\n",
    "        ('alexandria', 'vru-count', 'publish_timestamp'),\n",
    "    ]\n",
    "\n",
    "    quality_results = []\n",
    "    for schema, table, ts_col in tables_to_check:\n",
    "        result = check_table_coverage(schema, table, ts_col)\n",
    "        quality_results.append(result)\n",
    "\n",
    "    print(f\"  ✓ Quality assessment complete\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Warning: Quality assessment failed: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: HISTORICAL BASELINE CONSTRUCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/7] Collecting historical baseline data...\")\n",
    "\n",
    "try:\n",
    "    # Collect severity-weighted safety events\n",
    "    baseline_events_wf = collect_baseline_events(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    # Collect exposure metrics\n",
    "    vehicle_counts_wf, vru_counts_wf = collect_exposure_metrics(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Baseline: {len(baseline_events_wf)} events, {len(vehicle_counts_wf)} vehicle counts, {len(vru_counts_wf)} VRU counts\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Baseline collection failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/7] Engineering features from BSM, PSM, and safety events...\")\n",
    "\n",
    "try:\n",
    "    # BSM-derived vehicle features\n",
    "    bsm_features_wf = collect_bsm_features(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    # PSM-derived VRU features\n",
    "    psm_features_wf = collect_psm_features(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    # Aggregate safety events\n",
    "    aggregated_events_wf = aggregate_safety_events(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    print(f\"  ✓ Features: {len(bsm_features_wf)} BSM intervals, {len(psm_features_wf)} PSM intervals, {len(aggregated_events_wf)} event intervals\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Feature engineering failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4: MASTER FEATURE TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/7] Creating master feature table...\")\n",
    "\n",
    "try:\n",
    "    master_features_wf = create_master_feature_table(\n",
    "        bsm_features=bsm_features_wf,\n",
    "        psm_features=psm_features_wf,\n",
    "        aggregated_events=aggregated_events_wf,\n",
    "        vehicle_counts=vehicle_counts_wf,\n",
    "        vru_counts=vru_counts_wf\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Master table: {len(master_features_wf)} 15-minute intervals across {master_features_wf['intersection'].nunique()} intersections\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Master table creation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5: NORMALIZATION CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/7] Computing normalization constants...\")\n",
    "\n",
    "try:\n",
    "    norm_constants_wf = compute_normalization_constants(master_features_wf)\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Constants: I_max={norm_constants_wf['I_max']:.1f}, V_max={norm_constants_wf['V_max']:.1f}, σ_max={norm_constants_wf['sigma_max']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Normalization computation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 6: SAFETY INDEX COMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/7] Computing safety indices...\")\n",
    "\n",
    "try:\n",
    "    indices_wf = compute_safety_indices(master_features_wf, norm_constants_wf)\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Indices computed: Mean Combined Index = {indices_wf['Combined_Index'].mean():.1f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Index computation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 7: EMPIRICAL BAYES STABILIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/7] Applying Empirical Bayes stabilization...\")\n",
    "\n",
    "try:\n",
    "    indices_eb_wf = apply_empirical_bayes(\n",
    "        indices_wf, baseline_events_wf, lambda_param=None)\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ EB adjustment applied: Mean EB Combined Index = {indices_eb_wf['Combined_Index_EB'].mean():.1f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Empirical Bayes adjustment failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Data Coverage:\")\n",
    "print(f\"  Total 15-minute intervals: {len(indices_eb_wf)}\")\n",
    "print(f\"  Intersections analyzed: {indices_eb_wf['intersection'].nunique()}\")\n",
    "print(\n",
    "    f\"  Date range: {indices_eb_wf['time_15min'].min()} to {indices_eb_wf['time_15min'].max()}\")\n",
    "\n",
    "print(f\"\\n🎯 Safety Index Statistics (EB-Adjusted):\")\n",
    "print(f\"  Combined Index:\")\n",
    "print(f\"    Mean: {indices_eb_wf['Combined_Index_EB'].mean():.2f}\")\n",
    "print(f\"    Std:  {indices_eb_wf['Combined_Index_EB'].std():.2f}\")\n",
    "print(f\"    Min:  {indices_eb_wf['Combined_Index_EB'].min():.2f}\")\n",
    "print(f\"    Max:  {indices_eb_wf['Combined_Index_EB'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n  VRU Index:\")\n",
    "print(f\"    Mean: {indices_eb_wf['VRU_Index_EB'].mean():.2f}\")\n",
    "print(f\"    Max:  {indices_eb_wf['VRU_Index_EB'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n  Vehicle Index:\")\n",
    "print(f\"    Mean: {indices_eb_wf['Vehicle_Index_EB'].mean():.2f}\")\n",
    "print(f\"    Max:  {indices_eb_wf['Vehicle_Index_EB'].max():.2f}\")\n",
    "\n",
    "# Top 10 highest risk intervals\n",
    "print(f\"\\n⚠️  TOP 10 HIGHEST RISK INTERVALS:\")\n",
    "top_risk = indices_eb_wf.nlargest(10, 'Combined_Index_EB')[\n",
    "    ['intersection', 'time_15min', 'Combined_Index_EB',\n",
    "        'VRU_Index_EB', 'Vehicle_Index_EB', 'I_VRU', 'vehicle_count']\n",
    "]\n",
    "print(top_risk.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "if EXPORT_RESULTS:\n",
    "    print(f\"\\n📁 Exporting results...\")\n",
    "\n",
    "    if EXPORT_PATH is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        EXPORT_PATH = f'safety_indices_{timestamp}.csv'\n",
    "\n",
    "    # Select columns for export\n",
    "    export_cols = [\n",
    "        'intersection', 'time_15min', 'hour_of_day', 'day_of_week',\n",
    "        'Combined_Index_EB', 'VRU_Index_EB', 'Vehicle_Index_EB',\n",
    "        'Combined_Index', 'VRU_Index', 'Vehicle_Index',\n",
    "        'vehicle_count', 'avg_speed', 'speed_variance', 'hard_braking_count',\n",
    "        'I_VRU', 'total_event_count', 'vru_event_count', 'vehicle_event_count',\n",
    "        'vru_volume', 'vehicle_volume'\n",
    "    ]\n",
    "\n",
    "    # Only include columns that exist\n",
    "    export_cols = [c for c in export_cols if c in indices_eb_wf.columns]\n",
    "\n",
    "    indices_eb_wf[export_cols].to_csv(EXPORT_PATH, index=False)\n",
    "    print(f\"  ✓ Results exported to: {EXPORT_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "if CREATE_VISUALIZATIONS:\n",
    "    print(f\"\\n📈 Creating visualizations...\")\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        # 1. Time series plot\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "        # If analyzing specific intersection, show detailed view\n",
    "        if TARGET_INTERSECTION:\n",
    "            data = indices_eb_wf[indices_eb_wf['intersection']\n",
    "                                 == TARGET_INTERSECTION].sort_values('time_15min')\n",
    "        else:\n",
    "            # Show average across all intersections\n",
    "            data = indices_eb_wf.groupby('time_15min').agg({\n",
    "                'Combined_Index_EB': 'mean',\n",
    "                'VRU_Index_EB': 'mean',\n",
    "                'Vehicle_Index_EB': 'mean'\n",
    "            }).reset_index().sort_values('time_15min')\n",
    "\n",
    "        # Combined Index\n",
    "        axes[0].plot(data['time_15min'], data['Combined_Index_EB'],\n",
    "                     linewidth=2, color='purple')\n",
    "        axes[0].axhline(y=50, color='orange', linestyle='--',\n",
    "                        alpha=0.5, label='Medium Risk')\n",
    "        axes[0].axhline(y=70, color='red', linestyle='--',\n",
    "                        alpha=0.5, label='High Risk')\n",
    "        axes[0].set_ylabel('Combined Index')\n",
    "        axes[0].set_title(\n",
    "            f'Safety Indices Over Time - {TARGET_INTERSECTION or \"All Intersections (Average)\"}')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend()\n",
    "\n",
    "        # VRU Index\n",
    "        axes[1].plot(data['time_15min'], data['VRU_Index_EB'],\n",
    "                     linewidth=2, color='blue')\n",
    "        axes[1].axhline(y=50, color='orange', linestyle='--', alpha=0.5)\n",
    "        axes[1].axhline(y=70, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1].set_ylabel('VRU Index')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Vehicle Index\n",
    "        axes[2].plot(data['time_15min'], data['Vehicle_Index_EB'],\n",
    "                     linewidth=2, color='green')\n",
    "        axes[2].axhline(y=50, color='orange', linestyle='--', alpha=0.5)\n",
    "        axes[2].axhline(y=70, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[2].set_ylabel('Vehicle Index')\n",
    "        axes[2].set_xlabel('Time')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 2. Heatmap: Average index by hour and day of week\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        pivot = indices_eb_wf.groupby(['hour_of_day', 'day_of_week'])[\n",
    "            'Combined_Index_EB'].mean().reset_index()\n",
    "        heatmap_data = pivot.pivot(\n",
    "            index='hour_of_day', columns='day_of_week', values='Combined_Index_EB')\n",
    "\n",
    "        sns.heatmap(heatmap_data, cmap='RdYlGn_r', annot=True, fmt='.1f',\n",
    "                    xticklabels=['Mon', 'Tue', 'Wed',\n",
    "                                 'Thu', 'Fri', 'Sat', 'Sun'],\n",
    "                    yticklabels=range(24), cbar_kws={'label': 'Combined Index'})\n",
    "        plt.title('Average Safety Index by Hour and Day of Week')\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.ylabel('Hour of Day')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 3. Distribution histogram\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "        for idx, (col, ax, color) in enumerate(zip(\n",
    "            ['Combined_Index_EB', 'VRU_Index_EB', 'Vehicle_Index_EB'],\n",
    "            axes,\n",
    "            ['purple', 'blue', 'green']\n",
    "        )):\n",
    "            ax.hist(indices_eb_wf[col], bins=30,\n",
    "                    edgecolor='black', alpha=0.7, color=color)\n",
    "            mean_val = indices_eb_wf[col].mean()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--',\n",
    "                       linewidth=2, label=f'Mean: {mean_val:.1f}')\n",
    "            ax.set_xlabel('Index Value')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(col.replace('_EB', '').replace('_', ' '))\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"  ✓ Visualizations created\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(f\"  ⚠ Matplotlib not available - skipping visualizations\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Visualization error: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ WORKFLOW COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\n",
    "    f\"  1. Review the exported CSV: {EXPORT_PATH if EXPORT_RESULTS else '(export disabled)'}\")\n",
    "print(f\"  2. Analyze high-risk intervals for intervention opportunities\")\n",
    "print(f\"  3. Compare indices across intersections to prioritize resources\")\n",
    "print(f\"  4. Use for real-time monitoring or policy evaluation\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0pb95w4gl7i",
   "metadata": {},
   "source": [
    "## 🚀 Complete End-to-End Workflow\n",
    "\n",
    "Run this cell to execute the entire safety index computation pipeline with one click.\n",
    "\n",
    "**Configurable parameters:**\n",
    "\n",
    "- `DATE_RANGE_DAYS`: Number of days to analyze (default: 7)\n",
    "- `TARGET_INTERSECTION`: Specific intersection or `None` for all (default: None)\n",
    "- `EXPORT_PATH`: Where to save results CSV (default: auto-generated filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014xkldj59x0j",
   "metadata": {},
   "source": [
    "## Phase 7: Empirical Bayes Stabilization\n",
    "\n",
    "Apply Empirical Bayes adjustment to reduce noise in low-sample-size situations.\n",
    "\n",
    "**Formula:** `Adjusted_Index = λ × Raw_Index + (1-λ) × Baseline_Index`\n",
    "\n",
    "Where λ depends on sample size (more data → higher λ → trust raw index more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a24sfh9s7",
   "metadata": {},
   "source": [
    "## Phase 5: Normalization Constants\n",
    "\n",
    "Compute all normalization constants required for the Safety Index formulas.\n",
    "\n",
    "These constants scale raw metrics to [0, 1] range before combining into the final index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dys0z0tzlxm",
   "metadata": {},
   "source": [
    "## Phase 4: Master Feature Table Construction\n",
    "\n",
    "Join all feature sources into a single unified table aligned on 15-minute intervals.\n",
    "\n",
    "**This becomes the core data model in PostgreSQL for the production system.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urnn6km9vok",
   "metadata": {},
   "source": [
    "## Phase 3: Feature Engineering\n",
    "\n",
    "Extract and aggregate features from BSM, PSM, and safety event data at 15-minute intervals.\n",
    "\n",
    "**This becomes the core data processing pipeline in the FastAPI backend.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sanjfeusrfs",
   "metadata": {},
   "source": [
    "## Phase 2: Historical Baseline Construction\n",
    "\n",
    "Collect severity-weighted crash data to establish baseline risk scores for Empirical Bayes stabilization.\n",
    "\n",
    "**Note**: This prototype will become the `build_baseline()` function in the FastAPI backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7xzmlervelo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: DATA QUALITY ASSESSMENT\n",
      "================================================================================\n",
      "\n",
      "Checking alexandria.bsm...\n",
      "  ✓ Records: 96,430,028\n",
      "  ✓ Intersections: 2\n",
      "  ✓ Date range: 1756826633215000 to 1762883071654000\n",
      "\n",
      "Checking alexandria.psm...\n",
      "  ✓ Records: 37,846,848\n",
      "  ✓ Intersections: 2\n",
      "  ✓ Date range: 1756905621342000 to 1762882859151000\n",
      "\n",
      "Checking alexandria.safety-event...\n",
      "  ✓ Records: 2,679\n",
      "  ✓ Intersections: 1\n",
      "  ✓ Date range: 1753195503825000 to 1762856764156000\n",
      "\n",
      "Checking alexandria.vehicle-count...\n",
      "  ✓ Records: 47,099\n",
      "  ✓ Intersections: 1\n",
      "  ✓ Date range: 1753214413884000 to 1762880404545000\n",
      "\n",
      "Checking alexandria.vru-count...\n",
      "  ✓ Records: 17,567\n",
      "  ✓ Intersections: 1\n",
      "  ✓ Date range: 1753214414851000 to 1762880404360000\n",
      "\n",
      "================================================================================\n",
      "COVERAGE SUMMARY\n",
      "================================================================================\n",
      "    schema         table  total_records  num_intersections         earliest           latest status\n",
      "alexandria           bsm       96430028                  2 1756826633215000 1762883071654000     OK\n",
      "alexandria           psm       37846848                  2 1756905621342000 1762882859151000     OK\n",
      "alexandria  safety-event           2679                  1 1753195503825000 1762856764156000     OK\n",
      "alexandria vehicle-count          47099                  1 1753214413884000 1762880404545000     OK\n",
      "alexandria     vru-count          17567                  1 1753214414851000 1762880404360000     OK\n"
     ]
    }
   ],
   "source": [
    "# Phase 1.1: Check temporal coverage and data availability for each key table\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Function to safely check table coverage\n",
    "\n",
    "\n",
    "def check_table_coverage(schema, table, timestamp_col='publish_timestamp'):\n",
    "    \"\"\"\n",
    "    Check temporal coverage and record counts for a table.\n",
    "    Handles both bigint timestamps and actual timestamp columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try with publish_timestamp first\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT intersection) as num_intersections,\n",
    "            MIN({timestamp_col}) as earliest_ts,\n",
    "            MAX({timestamp_col}) as latest_ts\n",
    "        FROM {schema}.\"{table}\"\n",
    "        WHERE {timestamp_col} > 0 AND {timestamp_col} < 9999999999999999\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "\n",
    "        if result and result[0] > 0:\n",
    "            # Convert timestamps if they're bigints\n",
    "            try:\n",
    "                earliest = datetime.fromtimestamp(\n",
    "                    result[2] / 1000) if result[2] and result[2] > 999999999999 else result[2]\n",
    "                latest = datetime.fromtimestamp(\n",
    "                    result[3] / 1000) if result[3] and result[3] > 999999999999 else result[3]\n",
    "            except:\n",
    "                earliest = result[2]\n",
    "                latest = result[3]\n",
    "\n",
    "            return {\n",
    "                'schema': schema,\n",
    "                'table': table,\n",
    "                'total_records': result[0],\n",
    "                'num_intersections': result[1],\n",
    "                'earliest': earliest,\n",
    "                'latest': latest,\n",
    "                'status': 'OK'\n",
    "            }\n",
    "        else:\n",
    "            return {'schema': schema, 'table': table, 'status': 'EMPTY'}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'schema': schema, 'table': table, 'status': f'ERROR: {str(e)[:100]}'}\n",
    "\n",
    "\n",
    "# Check all key tables\n",
    "tables_to_check = [\n",
    "    ('alexandria', 'bsm', 'publish_timestamp'),\n",
    "    ('alexandria', 'psm', 'publish_timestamp'),\n",
    "    # Note: using time_at_site not publish_timestamp\n",
    "    ('alexandria', 'safety-event', 'time_at_site'),\n",
    "    ('alexandria', 'vehicle-count', 'publish_timestamp'),\n",
    "    ('alexandria', 'vru-count', 'publish_timestamp'),\n",
    "]\n",
    "\n",
    "coverage_results = []\n",
    "for schema, table, ts_col in tables_to_check:\n",
    "    print(f\"\\nChecking {schema}.{table}...\")\n",
    "    result = check_table_coverage(schema, table, ts_col)\n",
    "    coverage_results.append(result)\n",
    "\n",
    "    if result['status'] == 'OK':\n",
    "        print(f\"  ✓ Records: {result['total_records']:,}\")\n",
    "        print(f\"  ✓ Intersections: {result['num_intersections']}\")\n",
    "        print(f\"  ✓ Date range: {result['earliest']} to {result['latest']}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Status: {result['status']}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "coverage_df = pd.DataFrame(coverage_results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COVERAGE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(coverage_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ly2weq7ep8f",
   "metadata": {},
   "source": [
    "## Phase 1: Data Quality Assessment\n",
    "\n",
    "Before collecting data, we need to assess what's actually available and identify any quality issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fp7yfnertgs",
   "metadata": {},
   "source": [
    "# DATA COLLECTION FOR SAFETY INDEX\n",
    "\n",
    "This section implements the systematic data collection workflow for the Virginia Transportation Safety Index (VTSI).\n",
    "\n",
    "## Project Requirements\n",
    "\n",
    "Based on the checkpoint document, we need to compute three indices refreshed every 15 minutes:\n",
    "\n",
    "1. **VRU Safety Index**: Risk to pedestrians and cyclists\n",
    "2. **Vehicle Safety Index**: Risk of vehicle-vehicle collisions\n",
    "3. **Combined Safety Index**: Weighted combination of both\n",
    "\n",
    "## Data Collection Phases\n",
    "\n",
    "1. **Phase 1**: Data source verification and quality assessment\n",
    "2. **Phase 2**: Historical baseline construction (severity-weighted crashes)\n",
    "3. **Phase 3**: Feature engineering (BSM, PSM, safety events, counts)\n",
    "4. **Phase 4**: Safety index computation with normalization\n",
    "5. **Phase 5**: Validation and calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwkhhf8mcj6",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Notebook vs. Production Architecture\n",
    "\n",
    "### This Notebook's Purpose\n",
    "\n",
    "This notebook is for **development, prototyping, and demonstration** purposes. It allows us to:\n",
    "\n",
    "- Validate data collection queries and logic\n",
    "- Explore data quality issues\n",
    "- Test index computation formulas\n",
    "- Demonstrate the methodology to stakeholders\n",
    "\n",
    "### Production Architecture (Eventual Deployment)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Production System                        │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                               │\n",
    "│  ┌──────────────┐      ┌────────────────────────────────┐  │\n",
    "│  │ Cloud        │      │   FastAPI Backend              │  │\n",
    "│  │ Scheduler    │─────>│   (Google Cloud Run)           │  │\n",
    "│  │ (Every 15min)│      │                                 │  │\n",
    "│  └──────────────┘      │   - Data Collection Service    │  │\n",
    "│                        │   - Index Computation Service   │  │\n",
    "│                        │   - Feature Engineering         │  │\n",
    "│                        └──────────┬─────────────────────┘  │\n",
    "│                                   │                          │\n",
    "│                                   ↓                          │\n",
    "│                        ┌──────────────────────────┐         │\n",
    "│                        │  PostgreSQL + PostGIS    │         │\n",
    "│                        │  (Google Cloud SQL)      │         │\n",
    "│                        │                          │         │\n",
    "│                        │  - safety_index_features │         │\n",
    "│                        │  - safety_index_computed │         │\n",
    "│                        │  - intersection_baselines│         │\n",
    "│                        └──────────┬───────────────┘         │\n",
    "│                                   │                          │\n",
    "│                                   ↓                          │\n",
    "│                        ┌──────────────────────────┐         │\n",
    "│                        │  Streamlit Frontend      │         │\n",
    "│                        │  (Google Cloud Run)      │         │\n",
    "│                        │                          │         │\n",
    "│                        │  - Interactive Map       │         │\n",
    "│                        │  - Real-time Dashboard   │         │\n",
    "│                        │  - VRU/Vehicle Views     │         │\n",
    "│                        └──────────────────────────┘         │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### API Endpoints (To Be Implemented)\n",
    "\n",
    "- `POST /api/collect-data` - Triggered every 15 minutes to collect and process new data\n",
    "- `GET /api/safety-index?intersection=X&time=Y` - Query computed indices\n",
    "- `GET /api/features?intersection=X` - Get raw features for debugging\n",
    "- `GET /api/intersections` - List available intersections\n",
    "\n",
    "### Event-Driven Data Collection Flow\n",
    "\n",
    "1. **Cloud Scheduler** triggers `/api/collect-data` every 15 minutes\n",
    "2. **FastAPI service** queries Trino API for latest BSM, PSM, events, counts\n",
    "3. **Feature engineering** computes aggregates and derives metrics\n",
    "4. **Index computation** applies formulas and stores results in PostgreSQL\n",
    "5. **Frontend** queries PostgreSQL for visualization (not Trino directly)\n",
    "\n",
    "**Note**: The code developed in this notebook will be refactored into the FastAPI backend services.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
