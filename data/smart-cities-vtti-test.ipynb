{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68cafe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trino in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.336.0)\n",
      "Requirement already satisfied: pandas in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: lz4 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (4.4.5)\n",
      "Requirement already satisfied: orjson>=3.11.0 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (3.11.4)\n",
      "Requirement already satisfied: python-dateutil in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: requests>=2.32.4 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tzlocal in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (5.3.1)\n",
      "Requirement already satisfied: zstandard in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from trino->-r requirements.txt (line 1)) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from python-dateutil->trino->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.32.4->trino->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.32.4->trino->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.32.4->trino->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryan/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.32.4->trino->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be8c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trino import dbapi\n",
    "from trino.auth import OAuth2Authentication\n",
    "\n",
    "conn = dbapi.connect(\n",
    "    host=\"smart-cities-trino.pre-prod.cloud.vtti.vt.edu\",\n",
    "    port=443,\n",
    "    http_scheme=\"https\",\n",
    "    auth=OAuth2Authentication(),   # <-- this is the right one\n",
    "    catalog=\"smartcities_iceberg\",  # optional default\n",
    "    # schema=\"...\",                # optional default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366f8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open the following URL in browser for the external authentication:\n",
      "https://smart-cities-trino.pre-prod.cloud.vtti.vt.edu/oauth2/token/initiate/2f700dfab49b71babd3aca4004865125447eddff533becdcbfca2e3bcc90c7c0\n"
     ]
    },
    {
     "ename": "TrinoAuthError",
     "evalue": "Exceeded max attempts while getting the token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTrinoAuthError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW SCHEMAS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur\u001b[38;5;241m.\u001b[39mfetchall())\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/dbapi.py:640\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, params)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query \u001b[38;5;241m=\u001b[39m trino\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mTrinoQuery(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, query\u001b[38;5;241m=\u001b[39moperation,\n\u001b[1;32m    639\u001b[0m                                           legacy_primitive_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy_primitive_types)\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/client.py:892\u001b[0m, in \u001b[0;36mTrinoQuery.execute\u001b[0;34m(self, additional_http_headers)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrinoUserError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery has been cancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_id)\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 892\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_http_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m trino\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTrinoConnectionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to execute: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/client.py:643\u001b[0m, in \u001b[0;36mTrinoRequest.post\u001b[0;34m(self, sql, additional_http_headers)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# Update the request headers with the additional_http_headers\u001b[39;00m\n\u001b[1;32m    641\u001b[0m http_headers\u001b[38;5;241m.\u001b[39mupdate(additional_http_headers \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m--> 643\u001b[0m http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatement_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROXIES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m http_response\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/client.py:1024\u001b[0m, in \u001b[0;36m_retry_with.<locals>.wrapper.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed after \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m attempts\u001b[39m\u001b[38;5;124m\"\u001b[39m, attempt)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/client.py:1006\u001b[0m, in \u001b[0;36m_retry_with.<locals>.wrapper.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_attempts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(guard(result) \u001b[38;5;28;01mfor\u001b[39;00m guard \u001b[38;5;129;01min\u001b[39;00m conditions):\n\u001b[1;32m   1008\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry-After\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mheaders:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/requests/sessions.py:710\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m r\u001b[38;5;241m.\u001b[39melapsed \u001b[38;5;241m=\u001b[39m timedelta(seconds\u001b[38;5;241m=\u001b[39melapsed)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# Response manipulation hooks\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Persist cookies\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mhistory:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# If the hooks create history then we want those cookies too\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/requests/hooks.py:30\u001b[0m, in \u001b[0;36mdispatch_hook\u001b[0;34m(key, hooks, hook_data, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m [hooks]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[0;32m---> 30\u001b[0m     _hook_data \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _hook_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         hook_data \u001b[38;5;241m=\u001b[39m _hook_data\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/auth.py:453\u001b[0m, in \u001b[0;36m_OAuth2TokenBearer._authenticate\u001b[0;34m(self, response, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acquired:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;66;03m# Lock is acquired, attempt the OAuth2 flow\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attempt_oauth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inside_oauth_attempt_blocker\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/auth.py:493\u001b[0m, in \u001b[0;36m_OAuth2TokenBearer._attempt_oauth\u001b[0;34m(self, response, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    491\u001b[0m response\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 493\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_server\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m request \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    496\u001b[0m host \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_host(request\u001b[38;5;241m.\u001b[39murl)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/trino/auth.py:540\u001b[0m, in \u001b[0;36m_OAuth2TokenBearer._get_token\u001b[0;34m(self, token_server, response, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrinoAuthError(\n\u001b[1;32m    536\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while getting the token response \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrinoAuthError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExceeded max attempts while getting the token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTrinoAuthError\u001b[0m: Exceeded max attempts while getting the token"
     ]
    }
   ],
   "source": [
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SHOW SCHEMAS\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['smartcities_iceberg'], ['system']]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SHOW CATALOGS\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbca743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alexandria'], ['cci'], ['falls-church'], ['information_schema'], ['smart_cities_test'], ['system'], ['tables'], ['vtti']]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SHOW SCHEMAS FROM smartcities_iceberg\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05016733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alexandria', 'bsm'], ['alexandria', 'psm'], ['alexandria', 'safety-event'], ['alexandria', 'speed-distribution'], ['alexandria', 'vehicle-count'], ['alexandria', 'vru-count'], ['cci', 'bsm'], ['falls-church', 'hiresdata'], ['falls-church', 'maple_washington'], ['falls-church', 'mediantraveltimes'], ['falls-church', 'old_hiresdata'], ['falls-church', 'old_mediantraveltimes'], ['falls-church', 'old_priority_requests'], ['falls-church', 'old_safety_conflicts'], ['falls-church', 'old_safety_pedcompliance'], ['falls-church', 'old_safety_redlightrunners'], ['falls-church', 'old_safety_simpledelay'], ['falls-church', 'old_tmc'], ['falls-church', 'old_tmc_crosswalk'], ['falls-church', 'old_tmc_lanes']]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT table_schema, table_name\n",
    "FROM smartcities_iceberg.information_schema.tables\n",
    "ORDER BY table_schema, table_name\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wye8zdjkqxq",
   "metadata": {},
   "source": [
    "## Best Practices for Querying\n",
    "\n",
    "### 1. Always Verify Column Names First\n",
    "\n",
    "**CRITICAL:** Column names may not be what you expect! Run the schema verification cells in this notebook to see actual field names.\n",
    "\n",
    "```python\n",
    "# See actual columns in a table\n",
    "cur.execute(\"SELECT * FROM smartcities_iceberg.alexandria.bsm LIMIT 1\")\n",
    "actual_columns = [desc[0] for desc in cur.description]\n",
    "print(actual_columns)\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "- ❌ `vehicle_id` may not exist\n",
    "- ❌ `latitude`, `longitude` → Use `lat`, `lon` instead\n",
    "- ✅ Always check the schema verification output first!\n",
    "\n",
    "### 2. Use Cursor Method (Not pd.read_sql)\n",
    "\n",
    "To avoid pandas SQLAlchemy warnings, use the cursor method:\n",
    "\n",
    "```python\n",
    "# Good: Use cursor method\n",
    "cur.execute(\"SELECT * FROM table LIMIT 5\")\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Avoid: pd.read_sql() triggers warning\n",
    "# df = pd.read_sql(\"SELECT * FROM table\", conn)\n",
    "```\n",
    "\n",
    "### 3. Start with SELECT \\*\n",
    "\n",
    "When exploring or unsure about column names, use `SELECT *` to get all columns:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    from_unixtime(publish_timestamp / 1000000) as timestamp,\n",
    "    *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "LIMIT 10\n",
    "```\n",
    "\n",
    "Then filter columns in pandas:\n",
    "\n",
    "```python\n",
    "df_filtered = df[['timestamp', 'lat', 'lon', 'speed']]\n",
    "```\n",
    "\n",
    "### 4. Handle publish_timestamp Correctly\n",
    "\n",
    "`publish_timestamp` is a **bigint** (microseconds since Unix epoch), NOT a timestamp type.\n",
    "\n",
    "**Convert to readable timestamp:**\n",
    "\n",
    "```sql\n",
    "from_unixtime(publish_timestamp / 1000000) as timestamp\n",
    "```\n",
    "\n",
    "**Filter by time:**\n",
    "\n",
    "```sql\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '24' hour) * 100000000\n",
    "```\n",
    "\n",
    "**Use in aggregations:**\n",
    "\n",
    "```sql\n",
    "date_trunc('hour', from_unixtime(publish_timestamp / 1000000))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s0nvwsudvfk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tables found: 52\n",
      "\n",
      "           Schema                      Table\n",
      "       alexandria                        bsm\n",
      "       alexandria                        psm\n",
      "       alexandria               safety-event\n",
      "       alexandria         speed-distribution\n",
      "       alexandria              vehicle-count\n",
      "       alexandria                  vru-count\n",
      "              cci                        bsm\n",
      "     falls-church                  hiresdata\n",
      "     falls-church           maple_washington\n",
      "     falls-church          mediantraveltimes\n",
      "     falls-church              old_hiresdata\n",
      "     falls-church      old_mediantraveltimes\n",
      "     falls-church      old_priority_requests\n",
      "     falls-church       old_safety_conflicts\n",
      "     falls-church   old_safety_pedcompliance\n",
      "     falls-church old_safety_redlightrunners\n",
      "     falls-church     old_safety_simpledelay\n",
      "     falls-church                    old_tmc\n",
      "     falls-church          old_tmc_crosswalk\n",
      "     falls-church              old_tmc_lanes\n",
      "     falls-church           safety-conflicts\n",
      "     falls-church               safety-event\n",
      "     falls-church              safety-event \n",
      "     falls-church     safety-redlightrunners\n",
      "     falls-church         safety-simpledelay\n",
      "     falls-church         speed-distribution\n",
      "     falls-church        speed-distribution \n",
      "     falls-church                        tmc\n",
      "     falls-church              tmc-crosswalk\n",
      "     falls-church                  tmc-lanes\n",
      "     falls-church              vehicle-count\n",
      "     falls-church             vehicle-count \n",
      "     falls-church                  vru-count\n",
      "     falls-church                 vru-count \n",
      "smart_cities_test                   bsm_data\n",
      "smart_cities_test            bsm_data_struct\n",
      "smart_cities_test              bsm_wide_data\n",
      "smart_cities_test                   psm_data\n",
      "smart_cities_test                   rse_data\n",
      "smart_cities_test               safety-event\n",
      "smart_cities_test                  spat_data\n",
      "smart_cities_test           table_properties\n",
      "           tables                        bsm\n",
      "           tables                        psm\n",
      "             vtti                environment\n",
      "             vtti          environment-alarm\n",
      "             vtti                      light\n",
      "             vtti                light-alarm\n",
      "             vtti                      noise\n",
      "             vtti                noise-alarm\n",
      "             vtti                    traffic\n",
      "             vtti              traffic-alarm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get all tables from all schemas\n",
    "cur.execute(\"\"\"\n",
    "SELECT table_schema, table_name\n",
    "FROM smartcities_iceberg.information_schema.tables\n",
    "WHERE table_schema NOT IN ('information_schema', 'system')\n",
    "ORDER BY table_schema, table_name\n",
    "\"\"\")\n",
    "all_tables = cur.fetchall()\n",
    "\n",
    "tables_df = pd.DataFrame(all_tables, columns=['Schema', 'Table'])\n",
    "print(f\"Total tables found: {len(tables_df)}\\n\")\n",
    "print(tables_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1b64brjzm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - BSM Table\n",
      "================================================================================\n",
      "Empty DataFrame\n",
      "Columns: [Column, Data Type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Function to examine table structure\n",
    "def describe_table(schema, table):\n",
    "    cur.execute(f\"\"\"\n",
    "    SELECT column_name, data_type\n",
    "    FROM smartcities_iceberg.information_schema.columns\n",
    "    WHERE table_schema = '{schema}' AND table_name = '{table}'\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    columns = cur.fetchall()\n",
    "    return pd.DataFrame(columns, columns=['Column', 'Data Type'])\n",
    "\n",
    "\n",
    "# Let's examine some key tables from each schema\n",
    "print(\"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - BSM Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'bsm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ts384lm93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - PSM Table (Pedestrian Safety Message)\n",
      "================================================================================\n",
      "                  Column  Data Type\n",
      "0                   city    varchar\n",
      "1           intersection    varchar\n",
      "2                  table    varchar\n",
      "3      publish_timestamp     bigint\n",
      "4          location_name    varchar\n",
      "5            source_type    varchar\n",
      "6            vendor_name    varchar\n",
      "7         vendor_version    varchar\n",
      "8                   misc    varchar\n",
      "9                msg_cnt    integer\n",
      "10            basic_type    integer\n",
      "11                    id     bigint\n",
      "12              sec_mark    integer\n",
      "13                   lat     double\n",
      "14                   lon     double\n",
      "15                  elev       real\n",
      "16   accuracy_semi_major       real\n",
      "17   accuracy_semi_minor       real\n",
      "18  accuracy_orientation       real\n",
      "19                 speed       real\n",
      "20               heading       real\n",
      "21             accel_lon       real\n",
      "22             accel_lat       real\n",
      "23            accel_vert       real\n",
      "24             accel_yaw       real\n",
      "25  event_responder_type    integer\n",
      "26         activity_type    integer\n",
      "27     activity_sub_type    integer\n",
      "28      original_content  varbinary\n",
      "\n",
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - Safety Event Table\n",
      "================================================================================\n",
      "               Column Data Type\n",
      "0          event_type   varchar\n",
      "1            event_id   varchar\n",
      "2        time_at_site    bigint\n",
      "3      detection_area   varchar\n",
      "4           camera_id   varchar\n",
      "5           direction   varchar\n",
      "6            movement   varchar\n",
      "7       object1_class   varchar\n",
      "8       object2_class   varchar\n",
      "9                city   varchar\n",
      "10       intersection   varchar\n",
      "11              table   varchar\n",
      "12  publish_timestamp    bigint\n",
      "\n",
      "================================================================================\n",
      "ALEXANDRIA SCHEMA - Vehicle Count Table\n",
      "================================================================================\n",
      "              Column Data Type\n",
      "0              count   integer\n",
      "1               date   varchar\n",
      "2      time_interval   varchar\n",
      "3           approach   varchar\n",
      "4              class   varchar\n",
      "5               city   varchar\n",
      "6       intersection   varchar\n",
      "7              table   varchar\n",
      "8  publish_timestamp    bigint\n",
      "9           movement   varchar\n",
      "\n",
      "================================================================================\n",
      "FALLS CHURCH SCHEMA - HiRes Data Table\n",
      "================================================================================\n",
      "               Column Data Type\n",
      "0                city   varchar\n",
      "1       data_provider   varchar\n",
      "2               table   varchar\n",
      "3     intersection_id   varchar\n",
      "4        intersection   varchar\n",
      "5   intersection_name   varchar\n",
      "6    intersection_lat    double\n",
      "7   intersection_long    double\n",
      "8   publish_timestamp    bigint\n",
      "9     event_timestamp    bigint\n",
      "10         event_code   integer\n",
      "11        event_param   integer\n",
      "12   original_content   varchar\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - PSM Table (Pedestrian Safety Message)\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'psm'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - Safety Event Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'safety-event'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALEXANDRIA SCHEMA - Vehicle Count Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('alexandria', 'vehicle-count'))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FALLS CHURCH SCHEMA - HiRes Data Table\")\n",
    "print(\"=\" * 80)\n",
    "print(describe_table('falls-church', 'hiresdata'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0s688l7vb3bo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STORING SCHEMAS FOR KEY TABLES\n",
      "================================================================================\n",
      "\n",
      "Alexandria BSM Schema:\n",
      "Empty DataFrame\n",
      "Columns: [Column, Data Type]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Alexandria PSM Schema:\n",
      "              Column Data Type\n",
      "                city   varchar\n",
      "        intersection   varchar\n",
      "               table   varchar\n",
      "   publish_timestamp    bigint\n",
      "       location_name   varchar\n",
      "         source_type   varchar\n",
      "         vendor_name   varchar\n",
      "      vendor_version   varchar\n",
      "                misc   varchar\n",
      "             msg_cnt   integer\n",
      "          basic_type   integer\n",
      "                  id    bigint\n",
      "            sec_mark   integer\n",
      "                 lat    double\n",
      "                 lon    double\n",
      "                elev      real\n",
      " accuracy_semi_major      real\n",
      " accuracy_semi_minor      real\n",
      "accuracy_orientation      real\n",
      "               speed      real\n",
      "             heading      real\n",
      "           accel_lon      real\n",
      "           accel_lat      real\n",
      "          accel_vert      real\n",
      "           accel_yaw      real\n",
      "event_responder_type   integer\n",
      "       activity_type   integer\n",
      "   activity_sub_type   integer\n",
      "    original_content varbinary\n",
      "\n",
      "\n",
      "Alexandria Safety Event Schema:\n",
      "           Column Data Type\n",
      "       event_type   varchar\n",
      "         event_id   varchar\n",
      "     time_at_site    bigint\n",
      "   detection_area   varchar\n",
      "        camera_id   varchar\n",
      "        direction   varchar\n",
      "         movement   varchar\n",
      "    object1_class   varchar\n",
      "    object2_class   varchar\n",
      "             city   varchar\n",
      "     intersection   varchar\n",
      "            table   varchar\n",
      "publish_timestamp    bigint\n",
      "\n",
      "\n",
      "Alexandria Vehicle Count Schema:\n",
      "           Column Data Type\n",
      "            count   integer\n",
      "             date   varchar\n",
      "    time_interval   varchar\n",
      "         approach   varchar\n",
      "            class   varchar\n",
      "             city   varchar\n",
      "     intersection   varchar\n",
      "            table   varchar\n",
      "publish_timestamp    bigint\n",
      "         movement   varchar\n",
      "\n",
      "\n",
      "Falls Church HiRes Data Schema:\n",
      "           Column Data Type\n",
      "             city   varchar\n",
      "    data_provider   varchar\n",
      "            table   varchar\n",
      "  intersection_id   varchar\n",
      "     intersection   varchar\n",
      "intersection_name   varchar\n",
      " intersection_lat    double\n",
      "intersection_long    double\n",
      "publish_timestamp    bigint\n",
      "  event_timestamp    bigint\n",
      "       event_code   integer\n",
      "      event_param   integer\n",
      " original_content   varchar\n",
      "\n",
      "================================================================================\n",
      "Schemas stored! Use these column lists to ensure correct field names in queries.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Store schemas in variables for later use to ensure correct field names\n",
    "print(\"=\" * 80)\n",
    "print(\"STORING SCHEMAS FOR KEY TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get and store BSM schema\n",
    "bsm_schema = describe_table('alexandria', 'bsm')\n",
    "print(\"\\nAlexandria BSM Schema:\")\n",
    "print(bsm_schema.to_string(index=False))\n",
    "bsm_columns = bsm_schema['Column'].tolist()\n",
    "\n",
    "# Get and store PSM schema\n",
    "psm_schema = describe_table('alexandria', 'psm')\n",
    "print(\"\\n\\nAlexandria PSM Schema:\")\n",
    "print(psm_schema.to_string(index=False))\n",
    "psm_columns = psm_schema['Column'].tolist()\n",
    "\n",
    "# Get and store Safety Event schema\n",
    "safety_event_schema = describe_table('alexandria', 'safety-event')\n",
    "print(\"\\n\\nAlexandria Safety Event Schema:\")\n",
    "print(safety_event_schema.to_string(index=False))\n",
    "safety_event_columns = safety_event_schema['Column'].tolist()\n",
    "\n",
    "# Get and store Vehicle Count schema\n",
    "vehicle_count_schema = describe_table('alexandria', 'vehicle-count')\n",
    "print(\"\\n\\nAlexandria Vehicle Count Schema:\")\n",
    "print(vehicle_count_schema.to_string(index=False))\n",
    "vehicle_count_columns = vehicle_count_schema['Column'].tolist()\n",
    "\n",
    "# Get and store Falls Church HiRes Data schema\n",
    "hiresdata_schema = describe_table('falls-church', 'hiresdata')\n",
    "print(\"\\n\\nFalls Church HiRes Data Schema:\")\n",
    "print(hiresdata_schema.to_string(index=False))\n",
    "hiresdata_columns = hiresdata_schema['Column'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Schemas stored! Use these column lists to ensure correct field names in queries.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tkmtv24xpwd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Checking if columns exist in BSM table:\n",
      "WARNING: The following columns do not exist in alexandria.bsm:\n",
      "  Missing: ['publish_timestamp', 'vehicle_id', 'lat', 'lon', 'speed', 'heading']\n",
      "  Available columns: []\n",
      "\n",
      "================================================================================\n",
      "COMMON FIELD NAME CORRECTIONS:\n",
      "================================================================================\n",
      "BSM Table:\n",
      "  ✗ latitude  → ✓ lat\n",
      "  ✗ longitude → ✓ lon\n",
      "\n",
      "Safety Event Table:\n",
      "  ✗ latitude  → ✓ lat\n",
      "  ✗ longitude → ✓ lon\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if columns exist in a table\n",
    "def check_columns(columns_to_check, available_columns, table_name):\n",
    "    \"\"\"\n",
    "    Verify that requested columns exist in the table schema\n",
    "\n",
    "    Args:\n",
    "        columns_to_check: List of column names you want to use\n",
    "        available_columns: List of actual columns from the schema\n",
    "        table_name: Name of the table (for error messages)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (all_valid: bool, missing_columns: list)\n",
    "    \"\"\"\n",
    "    missing = [col for col in columns_to_check if col not in available_columns]\n",
    "\n",
    "    if missing:\n",
    "        print(f\"WARNING: The following columns do not exist in {table_name}:\")\n",
    "        print(f\"  Missing: {missing}\")\n",
    "        print(f\"  Available columns: {available_columns}\")\n",
    "        return False, missing\n",
    "    else:\n",
    "        print(f\"✓ All columns exist in {table_name}\")\n",
    "        return True, []\n",
    "\n",
    "\n",
    "# Example: Verify BSM columns before using them\n",
    "print(\"Example: Checking if columns exist in BSM table:\")\n",
    "columns_i_want = ['publish_timestamp',\n",
    "                  'vehicle_id', 'lat', 'lon', 'speed', 'heading']\n",
    "check_columns(columns_i_want, bsm_columns, 'alexandria.bsm')\n",
    "\n",
    "# Show common column name corrections\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMMON FIELD NAME CORRECTIONS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"BSM Table:\")\n",
    "print(\"  ✗ latitude  → ✓ lat\")\n",
    "print(\"  ✗ longitude → ✓ lon\")\n",
    "print(\"\\nSafety Event Table:\")\n",
    "print(\"  ✗ latitude  → ✓ lat\")\n",
    "print(\"  ✗ longitude → ✓ lon\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t31t48pbqz",
   "metadata": {},
   "source": [
    "## Using Stored Schemas\n",
    "\n",
    "The schemas for key tables are now stored in variables:\n",
    "\n",
    "- `bsm_columns` - List of columns in alexandria.bsm\n",
    "- `psm_columns` - List of columns in alexandria.psm\n",
    "- `safety_event_columns` - List of columns in alexandria.safety-event\n",
    "- `vehicle_count_columns` - List of columns in alexandria.vehicle-count\n",
    "- `hiresdata_columns` - List of columns in falls-church.hiresdata\n",
    "\n",
    "**Always verify column names before writing queries** to avoid errors!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30hy3yg34xj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a query for BSM data:\n",
      "WARNING: The following columns do not exist in alexandria.bsm:\n",
      "  Missing: ['publish_timestamp', 'vehicle_id', 'lat', 'lon', 'speed']\n",
      "  Available columns: []\n",
      "Cannot build query - missing columns: ['publish_timestamp', 'vehicle_id', 'lat', 'lon', 'speed']\n"
     ]
    }
   ],
   "source": [
    "# Example: Dynamically build a query using stored schema\n",
    "def build_select_query(table_schema, table_name, columns_to_select, available_columns, limit=10):\n",
    "    \"\"\"\n",
    "    Build a SELECT query dynamically after verifying columns exist\n",
    "\n",
    "    Args:\n",
    "        table_schema: Schema name (e.g., 'alexandria')\n",
    "        table_name: Table name (e.g., 'bsm')\n",
    "        columns_to_select: List of columns to select\n",
    "        available_columns: List of available columns from schema\n",
    "        limit: Number of rows to return\n",
    "\n",
    "    Returns:\n",
    "        str: SQL query string or None if columns are invalid\n",
    "    \"\"\"\n",
    "    # Verify all columns exist\n",
    "    is_valid, missing = check_columns(\n",
    "        columns_to_select, available_columns, f\"{table_schema}.{table_name}\")\n",
    "\n",
    "    if not is_valid:\n",
    "        print(f\"Cannot build query - missing columns: {missing}\")\n",
    "        return None\n",
    "\n",
    "    # Build column list\n",
    "    column_str = \", \".join(columns_to_select)\n",
    "\n",
    "    # Build query\n",
    "    query = f\"\"\"\n",
    "    SELECT {column_str}\n",
    "    FROM smartcities_iceberg.{table_schema}.\"{table_name}\"\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "# Example: Build a safe BSM query\n",
    "print(\"Building a query for BSM data:\")\n",
    "columns_needed = ['publish_timestamp', 'vehicle_id', 'lat', 'lon', 'speed']\n",
    "query = build_select_query(\n",
    "    'alexandria', 'bsm', columns_needed, bsm_columns, limit=5)\n",
    "\n",
    "if query:\n",
    "    print(\"\\nGenerated Query:\")\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aolrfk0z9xu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFYING ACTUAL COLUMN NAMES IN BSM TABLE\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TrinoExternalError",
     "evalue": "TrinoExternalError(type=EXTERNAL, name=ICEBERG_CATALOG_ERROR, message=\"Failed to load table: bsm in alexandria namespace\", query_id=20251112_025236_00037_y6d6c)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTrinoExternalError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Get a sample row to see actual column names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43mSELECT *\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43mFROM smartcities_iceberg.alexandria.bsm\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43mLIMIT 1\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Get actual column names from the query result\u001b[39;00m\n\u001b[32m     14\u001b[39m actual_bsm_columns = [desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m desc \u001b[38;5;129;01min\u001b[39;00m cur.description]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\dbapi.py:640\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, operation, params)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m._query = trino.client.TrinoQuery(\u001b[38;5;28mself\u001b[39m._request, query=operation,\n\u001b[32m    639\u001b[39m                                           legacy_primitive_types=\u001b[38;5;28mself\u001b[39m._legacy_primitive_types)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py:909\u001b[39m, in \u001b[36mTrinoQuery.execute\u001b[39m\u001b[34m(self, additional_http_headers)\u001b[39m\n\u001b[32m    907\u001b[39m \u001b[38;5;66;03m# Execute should block until at least one row is received or query is finished or cancelled\u001b[39;00m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.finished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cancelled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._result.rows) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     \u001b[38;5;28mself\u001b[39m._result.rows += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py:929\u001b[39m, in \u001b[36mTrinoQuery.fetch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m trino.exceptions.TrinoConnectionError(\u001b[33m\"\u001b[39m\u001b[33mfailed to fetch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(e))\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m status = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state(status)\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status.next_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py:698\u001b[39m, in \u001b[36mTrinoRequest.process\u001b[39m\u001b[34m(self, http_response)\u001b[39m\n\u001b[32m    696\u001b[39m response = json.loads(http_response.text)\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m constants.HEADER_CLEAR_SESSION \u001b[38;5;129;01min\u001b[39;00m http_response.headers:\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m get_header_values(\n\u001b[32m    702\u001b[39m         http_response.headers, constants.HEADER_CLEAR_SESSION\n\u001b[32m    703\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Git\\cs6604-trafficsafety\\.conda\\Lib\\site-packages\\trino\\client.py:667\u001b[39m, in \u001b[36mTrinoRequest._process_error\u001b[39m\u001b[34m(error, query_id)\u001b[39m\n\u001b[32m    665\u001b[39m error_type = error[\u001b[33m\"\u001b[39m\u001b[33merrorType\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_type == \u001b[33m\"\u001b[39m\u001b[33mEXTERNAL\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TrinoExternalError(error, query_id)\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_type == \u001b[33m\"\u001b[39m\u001b[33mUSER_ERROR\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    669\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exceptions.TrinoUserError(error, query_id)\n",
      "\u001b[31mTrinoExternalError\u001b[39m: TrinoExternalError(type=EXTERNAL, name=ICEBERG_CATALOG_ERROR, message=\"Failed to load table: bsm in alexandria namespace\", query_id=20251112_025236_00037_y6d6c)"
     ]
    }
   ],
   "source": [
    "# Let's see what the ACTUAL column names are by querying sample data\n",
    "print(\"=\" * 80)\n",
    "print(\"VERIFYING ACTUAL COLUMN NAMES IN BSM TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a sample row to see actual column names\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# Get actual column names from the query result\n",
    "actual_bsm_columns = [desc[0] for desc in cur.description]\n",
    "print(f\"\\nActual columns in BSM table ({len(actual_bsm_columns)} total):\")\n",
    "for i, col in enumerate(actual_bsm_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Store for comparison\n",
    "print(f\"\\nColumns stored from schema query: {len(bsm_columns)} total\")\n",
    "if set(actual_bsm_columns) != set(bsm_columns):\n",
    "    print(\"WARNING: Mismatch between schema query and actual columns!\")\n",
    "else:\n",
    "    print(\"✓ Schema matches actual columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9qoqv9q8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Safety Event columns too\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFYING ACTUAL COLUMN NAMES IN SAFETY-EVENT TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.\"safety-event\"\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "actual_safety_columns = [desc[0] for desc in cur.description]\n",
    "print(\n",
    "    f\"\\nActual columns in Safety-Event table ({len(actual_safety_columns)} total):\")\n",
    "for i, col in enumerate(actual_safety_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STORE THESE VERIFIED COLUMNS FOR USE IN QUERIES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nbsm_columns_verified = {actual_bsm_columns}\")\n",
    "print(f\"\\nsafety_event_columns_verified = {actual_safety_columns}\")\n",
    "\n",
    "# Update the stored columns with verified ones\n",
    "bsm_columns = actual_bsm_columns\n",
    "safety_event_columns = actual_safety_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csnfcywl3x",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Workflow for Using This Notebook\n",
    "\n",
    "**Before running the example queries below, you MUST:**\n",
    "\n",
    "1. **Run ALL cells in order** from the beginning of the notebook\n",
    "2. **Pay special attention to the schema verification cells** (above) which show you the ACTUAL column names\n",
    "3. **Note the actual column names** displayed in the verification output\n",
    "4. **Update queries** to use the correct column names based on what you see\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "- ❌ Assuming field names like `vehicle_id`, `latitude`, `longitude`\n",
    "- ✅ Use the ACTUAL field names shown in the verification cells above\n",
    "- ❌ Using `timestamp` instead of `publish_timestamp` for filtering\n",
    "- ✅ `publish_timestamp` is a **bigint** (microseconds) - use conversion functions\n",
    "\n",
    "### Safe Approach:\n",
    "\n",
    "All example queries below now use `SELECT *` to get ALL columns. After seeing what's available, you can filter to specific columns in your DataFrame:\n",
    "\n",
    "```python\n",
    "# After running a query with SELECT *\n",
    "df_filtered = df[['column1', 'column2', 'column3']]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "argzlf026c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check data availability and time range\n",
    "def check_data_range(schema, table, timestamp_col='publish_timestamp'):\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as record_count,\n",
    "            from_unixtime(MIN({timestamp_col}) / 1000) as earliest_record,\n",
    "            from_unixtime(MAX({timestamp_col}) / 1000) as latest_record\n",
    "        FROM smartcities_iceberg.{schema}.\"{table}\"\n",
    "        \"\"\"\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "        return {\n",
    "            'Schema': schema,\n",
    "            'Table': table,\n",
    "            'Record Count': result[0] if result else 0,\n",
    "            'Earliest': result[1] if result else None,\n",
    "            'Latest': result[2] if result else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'Schema': schema,\n",
    "            'Table': table,\n",
    "            'Error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Check data ranges for key tables\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA AVAILABILITY AND TIME RANGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ranges = []\n",
    "tables_to_check = [\n",
    "    ('alexandria', 'bsm'),\n",
    "    ('alexandria', 'psm'),\n",
    "    ('alexandria', 'safety-event'),\n",
    "    ('alexandria', 'vehicle-count'),\n",
    "    ('falls-church', 'hiresdata'),\n",
    "]\n",
    "\n",
    "for schema, table in tables_to_check:\n",
    "    print(f\"\\nChecking {schema}.{table}...\")\n",
    "    range_info = check_data_range(schema, table)\n",
    "    ranges.append(range_info)\n",
    "    if 'Error' not in range_info:\n",
    "        print(f\"  Records: {range_info['Record Count']:,}\")\n",
    "        print(\n",
    "            f\"  Time range: {range_info['Earliest']} to {range_info['Latest']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {range_info['Error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blqomcv0go",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample data from BSM table (using cursor to avoid pandas warning)\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE DATA: Alexandria BSM (Basic Safety Message)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_bsm = pd.DataFrame(results, columns=columns)\n",
    "print(df_bsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6jyxrlg2x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample data from Safety Event table (using cursor to avoid pandas warning)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA: Alexandria Safety Events\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.alexandria.\"safety-event\"\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_safety = pd.DataFrame(results, columns=columns)\n",
    "print(df_safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v68sy8dltf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample data from Falls Church median travel times (using cursor to avoid pandas warning)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA: Falls Church - Median Travel Times\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT *\n",
    "FROM smartcities_iceberg.\"falls-church\".mediantraveltimes\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_travel = pd.DataFrame(results, columns=columns)\n",
    "print(df_travel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xopxgvk0zdc",
   "metadata": {},
   "source": [
    "## API Summary\n",
    "\n",
    "This smart-cities API provides access to connected vehicle and traffic management data from multiple cities:\n",
    "\n",
    "### Available Data Types:\n",
    "\n",
    "1. **BSM (Basic Safety Message)**: Real-time vehicle position, speed, heading from connected vehicles\n",
    "2. **PSM (Pedestrian Safety Message)**: Pedestrian detection and safety data\n",
    "3. **Safety Events**: Detected safety conflicts and incidents\n",
    "4. **Vehicle/VRU Counts**: Traffic volume data\n",
    "5. **Speed Distribution**: Speed profiles across locations\n",
    "6. **High-Resolution Traffic Data**: Detailed signal performance metrics\n",
    "7. **Travel Times**: Corridor travel time measurements\n",
    "\n",
    "### Geographic Coverage:\n",
    "\n",
    "- Alexandria, VA\n",
    "- Falls Church, VA\n",
    "- CCI (Center for Connected Infrastructure)\n",
    "- VTTI (Virginia Tech Transportation Institute)\n",
    "\n",
    "### Data Collection Approaches:\n",
    "\n",
    "Run the cells below to see examples of collecting data over time periods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q8l685650w",
   "metadata": {},
   "source": [
    "## Example 1: Collecting BSM Data Over a Time Period\n",
    "\n",
    "This example shows how to collect vehicle trajectory data (BSM) for a specific time range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otvff2r5z",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect BSM data for the last 24 hours\n",
    "# Using SELECT * to get all columns - customize after verifying column names above\n",
    "# Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT \n",
    "    from_unixtime(publish_timestamp / 1000000) as timestamp,\n",
    "    *\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '24' hour) * 1000000\n",
    "ORDER BY publish_timestamp DESC\n",
    "LIMIT 1000\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_bsm_daily = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Collected {len(df_bsm_daily)} BSM records from the last 24 hours\")\n",
    "print(f\"\\nColumns available: {list(df_bsm_daily.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_bsm_daily.head())\n",
    "\n",
    "# After seeing the columns, you can select specific ones like this:\n",
    "# columns_i_want = ['timestamp', 'publish_timestamp', 'lat', 'lon', 'speed', 'heading']\n",
    "# df_bsm_daily_filtered = df_bsm_daily[columns_i_want]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vgetkelhpbi",
   "metadata": {},
   "source": [
    "## Example 2: Collecting Safety Events Over a Date Range\n",
    "\n",
    "This example shows how to collect safety event data between specific dates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ytyxhosfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect safety events for a specific date range\n",
    "# Using SELECT * to get all columns - customize after verifying column names above\n",
    "# Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT \n",
    "    from_unixtime(publish_timestamp / 1000000) as timestamp,\n",
    "    *\n",
    "FROM smartcities_iceberg.alexandria.\"safety-event\"\n",
    "WHERE publish_timestamp BETWEEN \n",
    "    to_unixtime(timestamp '2024-01-01 00:00:00') * 1000000 AND \n",
    "    to_unixtime(timestamp '2024-12-31 23:59:59') * 1000000\n",
    "ORDER BY publish_timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_safety_events = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Collected {len(df_safety_events)} safety events\")\n",
    "print(f\"\\nColumns available: {list(df_safety_events.columns)}\")\n",
    "\n",
    "if len(df_safety_events) > 0:\n",
    "    # Check if 'event_type' column exists before using it\n",
    "    if 'event_type' in df_safety_events.columns:\n",
    "        print(\"\\nEvent type breakdown:\")\n",
    "        print(df_safety_events['event_type'].value_counts())\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_safety_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kojtnd5jhkj",
   "metadata": {},
   "source": [
    "## Example 3: Aggregated Traffic Data by Hour\n",
    "\n",
    "This example shows how to aggregate traffic metrics over time periods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhonfeub11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate vehicle counts by hour for the last 7 days\n",
    "# Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "# Note: Adjust aggregation fields based on actual columns available\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', from_unixtime(publish_timestamp / 1000000)) as hour,\n",
    "    COUNT(*) as vehicle_count,\n",
    "    AVG(speed) as avg_speed,\n",
    "    MAX(speed) as max_speed,\n",
    "    MIN(speed) as min_speed\n",
    "FROM smartcities_iceberg.alexandria.bsm\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '7' day) * 1000000\n",
    "GROUP BY date_trunc('hour', from_unixtime(publish_timestamp / 1000000))\n",
    "ORDER BY hour DESC\n",
    "\"\"\")\n",
    "\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df_hourly_traffic = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "print(f\"Collected {len(df_hourly_traffic)} hourly aggregates\")\n",
    "print(f\"\\nColumns: {list(df_hourly_traffic.columns)}\")\n",
    "print(df_hourly_traffic.head(10))\n",
    "\n",
    "# Plot if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if len(df_hourly_traffic) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "        ax1.plot(df_hourly_traffic['hour'], df_hourly_traffic['vehicle_count'])\n",
    "        ax1.set_ylabel('Vehicle Count')\n",
    "        ax1.set_title('Hourly Vehicle Counts - Last 7 Days')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(df_hourly_traffic['hour'], df_hourly_traffic['avg_speed'])\n",
    "        ax2.set_ylabel('Average Speed')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_title('Average Speed by Hour')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except ImportError:\n",
    "    print(\"\\nInstall matplotlib to visualize the data: %pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810as7jw2",
   "metadata": {},
   "source": [
    "## Example 4: Batch Collection Over Multiple Days\n",
    "\n",
    "This example shows how to collect data in batches over a longer time period:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cwtlmu2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to collect data in daily batches (using cursor to avoid pandas warning)\n",
    "\n",
    "\n",
    "def collect_data_batches(start_date, end_date, table_schema, table_name, columns_to_select='*'):\n",
    "    \"\"\"\n",
    "    Collect data in daily batches to avoid memory issues with large datasets\n",
    "\n",
    "    Args:\n",
    "        start_date: Start date (string or datetime)\n",
    "        end_date: End date (string or datetime)\n",
    "        table_schema: Database schema name\n",
    "        table_name: Table name\n",
    "        columns_to_select: Columns to select (default '*' for all columns)\n",
    "\n",
    "    Returns:\n",
    "        List of dataframes, one per day\n",
    "    \"\"\"\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    if isinstance(end_date, str):\n",
    "        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    batches = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        next_date = current_date + timedelta(days=1)\n",
    "\n",
    "        # Note: publish_timestamp is a bigint (microseconds since epoch)\n",
    "        # Using SELECT * to get all columns, or specify columns_to_select\n",
    "        if columns_to_select == '*':\n",
    "            select_clause = \"from_unixtime(publish_timestamp / 1000000) as timestamp, *\"\n",
    "        else:\n",
    "            select_clause = f\"from_unixtime(publish_timestamp / 1000000) as timestamp, {columns_to_select}\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT {select_clause}\n",
    "        FROM smartcities_iceberg.{table_schema}.\"{table_name}\"\n",
    "        WHERE publish_timestamp >= to_unixtime(timestamp '{current_date.strftime('%Y-%m-%d 00:00:00')}') * 100000000\n",
    "          AND publish_timestamp < to_unixtime(timestamp '{next_date.strftime('%Y-%m-%d 00:00:00')}') * 100000000\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Collecting data for {current_date.strftime('%Y-%m-%d')}...\")\n",
    "\n",
    "        # Use cursor instead of pd.read_sql to avoid pandas warning\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df_batch = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        print(f\"  Found {len(df_batch)} records\")\n",
    "\n",
    "        if len(df_batch) > 0:\n",
    "            batches.append(df_batch)\n",
    "\n",
    "        current_date = next_date\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "# Example: Collect safety events for a week in daily batches\n",
    "print(\"=\" * 80)\n",
    "print(\"Collecting safety events in daily batches...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Note: Using SELECT * to get all columns - adjust dates based on actual data availability\n",
    "batches = collect_data_batches(\n",
    "    start_date='2024-01-01',\n",
    "    end_date='2024-01-07',\n",
    "    table_schema='alexandria',\n",
    "    table_name='safety-event',\n",
    "    columns_to_select='*'  # Get all columns\n",
    ")\n",
    "\n",
    "if batches:\n",
    "    # Combine all batches\n",
    "    df_all = pd.concat(batches, ignore_index=True)\n",
    "    print(f\"\\nTotal records collected: {len(df_all)}\")\n",
    "    print(f\"\\nColumns available: {list(df_all.columns)}\")\n",
    "    print(\n",
    "        f\"\\nDate range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_all.head())\n",
    "else:\n",
    "    print(\"\\nNo data found in this date range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1mjqsfi2sge",
   "metadata": {},
   "source": [
    "## Tips for Data Collection\n",
    "\n",
    "### STEP 1: Always Verify Column Names First!\n",
    "\n",
    "**Run the schema verification cells at the beginning of this notebook to see actual column names.**\n",
    "\n",
    "Don't assume field names! Common mistakes:\n",
    "\n",
    "- ❌ `vehicle_id` - may not exist\n",
    "- ❌ `latitude`, `longitude` - actual names are `lat`, `lon`\n",
    "- ✅ Check the verification output to see real column names\n",
    "\n",
    "### STEP 2: Understand the Data Types\n",
    "\n",
    "**publish_timestamp is a BIGINT (Unix epoch in milliseconds)**\n",
    "\n",
    "Since `publish_timestamp` is stored as a bigint (microseconds since Unix epoch), you need to convert it:\n",
    "\n",
    "**For display/conversion to readable timestamp:**\n",
    "\n",
    "```sql\n",
    "from_unixtime(publish_timestamp / 1000000) as timestamp\n",
    "```\n",
    "\n",
    "**For time-based filtering:**\n",
    "\n",
    "```sql\n",
    "WHERE publish_timestamp >= to_unixtime(current_timestamp - interval '24' hour) * 100000000\n",
    "```\n",
    "\n",
    "**For date range filtering:**\n",
    "\n",
    "```sql\n",
    "WHERE publish_timestamp BETWEEN\n",
    "    to_unixtime(timestamp '2024-01-01 00:00:00') * 100000000 AND\n",
    "    to_unixtime(timestamp '2024-12-31 23:59:59') * 1000000\n",
    "```\n",
    "\n",
    "**For aggregation:**\n",
    "\n",
    "```sql\n",
    "date_trunc('hour', from_unixtime(publish_timestamp / 1000000))\n",
    "```\n",
    "\n",
    "### STEP 3: Start with SELECT \\*\n",
    "\n",
    "When exploring a new table, always start with `SELECT *` to see all available columns:\n",
    "\n",
    "```python\n",
    "cur.execute(\"SELECT * FROM smartcities_iceberg.alexandria.bsm LIMIT 5\")\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "results = cur.fetchall()\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "print(df.columns.tolist())  # See what columns are actually available\n",
    "```\n",
    "\n",
    "Then filter in pandas after seeing the data:\n",
    "\n",
    "```python\n",
    "df_filtered = df[['column1', 'column2', 'column3']]\n",
    "```\n",
    "\n",
    "### Performance Tips:\n",
    "\n",
    "1. **Verify column names first** - Run schema verification cells\n",
    "2. **Start with SELECT \\*** - See what's available before selecting specific columns\n",
    "3. **Add LIMIT**: Always test queries with LIMIT first to check structure\n",
    "4. **Use WHERE clauses**: Filter by publish_timestamp to reduce data volume\n",
    "5. **Aggregate when possible**: Use GROUP BY and aggregation functions (COUNT, AVG, etc.)\n",
    "6. **Batch large requests**: Use the batch collection function for multi-day/multi-week requests\n",
    "\n",
    "### Common Time Intervals:\n",
    "\n",
    "- Last hour: `to_unixtime(current_timestamp - interval '1' hour) * 1000000`\n",
    "- Last 24 hours: `to_unixtime(current_timestamp - interval '24' hour) * 100000000`\n",
    "- Last week: `to_unixtime(current_timestamp - interval '7' day) * 1000000`\n",
    "- Last month: `to_unixtime(current_timestamp - interval '30' day) * 1000000`\n",
    "\n",
    "### Key Tables for Traffic Safety Analysis:\n",
    "\n",
    "- **alexandria.bsm**: Vehicle trajectories (position, speed, acceleration)\n",
    "- **alexandria.safety-event**: Detected safety conflicts\n",
    "- **alexandria.vehicle-count**: Traffic volumes\n",
    "- **alexandria.speed-distribution**: Speed profiles\n",
    "- **falls-church.hiresdata**: High-resolution signal data\n",
    "- **falls-church.mediantraveltimes**: Travel time measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9sjupzoy53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1.2: Identify intersections with sufficient data overlap\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1.2: INTERSECTION DATA OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find intersections that have data in all key tables\n",
    "query_intersections = \"\"\"\n",
    "WITH bsm_intersections AS (\n",
    "    SELECT DISTINCT intersection\n",
    "    FROM alexandria.bsm\n",
    "    WHERE publish_timestamp > 0 AND publish_timestamp < 9999999999999999\n",
    "    LIMIT 100\n",
    "),\n",
    "psm_intersections AS (\n",
    "    SELECT DISTINCT intersection  \n",
    "    FROM alexandria.psm\n",
    "    WHERE publish_timestamp > 0 AND publish_timestamp < 9999999999999999\n",
    "    LIMIT 100\n",
    "),\n",
    "event_intersections AS (\n",
    "    SELECT DISTINCT intersection\n",
    "    FROM alexandria.\"safety-event\"\n",
    "    WHERE time_at_site > 0 AND time_at_site < 9999999999999999\n",
    "    LIMIT 100\n",
    "),\n",
    "count_intersections AS (\n",
    "    SELECT DISTINCT intersection\n",
    "    FROM alexandria.\"vehicle-count\"\n",
    "    WHERE publish_timestamp > 0 AND publish_timestamp < 9999999999999999\n",
    "    LIMIT 100\n",
    ")\n",
    "SELECT \n",
    "    b.intersection,\n",
    "    CASE WHEN p.intersection IS NOT NULL THEN 'Y' ELSE 'N' END as has_psm,\n",
    "    CASE WHEN e.intersection IS NOT NULL THEN 'Y' ELSE 'N' END as has_events,\n",
    "    CASE WHEN c.intersection IS NOT NULL THEN 'Y' ELSE 'N' END as has_counts\n",
    "FROM bsm_intersections b\n",
    "LEFT JOIN psm_intersections p ON b.intersection = p.intersection\n",
    "LEFT JOIN event_intersections e ON b.intersection = e.intersection\n",
    "LEFT JOIN count_intersections c ON b.intersection = c.intersection\n",
    "ORDER BY b.intersection\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cur.execute(query_intersections)\n",
    "    columns = [desc[0] for desc in cur.description]\n",
    "    results = cur.fetchall()\n",
    "    intersection_coverage = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    print(f\"\\nFound {len(intersection_coverage)} intersections with BSM data\")\n",
    "    print(\"\\nData availability by intersection:\")\n",
    "    print(intersection_coverage.to_string(index=False))\n",
    "\n",
    "    # Find intersections with complete data\n",
    "    complete_data = intersection_coverage[\n",
    "        (intersection_coverage['has_psm'] == 'Y') &\n",
    "        (intersection_coverage['has_events'] == 'Y') &\n",
    "        (intersection_coverage['has_counts'] == 'Y')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\n",
    "        f\"Intersections with COMPLETE data (BSM + PSM + Events + Counts): {len(complete_data)}\")\n",
    "    if len(complete_data) > 0:\n",
    "        print(\"Recommended intersections for analysis:\")\n",
    "        print(complete_data['intersection'].tolist())\n",
    "    else:\n",
    "        print(\"⚠ WARNING: No intersections have complete data across all tables\")\n",
    "        print(\"Consider using intersections with partial data and handling missing values\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing intersection coverage: {e}\")\n",
    "    print(\"Proceeding with available data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tj7oe82pp6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2.1: Collect severity-weighted safety events for baseline\n",
    "from datetime import datetime, timedelta\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2.1: SEVERITY-WEIGHTED BASELINE CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Severity weights from checkpoint document\n",
    "SEVERITY_WEIGHTS = {\n",
    "    'FATAL': 10,\n",
    "    'INJURY': 3,\n",
    "    'PDO': 1,  # Property Damage Only\n",
    "    'DEFAULT': 1  # For events without explicit severity\n",
    "}\n",
    "\n",
    "# Define function that will eventually go in FastAPI backend\n",
    "\n",
    "\n",
    "def collect_baseline_events(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Collect and weight safety events for baseline risk calculation.\n",
    "    This will become an API backend function.\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period (datetime or timestamp)\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with weighted events by intersection, TOD, DOW\n",
    "    \"\"\"\n",
    "\n",
    "    # Build query with optional filters\n",
    "    where_clauses = [\"time_at_site > 0\", \"time_at_site < 9999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        # Convert to milliseconds timestamp if needed\n",
    "        where_clauses.append(\n",
    "            f\"time_at_site >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"time_at_site <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        event_type,\n",
    "        event_id,\n",
    "        from_unixtime(time_at_site / 1000000) as event_time,\n",
    "        time_at_site,\n",
    "        HOUR(from_unixtime(time_at_site / 1000000)) as hour_of_day,\n",
    "        DAY_OF_WEEK(from_unixtime(time_at_site / 1000000)) as day_of_week,\n",
    "        object1_class,\n",
    "        object2_class,\n",
    "        detection_area\n",
    "    FROM alexandria.\"safety-event\"\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, time_at_site\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        if len(df) > 0:\n",
    "            # Apply severity weighting\n",
    "            # Note: We'll need to map event types to severity levels\n",
    "            # For now, use a simple heuristic\n",
    "            def assign_severity_weight(row):\n",
    "                event_type = str(row['event_type']).upper()\n",
    "\n",
    "                # VRU-involved events get higher weight\n",
    "                is_vru = ('PEDESTRIAN' in str(row['object1_class']).upper() or\n",
    "                          'CYCLIST' in str(row['object1_class']).upper() or\n",
    "                          'PEDESTRIAN' in str(row['object2_class']).upper() or\n",
    "                          'CYCLIST' in str(row['object2_class']).upper())\n",
    "\n",
    "                if 'FATAL' in event_type or 'DEATH' in event_type:\n",
    "                    return SEVERITY_WEIGHTS['FATAL']\n",
    "                elif 'INJURY' in event_type or is_vru:\n",
    "                    return SEVERITY_WEIGHTS['INJURY']\n",
    "                else:\n",
    "                    return SEVERITY_WEIGHTS['PDO']\n",
    "\n",
    "            df['severity_weight'] = df.apply(assign_severity_weight, axis=1)\n",
    "            df['is_vru_involved'] = df.apply(\n",
    "                lambda row: 1 if ('PEDESTRIAN' in str(row['object1_class']).upper() or\n",
    "                                  'CYCLIST' in str(row['object1_class']).upper() or\n",
    "                                  'PEDESTRIAN' in str(row['object2_class']).upper() or\n",
    "                                  'CYCLIST' in str(row['object2_class']).upper()) else 0,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting baseline events: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Test the function - collect last 30 days of data\n",
    "\n",
    "end_dt = datetime.now()\n",
    "start_dt = end_dt - timedelta(days=30)\n",
    "\n",
    "print(f\"\\nCollecting events from {start_dt.date()} to {end_dt.date()}...\")\n",
    "baseline_events = collect_baseline_events(start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(baseline_events) > 0:\n",
    "    print(f\"\\n✓ Collected {len(baseline_events)} events\")\n",
    "    print(f\"\\nEvent type distribution:\")\n",
    "    print(baseline_events['event_type'].value_counts())\n",
    "    print(f\"\\nVRU-involved events: {baseline_events['is_vru_involved'].sum()}\")\n",
    "    print(f\"\\nSeverity weight distribution:\")\n",
    "    print(baseline_events['severity_weight'].value_counts().sort_index())\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(baseline_events[['intersection', 'event_time',\n",
    "          'event_type', 'severity_weight', 'is_vru_involved']].head(10))\n",
    "else:\n",
    "    print(\"⚠ No events found in the specified date range\")\n",
    "    print(\"Try adjusting the date range or checking data availability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foavg978bkb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2.2: Build exposure metrics from vehicle and VRU count data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2.2: EXPOSURE METRICS CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def collect_exposure_metrics(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Collect vehicle and VRU counts for exposure normalization.\n",
    "    This will become an API backend function.\n",
    "\n",
    "    Returns:\n",
    "        Two DataFrames: vehicle_counts, vru_counts\n",
    "    \"\"\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_clauses = [\"publish_timestamp > 0\",\n",
    "                     \"publish_timestamp < 9999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    # Collect vehicle counts\n",
    "    vehicle_query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        approach,\n",
    "        movement,\n",
    "        class as vehicle_class,\n",
    "        count as vehicle_count,\n",
    "        HOUR(from_unixtime(publish_timestamp / 1000000)) as hour_of_day,\n",
    "        DAY_OF_WEEK(from_unixtime(publish_timestamp / 1000000)) as day_of_week\n",
    "    FROM alexandria.\"vehicle-count\"\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect VRU counts\n",
    "    vru_query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        approach,\n",
    "        count as vru_count,\n",
    "        HOUR(from_unixtime(publish_timestamp / 1000000)) as hour_of_day,\n",
    "        DAY_OF_WEEK(from_unixtime(publish_timestamp / 1000000)) as day_of_week\n",
    "    FROM alexandria.\"vru-count\"\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Get vehicle counts\n",
    "        cur.execute(vehicle_query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        vehicle_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        # Get VRU counts\n",
    "        cur.execute(vru_query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        vru_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        return vehicle_df, vru_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting exposure metrics: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\n",
    "    f\"\\nCollecting exposure metrics from {start_dt.date()} to {end_dt.date()}...\")\n",
    "vehicle_counts, vru_counts = collect_exposure_metrics(\n",
    "    start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    print(f\"\\n✓ Vehicle Counts: {len(vehicle_counts)} records\")\n",
    "    print(f\"  Intersections: {vehicle_counts['intersection'].nunique()}\")\n",
    "    print(f\"  Total vehicles: {vehicle_counts['vehicle_count'].sum():,}\")\n",
    "    print(f\"\\n  Sample vehicle counts:\")\n",
    "    print(vehicle_counts[['intersection', 'time',\n",
    "          'approach', 'movement', 'vehicle_count']].head(5))\n",
    "else:\n",
    "    print(\"\\n⚠ No vehicle count data found\")\n",
    "\n",
    "if len(vru_counts) > 0:\n",
    "    print(f\"\\n✓ VRU Counts: {len(vru_counts)} records\")\n",
    "    print(f\"  Intersections: {vru_counts['intersection'].nunique()}\")\n",
    "    print(f\"  Total VRUs: {vru_counts['vru_count'].sum():,}\")\n",
    "    print(f\"\\n  Sample VRU counts:\")\n",
    "    print(vru_counts[['intersection', 'time',\n",
    "          'approach', 'vru_count']].head(5))\n",
    "else:\n",
    "    print(\"\\n⚠ No VRU count data found\")\n",
    "\n",
    "# Calculate normalization constants (V_max, N_VRU_max) for index formulas\n",
    "if len(vehicle_counts) > 0 and len(vru_counts) > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NORMALIZATION CONSTANTS FOR INDEX COMPUTATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Aggregate to 15-minute intervals to match index granularity\n",
    "    vehicle_counts['time_15min'] = pd.to_datetime(\n",
    "        vehicle_counts['time'], utc=True).dt.floor('15min')\n",
    "    vru_counts['time_15min'] = pd.to_datetime(\n",
    "        vru_counts['time'], utc=True).dt.floor('15min')\n",
    "\n",
    "    vehicle_15min = vehicle_counts.groupby(['intersection', 'time_15min'])[\n",
    "        'vehicle_count'].sum().reset_index()\n",
    "    vru_15min = vru_counts.groupby(['intersection', 'time_15min'])[\n",
    "        'vru_count'].sum().reset_index()\n",
    "\n",
    "    V_max = vehicle_15min['vehicle_count'].max()\n",
    "    N_VRU_max = vru_15min['vru_count'].max()\n",
    "\n",
    "    print(f\"\\nV_max (max vehicle volume per 15-min): {V_max}\")\n",
    "    print(f\"N_VRU_max (max VRU count per 15-min): {N_VRU_max}\")\n",
    "    print(f\"\\nThese constants will be used to normalize the Safety Index formulas.\")\n",
    "\n",
    "    # Store for later use\n",
    "    NORMALIZATION_CONSTANTS = {\n",
    "        'V_max': V_max,\n",
    "        'N_VRU_max': N_VRU_max\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734le8n83k8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3.1: BSM-derived vehicle features (15-minute aggregates)\n",
    "import numpy as np\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3.1: BSM-DERIVED VEHICLE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def collect_bsm_features(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Extract vehicle behavior features from BSM data aggregated to 15-minute intervals.\n",
    "\n",
    "    Features computed:\n",
    "    - Vehicle count (exposure metric V)\n",
    "    - Average speed (S)\n",
    "    - Speed variance (σ_S) \n",
    "    - Hard braking frequency\n",
    "    - Heading change rate (proxy for weaving/erratic behavior)\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 15-minute aggregated features\n",
    "    \"\"\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_clauses = [\"publish_timestamp > 0\",\n",
    "                     \"publish_timestamp < 9999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    # Query BSM data with relevant fields\n",
    "    # Note: Using verified column names (lat, lon, id, speed, heading, brake_applied_status)\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        id as vehicle_id,\n",
    "        lat,\n",
    "        lon,\n",
    "        speed,\n",
    "        heading,\n",
    "        brake_applied_status,\n",
    "        accel_lon,\n",
    "        accel_lat\n",
    "    FROM alexandria.bsm\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(\"⚠ No BSM data found for specified criteria\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"✓ Retrieved {len(df):,} BSM records\")\n",
    "\n",
    "        # Convert time to datetime and create 15-minute bins\n",
    "        df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "        df['time_15min'] = df['time'].dt.floor('15min')\n",
    "\n",
    "        # Identify hard braking events (brake_applied_status & 0x04 for hard braking)\n",
    "        # Brake status is a bitmask: bit 2 (0x04) indicates hard braking\n",
    "        df['hard_braking'] = df['brake_applied_status'].apply(\n",
    "            lambda x: 1 if pd.notna(x) and (int(x) & 0x04) else 0\n",
    "        )\n",
    "\n",
    "        # Calculate features by 15-minute interval and intersection\n",
    "        print(\"Computing aggregated features...\")\n",
    "\n",
    "        # Group by intersection and 15-minute interval\n",
    "        grouped = df.groupby(['intersection', 'time_15min'])\n",
    "\n",
    "        features = grouped.agg({\n",
    "            'vehicle_id': 'nunique',  # Unique vehicle count\n",
    "            'speed': ['mean', 'std'],  # Average speed and speed variance\n",
    "            'hard_braking': 'sum',  # Count of hard braking events\n",
    "            # Heading variance (weaving proxy)\n",
    "            'heading': lambda x: calculate_heading_change_rate(x),\n",
    "            'accel_lon': 'std',  # Longitudinal acceleration variance\n",
    "            'accel_lat': 'std'  # Lateral acceleration variance\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        features.columns = [\n",
    "            'intersection',\n",
    "            'time_15min',\n",
    "            'vehicle_count',\n",
    "            'avg_speed',\n",
    "            'speed_variance',\n",
    "            'hard_braking_count',\n",
    "            'heading_change_rate',\n",
    "            'accel_lon_variance',\n",
    "            'accel_lat_variance'\n",
    "        ]\n",
    "\n",
    "        # Add time-of-day and day-of-week for contextual analysis\n",
    "        features['hour_of_day'] = features['time_15min'].dt.hour\n",
    "        features['day_of_week'] = features['time_15min'].dt.dayofweek\n",
    "\n",
    "        print(f\"✓ Generated {len(features)} 15-minute feature records\")\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting BSM features: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def calculate_heading_change_rate(heading_series):\n",
    "    \"\"\"\n",
    "    Calculate rate of heading changes as a proxy for weaving behavior.\n",
    "    Returns standard deviation of heading changes.\n",
    "    \"\"\"\n",
    "    if len(heading_series) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate absolute differences in heading\n",
    "    # Note: Need to handle circular nature of heading (0-360 degrees)\n",
    "    headings = heading_series.dropna().values\n",
    "    if len(headings) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate angular differences\n",
    "    diffs = []\n",
    "    for i in range(1, len(headings)):\n",
    "        diff = abs(headings[i] - headings[i-1])\n",
    "        # Handle wrap-around (e.g., 359° to 1° is a 2° change, not 358°)\n",
    "        if diff > 180:\n",
    "            diff = 360 - diff\n",
    "        diffs.append(diff)\n",
    "\n",
    "    return np.std(diffs) if len(diffs) > 0 else 0.0\n",
    "\n",
    "\n",
    "# Test the function\n",
    "\n",
    "print(\n",
    "    f\"\\nCollecting BSM features from {start_dt.date()} to {end_dt.date()}...\")\n",
    "bsm_features = collect_bsm_features(start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(bsm_features) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BSM FEATURE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total 15-minute intervals: {len(bsm_features)}\")\n",
    "    print(f\"Intersections covered: {bsm_features['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"Date range: {bsm_features['time_15min'].min()} to {bsm_features['time_15min'].max()}\")\n",
    "\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(bsm_features[['vehicle_count', 'avg_speed', 'speed_variance',\n",
    "          'hard_braking_count', 'heading_change_rate']].describe())\n",
    "\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(bsm_features[['intersection', 'time_15min', 'vehicle_count', 'avg_speed',\n",
    "          'speed_variance', 'hard_braking_count']].head(10).to_string(index=False))\n",
    "\n",
    "    # Store normalization constants\n",
    "    if 'NORMALIZATION_CONSTANTS' not in dir():\n",
    "        NORMALIZATION_CONSTANTS = {}\n",
    "\n",
    "    NORMALIZATION_CONSTANTS['speed_ref'] = bsm_features['avg_speed'].quantile(\n",
    "        0.85)  # 85th percentile speed\n",
    "    NORMALIZATION_CONSTANTS['speed_variance_max'] = bsm_features['speed_variance'].max(\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"UPDATED NORMALIZATION CONSTANTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"S_ref (reference speed, 85th percentile): {NORMALIZATION_CONSTANTS['speed_ref']:.2f} m/s\")\n",
    "    print(\n",
    "        f\"σ_max (max speed variance): {NORMALIZATION_CONSTANTS['speed_variance_max']:.2f}\")\n",
    "else:\n",
    "    print(\"⚠ No BSM features generated - check data availability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdewx17ivq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3.2: PSM-derived VRU features (15-minute aggregates)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3.2: PSM-DERIVED VRU FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def collect_psm_features(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Extract VRU (Vulnerable Road User) behavior features from PSM data.\n",
    "\n",
    "    Features computed:\n",
    "    - VRU count (pedestrians, cyclists)\n",
    "    - Average VRU speed\n",
    "    - VRU activity type distribution\n",
    "    - Emergency responder presence\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 15-minute aggregated VRU features\n",
    "    \"\"\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_clauses = [\"publish_timestamp > 0\",\n",
    "                     \"publish_timestamp < 9999999999999999\"]\n",
    "\n",
    "    if intersection:\n",
    "        where_clauses.append(f\"intersection = '{intersection}'\")\n",
    "    if start_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp >= {int(start_date.timestamp() * 1000000)}\")\n",
    "    if end_date:\n",
    "        where_clauses.append(\n",
    "            f\"publish_timestamp <= {int(end_date.timestamp() * 1000000)}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "    # Query PSM data with relevant fields\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        intersection,\n",
    "        from_unixtime(publish_timestamp / 1000000) as time,\n",
    "        publish_timestamp,\n",
    "        id as vru_id,\n",
    "        lat,\n",
    "        lon,\n",
    "        speed,\n",
    "        heading,\n",
    "        basic_type,\n",
    "        activity_type,\n",
    "        activity_sub_type,\n",
    "        event_responder_type\n",
    "    FROM alexandria.psm\n",
    "    WHERE {where_clause}\n",
    "    ORDER BY intersection, publish_timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        results = cur.fetchall()\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(\"⚠ No PSM data found for specified criteria\")\n",
    "            print(\"   Note: PSM data may be sparse due to limited VRU device adoption\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"✓ Retrieved {len(df):,} PSM records\")\n",
    "\n",
    "        # Convert time to datetime and create 15-minute bins\n",
    "        df['time'] = pd.to_datetime(df['time'], utc=True)\n",
    "        df['time_15min'] = df['time'].dt.floor('15min')\n",
    "\n",
    "        # Classify VRU types based on basic_type field\n",
    "        # PSM basic_type values (from J2735 standard):\n",
    "        # 0 = unknown, 1 = pedestrian, 2 = cyclist, 3 = wheelchair, 4 = animal\n",
    "        df['is_pedestrian'] = df['basic_type'].apply(\n",
    "            lambda x: 1 if x == 1 else 0)\n",
    "        df['is_cyclist'] = df['basic_type'].apply(lambda x: 1 if x == 2 else 0)\n",
    "        df['is_emergency_responder'] = df['event_responder_type'].apply(\n",
    "            lambda x: 1 if pd.notna(x) and x > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Calculate features by 15-minute interval and intersection\n",
    "        print(\"Computing aggregated VRU features...\")\n",
    "\n",
    "        grouped = df.groupby(['intersection', 'time_15min'])\n",
    "\n",
    "        features = grouped.agg({\n",
    "            'vru_id': 'nunique',  # Unique VRU count\n",
    "            'speed': 'mean',  # Average VRU speed\n",
    "            'is_pedestrian': 'sum',  # Pedestrian count\n",
    "            'is_cyclist': 'sum',  # Cyclist count\n",
    "            'is_emergency_responder': 'sum'  # Emergency responder count\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        features.columns = [\n",
    "            'intersection',\n",
    "            'time_15min',\n",
    "            'vru_count',\n",
    "            'avg_vru_speed',\n",
    "            'pedestrian_count',\n",
    "            'cyclist_count',\n",
    "            'emergency_responder_count'\n",
    "        ]\n",
    "\n",
    "        # Add time context\n",
    "        features['hour_of_day'] = features['time_15min'].dt.hour\n",
    "        features['day_of_week'] = features['time_15min'].dt.dayofweek\n",
    "\n",
    "        print(f\"✓ Generated {len(features)} 15-minute VRU feature records\")\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting PSM features: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\n",
    "    f\"\\nCollecting PSM features from {start_dt.date()} to {end_dt.date()}...\")\n",
    "psm_features = collect_psm_features(start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(psm_features) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PSM FEATURE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total 15-minute intervals: {len(psm_features)}\")\n",
    "    print(f\"Intersections covered: {psm_features['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"Date range: {psm_features['time_15min'].min()} to {psm_features['time_15min'].max()}\")\n",
    "\n",
    "    print(f\"\\nVRU statistics:\")\n",
    "    print(f\"  Total VRU observations: {psm_features['vru_count'].sum():.0f}\")\n",
    "    print(f\"  Pedestrians: {psm_features['pedestrian_count'].sum():.0f}\")\n",
    "    print(f\"  Cyclists: {psm_features['cyclist_count'].sum():.0f}\")\n",
    "    print(\n",
    "        f\"  Emergency responders: {psm_features['emergency_responder_count'].sum():.0f}\")\n",
    "\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(psm_features[['vru_count', 'avg_vru_speed',\n",
    "          'pedestrian_count', 'cyclist_count']].describe())\n",
    "\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(psm_features[['intersection', 'time_15min', 'vru_count',\n",
    "          'pedestrian_count', 'cyclist_count']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"⚠ No PSM features generated\")\n",
    "    print(\"   PSM data may be limited or unavailable for this time period\")\n",
    "    print(\"   The Safety Index can still be computed with reduced VRU component accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdxbtsvnfg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3.3: Aggregate safety events by 15-minute intervals\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3.3: SAFETY EVENT AGGREGATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def aggregate_safety_events(intersection=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Aggregate safety events to 15-minute intervals with severity weighting.\n",
    "\n",
    "    Features computed:\n",
    "    - Total event count\n",
    "    - VRU-involved event count (I_VRU)\n",
    "    - Vehicle-only event count\n",
    "    - Severity-weighted event score\n",
    "    - Event type distribution\n",
    "\n",
    "    Args:\n",
    "        intersection: Specific intersection or None for all\n",
    "        start_date: Start of analysis period\n",
    "        end_date: End of analysis period\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 15-minute aggregated safety events\n",
    "    \"\"\"\n",
    "\n",
    "    # Reuse the baseline_events collection function but aggregate differently\n",
    "    events_df = collect_baseline_events(intersection, start_date, end_date)\n",
    "\n",
    "    if len(events_df) == 0:\n",
    "        print(\"⚠ No safety events to aggregate\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"✓ Retrieved {len(events_df)} safety events\")\n",
    "    print(\"Aggregating to 15-minute intervals...\")\n",
    "\n",
    "    # Convert event_time to datetime and create 15-minute bins\n",
    "    events_df['event_time'] = pd.to_datetime(events_df['event_time'], utc=True)\n",
    "    events_df['time_15min'] = events_df['event_time'].dt.floor('15min')\n",
    "\n",
    "    # Group by intersection and 15-minute interval\n",
    "    grouped = events_df.groupby(['intersection', 'time_15min'])\n",
    "\n",
    "    aggregated = grouped.agg({\n",
    "        'event_id': 'count',  # Total event count\n",
    "        'is_vru_involved': 'sum',  # VRU-involved events (I_VRU)\n",
    "        'severity_weight': 'sum',  # Severity-weighted score\n",
    "        # Most common event type\n",
    "        'event_type': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    }).reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    aggregated.columns = [\n",
    "        'intersection',\n",
    "        'time_15min',\n",
    "        'total_event_count',\n",
    "        'vru_event_count',\n",
    "        'severity_weighted_score',\n",
    "        'dominant_event_type'\n",
    "    ]\n",
    "\n",
    "    # Calculate vehicle-only events\n",
    "    aggregated['vehicle_event_count'] = aggregated['total_event_count'] - \\\n",
    "        aggregated['vru_event_count']\n",
    "\n",
    "    # Add time context\n",
    "    aggregated['hour_of_day'] = aggregated['time_15min'].dt.hour\n",
    "    aggregated['day_of_week'] = aggregated['time_15min'].dt.dayofweek\n",
    "\n",
    "    # Calculate I_VRU (VRU conflict intensity) per interval\n",
    "    # This is the key metric for VRU Safety Index\n",
    "    aggregated['I_VRU'] = aggregated['vru_event_count']\n",
    "\n",
    "    print(f\"✓ Generated {len(aggregated)} 15-minute event aggregates\")\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\n",
    "    f\"\\nAggregating safety events from {start_dt.date()} to {end_dt.date()}...\")\n",
    "aggregated_events = aggregate_safety_events(\n",
    "    start_date=start_dt, end_date=end_dt)\n",
    "\n",
    "if len(aggregated_events) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAFETY EVENT AGGREGATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total 15-minute intervals with events: {len(aggregated_events)}\")\n",
    "    print(\n",
    "        f\"Intersections covered: {aggregated_events['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"Date range: {aggregated_events['time_15min'].min()} to {aggregated_events['time_15min'].max()}\")\n",
    "\n",
    "    print(f\"\\nEvent statistics:\")\n",
    "    print(\n",
    "        f\"  Total events: {aggregated_events['total_event_count'].sum():.0f}\")\n",
    "    print(\n",
    "        f\"  VRU-involved: {aggregated_events['vru_event_count'].sum():.0f} ({100*aggregated_events['vru_event_count'].sum()/aggregated_events['total_event_count'].sum():.1f}%)\")\n",
    "    print(\n",
    "        f\"  Vehicle-only: {aggregated_events['vehicle_event_count'].sum():.0f}\")\n",
    "    print(\n",
    "        f\"  Total severity-weighted score: {aggregated_events['severity_weighted_score'].sum():.0f}\")\n",
    "\n",
    "    print(f\"\\nI_VRU (VRU conflict intensity) statistics:\")\n",
    "    print(aggregated_events['I_VRU'].describe())\n",
    "\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(aggregated_events[['intersection', 'time_15min', 'total_event_count',\n",
    "          'vru_event_count', 'I_VRU', 'severity_weighted_score']].head(10).to_string(index=False))\n",
    "\n",
    "    # Store I_max for normalization\n",
    "    if 'NORMALIZATION_CONSTANTS' not in dir():\n",
    "        NORMALIZATION_CONSTANTS = {}\n",
    "\n",
    "    NORMALIZATION_CONSTANTS['I_max'] = aggregated_events['I_VRU'].max()\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"UPDATED NORMALIZATION CONSTANTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"I_max (max VRU conflict intensity): {NORMALIZATION_CONSTANTS['I_max']:.0f}\")\n",
    "else:\n",
    "    print(\"⚠ No aggregated events generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qe9f5x2bmvi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Create master feature table by joining all data sources\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: MASTER FEATURE TABLE CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def create_master_feature_table(bsm_features, psm_features, aggregated_events,\n",
    "                                vehicle_counts, vru_counts):\n",
    "    \"\"\"\n",
    "    Join all feature sources into a unified feature table.\n",
    "\n",
    "    Inputs:\n",
    "    - bsm_features: Vehicle behavior features (speed, variance, braking)\n",
    "    - psm_features: VRU features (pedestrian/cyclist counts, speed)\n",
    "    - aggregated_events: Safety events aggregated to 15-min intervals\n",
    "    - vehicle_counts: Vehicle exposure metrics\n",
    "    - vru_counts: VRU exposure metrics\n",
    "\n",
    "    Returns:\n",
    "        Comprehensive feature table ready for index computation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Building master feature table...\")\n",
    "\n",
    "    # Start with BSM features as the base (most comprehensive)\n",
    "    if len(bsm_features) == 0:\n",
    "        print(\"⚠ ERROR: No BSM features available - cannot build master table\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    master = bsm_features.copy()\n",
    "    print(f\"  Base table (BSM features): {len(master)} records\")\n",
    "\n",
    "    # Aggregate vehicle counts to 15-minute intervals\n",
    "    if len(vehicle_counts) > 0:\n",
    "        vehicle_counts['time_15min'] = pd.to_datetime(\n",
    "            vehicle_counts['time'], utc=True).dt.floor('15min')\n",
    "        vehicle_agg = vehicle_counts.groupby(['intersection', 'time_15min'])[\n",
    "            'vehicle_count'].sum().reset_index()\n",
    "        vehicle_agg.rename(\n",
    "            columns={'vehicle_count': 'vehicle_volume'}, inplace=True)\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            vehicle_agg, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + Vehicle counts: {len(vehicle_agg)} records merged\")\n",
    "    else:\n",
    "        master['vehicle_volume'] = 0\n",
    "        print(\"  ⚠ No vehicle count data available\")\n",
    "\n",
    "    # Aggregate VRU counts to 15-minute intervals\n",
    "    if len(vru_counts) > 0:\n",
    "        vru_counts['time_15min'] = pd.to_datetime(\n",
    "            vru_counts['time'], utc=True).dt.floor('15min')\n",
    "        vru_agg = vru_counts.groupby(['intersection', 'time_15min'])[\n",
    "            'vru_count'].sum().reset_index()\n",
    "        vru_agg.rename(columns={'vru_count': 'vru_volume'}, inplace=True)\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            vru_agg, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + VRU counts: {len(vru_agg)} records merged\")\n",
    "    else:\n",
    "        master['vru_volume'] = 0\n",
    "        print(\"  ⚠ No VRU count data available\")\n",
    "\n",
    "    # Join PSM features\n",
    "    if len(psm_features) > 0:\n",
    "        psm_cols = ['intersection', 'time_15min', 'vru_count', 'avg_vru_speed',\n",
    "                    'pedestrian_count', 'cyclist_count']\n",
    "        psm_subset = psm_features[psm_cols].copy()\n",
    "        psm_subset.rename(columns={'vru_count': 'psm_vru_count'}, inplace=True)\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            psm_subset, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + PSM features: {len(psm_features)} records merged\")\n",
    "    else:\n",
    "        master['psm_vru_count'] = 0\n",
    "        master['avg_vru_speed'] = 0\n",
    "        master['pedestrian_count'] = 0\n",
    "        master['cyclist_count'] = 0\n",
    "        print(\"  ⚠ No PSM data available\")\n",
    "\n",
    "    # Join aggregated safety events\n",
    "    if len(aggregated_events) > 0:\n",
    "        event_cols = ['intersection', 'time_15min', 'total_event_count', 'vru_event_count',\n",
    "                      'vehicle_event_count', 'severity_weighted_score', 'I_VRU']\n",
    "        event_subset = aggregated_events[event_cols].copy()\n",
    "\n",
    "        # Left join to master\n",
    "        master = master.merge(\n",
    "            event_subset, on=['intersection', 'time_15min'], how='left')\n",
    "        print(f\"  + Safety events: {len(aggregated_events)} records merged\")\n",
    "    else:\n",
    "        master['total_event_count'] = 0\n",
    "        master['vru_event_count'] = 0\n",
    "        master['vehicle_event_count'] = 0\n",
    "        master['severity_weighted_score'] = 0\n",
    "        master['I_VRU'] = 0\n",
    "        print(\"  ⚠ No safety event data available\")\n",
    "\n",
    "    # Fill NaN values with 0 for count/intensity metrics\n",
    "    count_cols = ['vehicle_volume', 'vru_volume', 'psm_vru_count', 'pedestrian_count',\n",
    "                  'cyclist_count', 'total_event_count', 'vru_event_count',\n",
    "                  'vehicle_event_count', 'severity_weighted_score', 'I_VRU']\n",
    "\n",
    "    for col in count_cols:\n",
    "        if col in master.columns:\n",
    "            master[col] = master[col].fillna(0)\n",
    "\n",
    "    # Fill NaN values for continuous features with median\n",
    "    continuous_cols = ['avg_vru_speed']\n",
    "    for col in continuous_cols:\n",
    "        if col in master.columns and master[col].notna().any():\n",
    "            master[col] = master[col].fillna(master[col].median())\n",
    "\n",
    "    print(f\"\\n✓ Master feature table created: {len(master)} records\")\n",
    "    print(f\"  Columns: {len(master.columns)}\")\n",
    "    print(f\"  Intersections: {master['intersection'].nunique()}\")\n",
    "    print(\n",
    "        f\"  Date range: {master['time_15min'].min()} to {master['time_15min'].max()}\")\n",
    "\n",
    "    return master\n",
    "\n",
    "\n",
    "# Create the master feature table\n",
    "print(\"Creating master feature table from collected data...\")\n",
    "\n",
    "# Check which data sources we have\n",
    "data_sources_available = {\n",
    "    'bsm_features': 'bsm_features' in dir() and len(bsm_features) > 0,\n",
    "    'psm_features': 'psm_features' in dir() and len(psm_features) > 0,\n",
    "    'aggregated_events': 'aggregated_events' in dir() and len(aggregated_events) > 0,\n",
    "    'vehicle_counts': 'vehicle_counts' in dir() and len(vehicle_counts) > 0,\n",
    "    'vru_counts': 'vru_counts' in dir() and len(vru_counts) > 0\n",
    "}\n",
    "\n",
    "print(\"\\nData sources availability:\")\n",
    "for source, available in data_sources_available.items():\n",
    "    status = \"✓\" if available else \"✗\"\n",
    "    print(f\"  {status} {source}\")\n",
    "\n",
    "if data_sources_available['bsm_features']:\n",
    "    master_features = create_master_feature_table(\n",
    "        bsm_features=bsm_features,\n",
    "        psm_features=psm_features if data_sources_available['psm_features'] else pd.DataFrame(\n",
    "        ),\n",
    "        aggregated_events=aggregated_events if data_sources_available['aggregated_events'] else pd.DataFrame(\n",
    "        ),\n",
    "        vehicle_counts=vehicle_counts if data_sources_available['vehicle_counts'] else pd.DataFrame(\n",
    "        ),\n",
    "        vru_counts=vru_counts if data_sources_available['vru_counts'] else pd.DataFrame(\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if len(master_features) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"MASTER FEATURE TABLE SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nShape: {master_features.shape}\")\n",
    "        print(f\"\\nColumn names:\")\n",
    "        for i, col in enumerate(master_features.columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "        print(f\"\\nKey metrics summary:\")\n",
    "        key_metrics = ['vehicle_count', 'vehicle_volume', 'avg_speed', 'speed_variance',\n",
    "                       'hard_braking_count', 'vru_volume', 'I_VRU', 'total_event_count']\n",
    "        available_metrics = [\n",
    "            m for m in key_metrics if m in master_features.columns]\n",
    "        print(master_features[available_metrics].describe())\n",
    "\n",
    "        print(f\"\\nSample records:\")\n",
    "        display_cols = ['intersection', 'time_15min', 'vehicle_count', 'avg_speed',\n",
    "                        'I_VRU', 'total_event_count', 'vru_volume']\n",
    "        display_cols = [\n",
    "            c for c in display_cols if c in master_features.columns]\n",
    "        print(master_features[display_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n⚠ ERROR: Cannot create master feature table - BSM features are required\")\n",
    "    master_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786qw0rygee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Compute all normalization constants\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 5: NORMALIZATION CONSTANTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def compute_normalization_constants(master_features):\n",
    "    \"\"\"\n",
    "    Compute all normalization constants from the master feature table.\n",
    "\n",
    "    Constants computed:\n",
    "    - I_max: Maximum VRU conflict intensity (events per 15-min)\n",
    "    - V_max: Maximum vehicle volume per 15-min interval\n",
    "    - σ_max: Maximum speed variance\n",
    "    - S_ref: Reference speed (85th percentile of average speeds)\n",
    "    - N_VRU_max: Maximum VRU count per 15-min interval\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of normalization constants\n",
    "    \"\"\"\n",
    "\n",
    "    if len(master_features) == 0:\n",
    "        print(\"⚠ ERROR: No master features available\")\n",
    "        return {}\n",
    "\n",
    "    constants = {}\n",
    "\n",
    "    # I_max: Maximum VRU conflict intensity\n",
    "    if 'I_VRU' in master_features.columns:\n",
    "        constants['I_max'] = float(master_features['I_VRU'].max())\n",
    "        if constants['I_max'] == 0:\n",
    "            constants['I_max'] = 1.0  # Avoid division by zero\n",
    "            print(\"  ⚠ Warning: I_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "    else:\n",
    "        constants['I_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: I_VRU not found, setting I_max to 1.0\")\n",
    "\n",
    "    # V_max: Maximum vehicle volume (use both BSM vehicle_count and detector vehicle_volume)\n",
    "    v_sources = []\n",
    "    if 'vehicle_count' in master_features.columns:\n",
    "        v_sources.append(master_features['vehicle_count'].max())\n",
    "    if 'vehicle_volume' in master_features.columns:\n",
    "        v_sources.append(master_features['vehicle_volume'].max())\n",
    "\n",
    "    constants['V_max'] = float(max(v_sources)) if v_sources else 1.0\n",
    "    if constants['V_max'] == 0:\n",
    "        constants['V_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: V_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "\n",
    "    # σ_max: Maximum speed variance\n",
    "    if 'speed_variance' in master_features.columns:\n",
    "        constants['sigma_max'] = float(master_features['speed_variance'].max())\n",
    "        if constants['sigma_max'] == 0:\n",
    "            constants['sigma_max'] = 1.0\n",
    "            print(\"  ⚠ Warning: sigma_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "    else:\n",
    "        constants['sigma_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: speed_variance not found, setting sigma_max to 1.0\")\n",
    "\n",
    "    # S_ref: Reference speed (85th percentile)\n",
    "    if 'avg_speed' in master_features.columns:\n",
    "        # Filter out zeros and NaNs for realistic speed reference\n",
    "        valid_speeds = master_features['avg_speed'][master_features['avg_speed'] > 0]\n",
    "        if len(valid_speeds) > 0:\n",
    "            constants['S_ref'] = float(valid_speeds.quantile(0.85))\n",
    "            if constants['S_ref'] == 0:\n",
    "                constants['S_ref'] = 1.0\n",
    "                print(\"  ⚠ Warning: S_ref is 0, setting to 1.0\")\n",
    "        else:\n",
    "            constants['S_ref'] = 1.0\n",
    "            print(\"  ⚠ Warning: No valid speeds found, setting S_ref to 1.0\")\n",
    "    else:\n",
    "        constants['S_ref'] = 1.0\n",
    "        print(\"  ⚠ Warning: avg_speed not found, setting S_ref to 1.0\")\n",
    "\n",
    "    # N_VRU_max: Maximum VRU count (use both PSM and detector counts)\n",
    "    vru_sources = []\n",
    "    if 'psm_vru_count' in master_features.columns:\n",
    "        vru_sources.append(master_features['psm_vru_count'].max())\n",
    "    if 'vru_volume' in master_features.columns:\n",
    "        vru_sources.append(master_features['vru_volume'].max())\n",
    "    if 'pedestrian_count' in master_features.columns:\n",
    "        vru_sources.append(master_features['pedestrian_count'].max())\n",
    "    if 'cyclist_count' in master_features.columns:\n",
    "        vru_sources.append(master_features['cyclist_count'].max())\n",
    "\n",
    "    constants['N_VRU_max'] = float(max(vru_sources)) if vru_sources else 1.0\n",
    "    if constants['N_VRU_max'] == 0:\n",
    "        constants['N_VRU_max'] = 1.0\n",
    "        print(\"  ⚠ Warning: N_VRU_max is 0, setting to 1.0 to avoid division by zero\")\n",
    "\n",
    "    # Additional constants for Vehicle Index\n",
    "    # Hard braking rate normalization\n",
    "    if 'hard_braking_count' in master_features.columns:\n",
    "        constants['hard_braking_max'] = float(\n",
    "            master_features['hard_braking_count'].max())\n",
    "        if constants['hard_braking_max'] == 0:\n",
    "            constants['hard_braking_max'] = 1.0\n",
    "    else:\n",
    "        constants['hard_braking_max'] = 1.0\n",
    "\n",
    "    # Heading change rate normalization\n",
    "    if 'heading_change_rate' in master_features.columns:\n",
    "        constants['heading_change_max'] = float(\n",
    "            master_features['heading_change_rate'].max())\n",
    "        if constants['heading_change_max'] == 0:\n",
    "            constants['heading_change_max'] = 1.0\n",
    "    else:\n",
    "        constants['heading_change_max'] = 1.0\n",
    "\n",
    "    return constants\n",
    "\n",
    "\n",
    "# Compute normalization constants from master feature table\n",
    "if 'master_features' in dir() and len(master_features) > 0:\n",
    "    NORMALIZATION_CONSTANTS = compute_normalization_constants(master_features)\n",
    "\n",
    "    print(\"\\n✓ Normalization constants computed:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"  I_max (max VRU conflict intensity):    {NORMALIZATION_CONSTANTS['I_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  V_max (max vehicle volume):            {NORMALIZATION_CONSTANTS['V_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  σ_max (max speed variance):            {NORMALIZATION_CONSTANTS['sigma_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  S_ref (reference speed, 85th pct):     {NORMALIZATION_CONSTANTS['S_ref']:.2f} m/s\")\n",
    "    print(\n",
    "        f\"  N_VRU_max (max VRU count):             {NORMALIZATION_CONSTANTS['N_VRU_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  hard_braking_max:                      {NORMALIZATION_CONSTANTS['hard_braking_max']:.2f}\")\n",
    "    print(\n",
    "        f\"  heading_change_max:                    {NORMALIZATION_CONSTANTS['heading_change_max']:.2f}\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"These constants will be used to normalize features in the Safety Index formulas.\")\n",
    "    print(\"In production, these should be periodically recalibrated (e.g., monthly).\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ ERROR: Master feature table not available - cannot compute constants\")\n",
    "    NORMALIZATION_CONSTANTS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qcz5en3no6",
   "metadata": {},
   "source": [
    "## Phase 6: Safety Index Computation\n",
    "\n",
    "Implement the VRU, Vehicle, and Combined Safety Index formulas from the checkpoint document.\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "- **VRU Index** = 100 × [0.4×(I_VRU/I_max) + 0.2×(V/V_max) + 0.2×(S/S_ref) + 0.2×(σ_S/σ_max)]\n",
    "- **Vehicle Index** = 100 × [0.3×(I_vehicle/I_max) + 0.3×(V/V_max) + 0.2×(σ_S/σ_max) + 0.2×(hard_braking_rate)]\n",
    "- **Combined Index** = 0.6×VRU_Index + 0.4×Vehicle_Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lfeaaup8jup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6: Implement Safety Index formulas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 6: SAFETY INDEX COMPUTATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def compute_safety_indices(master_features, norm_constants):\n",
    "    \"\"\"\n",
    "    Compute VRU, Vehicle, and Combined Safety Indices for each 15-minute interval.\n",
    "\n",
    "    Formulas from checkpoint document:\n",
    "    - VRU Index = 100 × [0.4×(I_VRU/I_max) + 0.2×(V/V_max) + 0.2×(S/S_ref) + 0.2×(σ_S/σ_max)]\n",
    "    - Vehicle Index = 100 × [0.3×(I_vehicle/I_max) + 0.3×(V/V_max) + 0.2×(σ_S/σ_max) + 0.2×(hard_braking)]\n",
    "    - Combined Index = 0.6×VRU_Index + 0.4×Vehicle_Index\n",
    "\n",
    "    Args:\n",
    "        master_features: Master feature table with all metrics\n",
    "        norm_constants: Dictionary of normalization constants\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with computed indices added\n",
    "    \"\"\"\n",
    "\n",
    "    if len(master_features) == 0:\n",
    "        print(\"⚠ ERROR: No master features available\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not norm_constants:\n",
    "        print(\"⚠ ERROR: No normalization constants available\")\n",
    "        return master_features\n",
    "\n",
    "    df = master_features.copy()\n",
    "\n",
    "    # Extract normalization constants\n",
    "    I_max = norm_constants.get('I_max', 1.0)\n",
    "    V_max = norm_constants.get('V_max', 1.0)\n",
    "    sigma_max = norm_constants.get('sigma_max', 1.0)\n",
    "    S_ref = norm_constants.get('S_ref', 1.0)\n",
    "    N_VRU_max = norm_constants.get('N_VRU_max', 1.0)\n",
    "    hard_braking_max = norm_constants.get('hard_braking_max', 1.0)\n",
    "\n",
    "    print(\"Computing normalized components...\")\n",
    "\n",
    "    # ========== VRU Safety Index Components ==========\n",
    "\n",
    "    # Component 1: VRU conflict intensity (I_VRU/I_max)\n",
    "    df['I_VRU_norm'] = df['I_VRU'] / I_max if I_max > 0 else 0\n",
    "\n",
    "    # Component 2: Vehicle volume exposure (V/V_max)\n",
    "    # Use vehicle_count from BSM as primary, fall back to vehicle_volume from detectors\n",
    "    df['V'] = df['vehicle_count'].fillna(0)\n",
    "    if 'vehicle_volume' in df.columns:\n",
    "        df['V'] = df['V'].combine_first(df['vehicle_volume'])\n",
    "    df['V_norm'] = df['V'] / V_max if V_max > 0 else 0\n",
    "\n",
    "    # Component 3: Speed factor (S/S_ref)\n",
    "    df['S_norm'] = df['avg_speed'] / S_ref if S_ref > 0 else 0\n",
    "\n",
    "    # Component 4: Speed variance (σ_S/σ_max)\n",
    "    df['sigma_norm'] = df['speed_variance'] / sigma_max if sigma_max > 0 else 0\n",
    "\n",
    "    # Compute VRU Safety Index\n",
    "    df['VRU_Index'] = 100 * (\n",
    "        0.4 * df['I_VRU_norm'] +\n",
    "        0.2 * df['V_norm'] +\n",
    "        0.2 * df['S_norm'] +\n",
    "        0.2 * df['sigma_norm']\n",
    "    )\n",
    "\n",
    "    # Cap at 100\n",
    "    df['VRU_Index'] = df['VRU_Index'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ VRU Safety Index computed\")\n",
    "\n",
    "    # ========== Vehicle Safety Index Components ==========\n",
    "\n",
    "    # Component 1: Vehicle-vehicle conflict intensity\n",
    "    if 'vehicle_event_count' in df.columns:\n",
    "        df['I_vehicle'] = df['vehicle_event_count']\n",
    "    else:\n",
    "        # Fall back to total events minus VRU events\n",
    "        df['I_vehicle'] = df.get('total_event_count', 0) - \\\n",
    "            df.get('vru_event_count', 0)\n",
    "\n",
    "    df['I_vehicle_norm'] = df['I_vehicle'] / I_max if I_max > 0 else 0\n",
    "\n",
    "    # Component 2: Vehicle volume (same as VRU index)\n",
    "    # Already computed as V_norm\n",
    "\n",
    "    # Component 3: Speed variance (same as VRU index)\n",
    "    # Already computed as sigma_norm\n",
    "\n",
    "    # Component 4: Hard braking rate\n",
    "    if 'hard_braking_count' in df.columns:\n",
    "        df['hard_braking_norm'] = df['hard_braking_count'] / \\\n",
    "            hard_braking_max if hard_braking_max > 0 else 0\n",
    "    else:\n",
    "        df['hard_braking_norm'] = 0\n",
    "\n",
    "    # Compute Vehicle Safety Index\n",
    "    df['Vehicle_Index'] = 100 * (\n",
    "        0.3 * df['I_vehicle_norm'] +\n",
    "        0.3 * df['V_norm'] +\n",
    "        0.2 * df['sigma_norm'] +\n",
    "        0.2 * df['hard_braking_norm']\n",
    "    )\n",
    "\n",
    "    # Cap at 100\n",
    "    df['Vehicle_Index'] = df['Vehicle_Index'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ Vehicle Safety Index computed\")\n",
    "\n",
    "    # ========== Combined Safety Index ==========\n",
    "\n",
    "    df['Combined_Index'] = (\n",
    "        0.6 * df['VRU_Index'] +\n",
    "        0.4 * df['Vehicle_Index']\n",
    "    )\n",
    "\n",
    "    # Cap at 100\n",
    "    df['Combined_Index'] = df['Combined_Index'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ Combined Safety Index computed\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Compute safety indices\n",
    "if 'master_features' in dir() and len(master_features) > 0 and NORMALIZATION_CONSTANTS:\n",
    "    print(\"Computing safety indices for all 15-minute intervals...\")\n",
    "\n",
    "    indices_df = compute_safety_indices(\n",
    "        master_features, NORMALIZATION_CONSTANTS)\n",
    "\n",
    "    if len(indices_df) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SAFETY INDEX SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(f\"\\nVRU Safety Index statistics:\")\n",
    "        print(indices_df['VRU_Index'].describe())\n",
    "\n",
    "        print(f\"\\nVehicle Safety Index statistics:\")\n",
    "        print(indices_df['Vehicle_Index'].describe())\n",
    "\n",
    "        print(f\"\\nCombined Safety Index statistics:\")\n",
    "        print(indices_df['Combined_Index'].describe())\n",
    "\n",
    "        # Show distribution by intersection\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AVERAGE SAFETY INDICES BY INTERSECTION\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        intersection_summary = indices_df.groupby('intersection').agg({\n",
    "            'VRU_Index': ['mean', 'std', 'max'],\n",
    "            'Vehicle_Index': ['mean', 'std', 'max'],\n",
    "            'Combined_Index': ['mean', 'std', 'max'],\n",
    "            'time_15min': 'count'\n",
    "        }).round(2)\n",
    "\n",
    "        intersection_summary.columns = [\n",
    "            '_'.join(col).strip() for col in intersection_summary.columns.values]\n",
    "        intersection_summary.rename(\n",
    "            columns={'time_15min_count': 'num_intervals'}, inplace=True)\n",
    "\n",
    "        print(intersection_summary.to_string())\n",
    "\n",
    "        # Show highest risk intervals\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TOP 10 HIGHEST RISK INTERVALS (Combined Index)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        top_risk = indices_df.nlargest(10, 'Combined_Index')[\n",
    "            ['intersection', 'time_15min', 'VRU_Index', 'Vehicle_Index', 'Combined_Index',\n",
    "             'I_VRU', 'vehicle_count', 'avg_speed', 'speed_variance']\n",
    "        ]\n",
    "        print(top_risk.to_string(index=False))\n",
    "\n",
    "        # Show safest intervals\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"TOP 10 SAFEST INTERVALS (Combined Index)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        safest = indices_df.nsmallest(10, 'Combined_Index')[\n",
    "            ['intersection', 'time_15min', 'VRU_Index',\n",
    "                'Vehicle_Index', 'Combined_Index']\n",
    "        ]\n",
    "        print(safest.to_string(index=False))\n",
    "\n",
    "        # Time-of-day analysis\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AVERAGE SAFETY INDICES BY HOUR OF DAY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        hourly_summary = indices_df.groupby('hour_of_day').agg({\n",
    "            'VRU_Index': 'mean',\n",
    "            'Vehicle_Index': 'mean',\n",
    "            'Combined_Index': 'mean'\n",
    "        }).round(2)\n",
    "\n",
    "        print(hourly_summary.to_string())\n",
    "\n",
    "else:\n",
    "    print(\"⚠ ERROR: Cannot compute safety indices - missing master features or normalization constants\")\n",
    "    indices_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xv37m8scn2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7: Empirical Bayes stabilization\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 7: EMPIRICAL BAYES STABILIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def apply_empirical_bayes(indices_df, baseline_events, lambda_param=None):\n",
    "    \"\"\"\n",
    "    Apply Empirical Bayes adjustment to safety indices.\n",
    "\n",
    "    Adjusts raw indices based on:\n",
    "    1. Historical baseline risk at the intersection\n",
    "    2. Reliability of the current estimate (based on sample size)\n",
    "\n",
    "    Formula: Adjusted_Index = λ × Raw_Index + (1-λ) × Baseline_Index\n",
    "\n",
    "    Where λ = N / (N + k), with:\n",
    "    - N = number of observations in current period\n",
    "    - k = tuning parameter (default: 50 for 15-min intervals)\n",
    "\n",
    "    Args:\n",
    "        indices_df: DataFrame with computed raw indices\n",
    "        baseline_events: Historical safety events for baseline calculation\n",
    "        lambda_param: Fixed lambda value (0-1), or None for adaptive lambda\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with EB-adjusted indices added\n",
    "    \"\"\"\n",
    "\n",
    "    if len(indices_df) == 0:\n",
    "        print(\"⚠ No indices to adjust\")\n",
    "        return indices_df\n",
    "\n",
    "    df = indices_df.copy()\n",
    "\n",
    "    # Calculate historical baseline indices by intersection\n",
    "    print(\"Computing baseline risk scores from historical data...\")\n",
    "\n",
    "    if len(baseline_events) > 0:\n",
    "        # Calculate baseline severity-weighted event rate by intersection and time-of-day\n",
    "        baseline_events['hour_of_day'] = baseline_events['hour_of_day'].astype(\n",
    "            int)\n",
    "\n",
    "        baseline_summary = baseline_events.groupby(['intersection', 'hour_of_day']).agg({\n",
    "            'severity_weight': 'sum',\n",
    "            'event_id': 'count',\n",
    "            'is_vru_involved': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        baseline_summary.rename(columns={\n",
    "            'severity_weight': 'baseline_severity',\n",
    "            'event_id': 'baseline_event_count',\n",
    "            'is_vru_involved': 'baseline_vru_count'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Merge baseline with current data\n",
    "        df['hour_of_day'] = df['hour_of_day'].astype(int)\n",
    "        df = df.merge(baseline_summary, on=[\n",
    "                      'intersection', 'hour_of_day'], how='left')\n",
    "\n",
    "        # Fill missing baselines with intersection average\n",
    "        intersection_avg = baseline_summary.groupby('intersection').agg({\n",
    "            'baseline_severity': 'mean',\n",
    "            'baseline_event_count': 'mean',\n",
    "            'baseline_vru_count': 'mean'\n",
    "        })\n",
    "\n",
    "        for col in ['baseline_severity', 'baseline_event_count', 'baseline_vru_count']:\n",
    "            df[col] = df.apply(\n",
    "                lambda row: intersection_avg.loc[row['intersection'], col]\n",
    "                if pd.isna(row[col]) and row['intersection'] in intersection_avg.index\n",
    "                else row[col],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Fill any remaining NaNs with global mean\n",
    "        df['baseline_severity'] = df['baseline_severity'].fillna(\n",
    "            baseline_summary['baseline_severity'].mean())\n",
    "        df['baseline_event_count'] = df['baseline_event_count'].fillna(\n",
    "            baseline_summary['baseline_event_count'].mean())\n",
    "        df['baseline_vru_count'] = df['baseline_vru_count'].fillna(\n",
    "            baseline_summary['baseline_vru_count'].mean())\n",
    "\n",
    "        print(\n",
    "            f\"  ✓ Baseline computed for {len(baseline_summary)} intersection-hour combinations\")\n",
    "    else:\n",
    "        print(\"  ⚠ No baseline data available - using global mean\")\n",
    "        df['baseline_severity'] = df['severity_weighted_score'].mean(\n",
    "        ) if 'severity_weighted_score' in df.columns else 0\n",
    "        df['baseline_event_count'] = df['total_event_count'].mean(\n",
    "        ) if 'total_event_count' in df.columns else 0\n",
    "        df['baseline_vru_count'] = df['vru_event_count'].mean(\n",
    "        ) if 'vru_event_count' in df.columns else 0\n",
    "\n",
    "    # Calculate lambda (weight for raw vs baseline)\n",
    "    print(\"\\nCalculating Empirical Bayes weights...\")\n",
    "\n",
    "    if lambda_param is not None:\n",
    "        # Use fixed lambda\n",
    "        df['lambda'] = lambda_param\n",
    "        print(f\"  Using fixed lambda = {lambda_param}\")\n",
    "    else:\n",
    "        # Adaptive lambda based on sample size\n",
    "        # Lambda = N / (N + k), where k is a tuning parameter\n",
    "        k = 50  # Tuning parameter: higher k = more shrinkage toward baseline\n",
    "\n",
    "        # Use vehicle count as proxy for sample size (more vehicles = more reliable estimate)\n",
    "        df['N'] = df['vehicle_count'].fillna(\n",
    "            0) + 1  # Add 1 to avoid division by zero\n",
    "        df['lambda'] = df['N'] / (df['N'] + k)\n",
    "\n",
    "        print(f\"  Using adaptive lambda with k={k}\")\n",
    "        print(\n",
    "            f\"  Lambda range: {df['lambda'].min():.3f} to {df['lambda'].max():.3f}\")\n",
    "        print(f\"  Mean lambda: {df['lambda'].mean():.3f}\")\n",
    "\n",
    "    # Convert baseline to index scale (approximate)\n",
    "    # Use baseline event count relative to maximum as a simple baseline index\n",
    "    max_baseline = df['baseline_event_count'].max()\n",
    "    df['baseline_index'] = (df['baseline_event_count'] /\n",
    "                            max_baseline * 100) if max_baseline > 0 else 20.0\n",
    "    df['baseline_index'] = df['baseline_index'].fillna(\n",
    "        20.0)  # Default baseline = 20 (low-medium risk)\n",
    "\n",
    "    # Apply Empirical Bayes adjustment\n",
    "    print(\"\\nApplying Empirical Bayes adjustment...\")\n",
    "\n",
    "    df['VRU_Index_EB'] = df['lambda'] * df['VRU_Index'] + \\\n",
    "        (1 - df['lambda']) * df['baseline_index']\n",
    "    df['Vehicle_Index_EB'] = df['lambda'] * df['Vehicle_Index'] + \\\n",
    "        (1 - df['lambda']) * df['baseline_index']\n",
    "    df['Combined_Index_EB'] = df['lambda'] * df['Combined_Index'] + \\\n",
    "        (1 - df['lambda']) * df['baseline_index']\n",
    "\n",
    "    # Cap at 100\n",
    "    df['VRU_Index_EB'] = df['VRU_Index_EB'].clip(0, 100)\n",
    "    df['Vehicle_Index_EB'] = df['Vehicle_Index_EB'].clip(0, 100)\n",
    "    df['Combined_Index_EB'] = df['Combined_Index_EB'].clip(0, 100)\n",
    "\n",
    "    print(\"  ✓ Empirical Bayes adjustment applied\")\n",
    "\n",
    "    # Calculate adjustment magnitude\n",
    "    df['EB_adjustment'] = df['Combined_Index'] - df['Combined_Index_EB']\n",
    "\n",
    "    print(f\"\\n  Adjustment statistics:\")\n",
    "    print(f\"    Mean adjustment: {df['EB_adjustment'].mean():.2f}\")\n",
    "    print(f\"    Std adjustment: {df['EB_adjustment'].std():.2f}\")\n",
    "    print(f\"    Max upward adjustment: {df['EB_adjustment'].min():.2f}\")\n",
    "    print(f\"    Max downward adjustment: {df['EB_adjustment'].max():.2f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply Empirical Bayes stabilization\n",
    "if 'indices_df' in dir() and len(indices_df) > 0:\n",
    "    print(\"Applying Empirical Bayes stabilization to safety indices...\")\n",
    "\n",
    "    baseline_data = baseline_events if 'baseline_events' in dir() and len(\n",
    "        baseline_events) > 0 else pd.DataFrame()\n",
    "\n",
    "    indices_df_eb = apply_empirical_bayes(\n",
    "        indices_df, baseline_data, lambda_param=None)\n",
    "\n",
    "    if len(indices_df_eb) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EMPIRICAL BAYES ADJUSTED INDICES SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(f\"\\nVRU Safety Index (EB-adjusted):\")\n",
    "        print(indices_df_eb['VRU_Index_EB'].describe())\n",
    "\n",
    "        print(f\"\\nVehicle Safety Index (EB-adjusted):\")\n",
    "        print(indices_df_eb['Vehicle_Index_EB'].describe())\n",
    "\n",
    "        print(f\"\\nCombined Safety Index (EB-adjusted):\")\n",
    "        print(indices_df_eb['Combined_Index_EB'].describe())\n",
    "\n",
    "        # Compare raw vs EB-adjusted\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPARISON: RAW vs EB-ADJUSTED INDICES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        comparison = pd.DataFrame({\n",
    "            'Metric': ['VRU Index', 'Vehicle Index', 'Combined Index'],\n",
    "            'Raw Mean': [\n",
    "                indices_df_eb['VRU_Index'].mean(),\n",
    "                indices_df_eb['Vehicle_Index'].mean(),\n",
    "                indices_df_eb['Combined_Index'].mean()\n",
    "            ],\n",
    "            'EB Mean': [\n",
    "                indices_df_eb['VRU_Index_EB'].mean(),\n",
    "                indices_df_eb['Vehicle_Index_EB'].mean(),\n",
    "                indices_df_eb['Combined_Index_EB'].mean()\n",
    "            ],\n",
    "            'Raw Std': [\n",
    "                indices_df_eb['VRU_Index'].std(),\n",
    "                indices_df_eb['Vehicle_Index'].std(),\n",
    "                indices_df_eb['Combined_Index'].std()\n",
    "            ],\n",
    "            'EB Std': [\n",
    "                indices_df_eb['VRU_Index_EB'].std(),\n",
    "                indices_df_eb['Vehicle_Index_EB'].std(),\n",
    "                indices_df_eb['Combined_Index_EB'].std()\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        print(comparison.round(2).to_string(index=False))\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"IMPACT OF EMPIRICAL BAYES ADJUSTMENT\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Note: EB adjustment typically:\")\n",
    "        print(\"  • Reduces variance (more stable estimates)\")\n",
    "        print(\"  • Shrinks extreme values toward baseline\")\n",
    "        print(\"  • Improves prediction accuracy for low-sample situations\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\"⚠ ERROR: No indices available for Empirical Bayes adjustment\")\n",
    "    indices_df_eb = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkta9g8m0m",
   "metadata": {},
   "source": [
    "## Phase 8: Validation & Next Steps\n",
    "\n",
    "### Validation Approaches\n",
    "\n",
    "**1. Predictive Validation**\n",
    "\n",
    "- Split data into training (historical) and testing (recent) periods\n",
    "- Test if high-index intervals predict future safety events\n",
    "- Metrics: ROC-AUC, precision-recall curves\n",
    "\n",
    "**2. Sensitivity Analysis**\n",
    "\n",
    "- Vary formula weights (e.g., VRU vs Vehicle Index contributions)\n",
    "- Test different Empirical Bayes k parameters\n",
    "- Assess robustness to normalization constant changes\n",
    "\n",
    "**3. Expert Review**\n",
    "\n",
    "- Present high-risk intervals to traffic safety experts\n",
    "- Validate against known problematic intersections\n",
    "- Incorporate domain knowledge for weight calibration\n",
    "\n",
    "**4. Comparison with Baseline Methods**\n",
    "\n",
    "- Compare against simple crash rate\n",
    "- Benchmark against national safety metrics (e.g., USDOT FAST tool)\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "**Backend (FastAPI)**\n",
    "\n",
    "- [ ] Refactor data collection functions into API endpoints\n",
    "- [ ] Implement PostgreSQL schema for feature storage\n",
    "- [ ] Set up Cloud Scheduler for 15-minute triggers\n",
    "- [ ] Add caching layer (Redis) for normalization constants\n",
    "- [ ] Implement error handling and data quality checks\n",
    "- [ ] Add logging and monitoring (Cloud Logging)\n",
    "\n",
    "**Frontend (Streamlit)**\n",
    "\n",
    "- [ ] Create interactive map visualization (Folium/Pydeck)\n",
    "- [ ] Build real-time dashboard with index charts\n",
    "- [ ] Add intersection comparison tool\n",
    "- [ ] Implement time-series plots and trend analysis\n",
    "- [ ] Add download functionality for reports\n",
    "\n",
    "**Infrastructure**\n",
    "\n",
    "- [ ] Deploy to Google Cloud Run (auto-scaling)\n",
    "- [ ] Set up Cloud SQL PostgreSQL with PostGIS\n",
    "- [ ] Configure VPC for secure Trino access\n",
    "- [ ] Implement CI/CD pipeline (GitHub Actions)\n",
    "- [ ] Set up monitoring and alerting\n",
    "\n",
    "### Calibration & Tuning\n",
    "\n",
    "**Normalization Constants**: Recalibrate monthly using rolling 3-6 month window\n",
    "\n",
    "**Empirical Bayes k parameter**: Tune via cross-validation\n",
    "\n",
    "- Lower k (e.g., 20) → more responsive to recent data\n",
    "- Higher k (e.g., 100) → more stable, less noisy\n",
    "\n",
    "**Formula Weights**: Consider intersection-specific tuning\n",
    "\n",
    "- High-speed corridors → increase speed variance weight\n",
    "- School zones → increase VRU component weight\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "1. **Predictive Modeling**: Train ML model to forecast next-interval risk\n",
    "2. **Weather Integration**: Incorporate precipitation, visibility data\n",
    "3. **Event Detection**: Real-time alerting for extreme index values\n",
    "4. **Mobile App**: Push notifications for high-risk conditions\n",
    "5. **Policy Evaluation**: Before/after analysis for interventions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wby6llsrwk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE END-TO-END WORKFLOW: Safety Index Computation\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATE_RANGE_DAYS = 7              # Number of days to analyze\n",
    "# None = all intersections, or specify like \"glebe-potomac\"\n",
    "TARGET_INTERSECTION = None\n",
    "EXPORT_RESULTS = True            # Set to False to skip CSV export\n",
    "EXPORT_PATH = None               # Auto-generate if None\n",
    "CREATE_VISUALIZATIONS = True     # Set to False to skip plots\n",
    "\n",
    "# Time range\n",
    "end_dt = datetime.now()\n",
    "start_dt = end_dt - timedelta(days=DATE_RANGE_DAYS)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VIRGINIA TRANSPORTATION SAFETY INDEX - END-TO-END WORKFLOW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(\n",
    "    f\"  Date range: {start_dt.date()} to {end_dt.date()} ({DATE_RANGE_DAYS} days)\")\n",
    "print(f\"  Target intersection: {TARGET_INTERSECTION or 'ALL'}\")\n",
    "print(f\"  Export results: {EXPORT_RESULTS}\")\n",
    "print(f\"  Create visualizations: {CREATE_VISUALIZATIONS}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: DATA QUALITY ASSESSMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/7] Assessing data quality...\")\n",
    "\n",
    "try:\n",
    "    tables_to_check = [\n",
    "        ('alexandria', 'bsm', 'publish_timestamp'),\n",
    "        ('alexandria', 'psm', 'publish_timestamp'),\n",
    "        ('alexandria', 'safety-event', 'time_at_site'),\n",
    "        ('alexandria', 'vehicle-count', 'publish_timestamp'),\n",
    "        ('alexandria', 'vru-count', 'publish_timestamp'),\n",
    "    ]\n",
    "\n",
    "    quality_results = []\n",
    "    for schema, table, ts_col in tables_to_check:\n",
    "        result = check_table_coverage(schema, table, ts_col)\n",
    "        quality_results.append(result)\n",
    "\n",
    "    print(f\"  ✓ Quality assessment complete\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Warning: Quality assessment failed: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: HISTORICAL BASELINE CONSTRUCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/7] Collecting historical baseline data...\")\n",
    "\n",
    "try:\n",
    "    # Collect severity-weighted safety events\n",
    "    baseline_events_wf = collect_baseline_events(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    # Collect exposure metrics\n",
    "    vehicle_counts_wf, vru_counts_wf = collect_exposure_metrics(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Baseline: {len(baseline_events_wf)} events, {len(vehicle_counts_wf)} vehicle counts, {len(vru_counts_wf)} VRU counts\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Baseline collection failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/7] Engineering features from BSM, PSM, and safety events...\")\n",
    "\n",
    "try:\n",
    "    # BSM-derived vehicle features\n",
    "    bsm_features_wf = collect_bsm_features(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    # PSM-derived VRU features\n",
    "    psm_features_wf = collect_psm_features(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    # Aggregate safety events\n",
    "    aggregated_events_wf = aggregate_safety_events(\n",
    "        intersection=TARGET_INTERSECTION,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt\n",
    "    )\n",
    "\n",
    "    print(f\"  ✓ Features: {len(bsm_features_wf)} BSM intervals, {len(psm_features_wf)} PSM intervals, {len(aggregated_events_wf)} event intervals\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Feature engineering failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4: MASTER FEATURE TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/7] Creating master feature table...\")\n",
    "\n",
    "try:\n",
    "    master_features_wf = create_master_feature_table(\n",
    "        bsm_features=bsm_features_wf,\n",
    "        psm_features=psm_features_wf,\n",
    "        aggregated_events=aggregated_events_wf,\n",
    "        vehicle_counts=vehicle_counts_wf,\n",
    "        vru_counts=vru_counts_wf\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Master table: {len(master_features_wf)} 15-minute intervals across {master_features_wf['intersection'].nunique()} intersections\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Master table creation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5: NORMALIZATION CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/7] Computing normalization constants...\")\n",
    "\n",
    "try:\n",
    "    norm_constants_wf = compute_normalization_constants(master_features_wf)\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Constants: I_max={norm_constants_wf['I_max']:.1f}, V_max={norm_constants_wf['V_max']:.1f}, σ_max={norm_constants_wf['sigma_max']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Normalization computation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 6: SAFETY INDEX COMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/7] Computing safety indices...\")\n",
    "\n",
    "try:\n",
    "    indices_wf = compute_safety_indices(master_features_wf, norm_constants_wf)\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ Indices computed: Mean Combined Index = {indices_wf['Combined_Index'].mean():.1f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Index computation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 7: EMPIRICAL BAYES STABILIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/7] Applying Empirical Bayes stabilization...\")\n",
    "\n",
    "try:\n",
    "    indices_eb_wf = apply_empirical_bayes(\n",
    "        indices_wf, baseline_events_wf, lambda_param=None)\n",
    "\n",
    "    print(\n",
    "        f\"  ✓ EB adjustment applied: Mean EB Combined Index = {indices_eb_wf['Combined_Index_EB'].mean():.1f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error: Empirical Bayes adjustment failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Data Coverage:\")\n",
    "print(f\"  Total 15-minute intervals: {len(indices_eb_wf)}\")\n",
    "print(f\"  Intersections analyzed: {indices_eb_wf['intersection'].nunique()}\")\n",
    "print(\n",
    "    f\"  Date range: {indices_eb_wf['time_15min'].min()} to {indices_eb_wf['time_15min'].max()}\")\n",
    "\n",
    "print(f\"\\n🎯 Safety Index Statistics (EB-Adjusted):\")\n",
    "print(f\"  Combined Index:\")\n",
    "print(f\"    Mean: {indices_eb_wf['Combined_Index_EB'].mean():.2f}\")\n",
    "print(f\"    Std:  {indices_eb_wf['Combined_Index_EB'].std():.2f}\")\n",
    "print(f\"    Min:  {indices_eb_wf['Combined_Index_EB'].min():.2f}\")\n",
    "print(f\"    Max:  {indices_eb_wf['Combined_Index_EB'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n  VRU Index:\")\n",
    "print(f\"    Mean: {indices_eb_wf['VRU_Index_EB'].mean():.2f}\")\n",
    "print(f\"    Max:  {indices_eb_wf['VRU_Index_EB'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n  Vehicle Index:\")\n",
    "print(f\"    Mean: {indices_eb_wf['Vehicle_Index_EB'].mean():.2f}\")\n",
    "print(f\"    Max:  {indices_eb_wf['Vehicle_Index_EB'].max():.2f}\")\n",
    "\n",
    "# Top 10 highest risk intervals\n",
    "print(f\"\\n⚠️  TOP 10 HIGHEST RISK INTERVALS:\")\n",
    "top_risk = indices_eb_wf.nlargest(10, 'Combined_Index_EB')[\n",
    "    ['intersection', 'time_15min', 'Combined_Index_EB',\n",
    "        'VRU_Index_EB', 'Vehicle_Index_EB', 'I_VRU', 'vehicle_count']\n",
    "]\n",
    "print(top_risk.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "if EXPORT_RESULTS:\n",
    "    print(f\"\\n📁 Exporting results...\")\n",
    "\n",
    "    if EXPORT_PATH is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        EXPORT_PATH = f'safety_indices_{timestamp}.csv'\n",
    "\n",
    "    # Select columns for export\n",
    "    export_cols = [\n",
    "        'intersection', 'time_15min', 'hour_of_day', 'day_of_week',\n",
    "        'Combined_Index_EB', 'VRU_Index_EB', 'Vehicle_Index_EB',\n",
    "        'Combined_Index', 'VRU_Index', 'Vehicle_Index',\n",
    "        'vehicle_count', 'avg_speed', 'speed_variance', 'hard_braking_count',\n",
    "        'I_VRU', 'total_event_count', 'vru_event_count', 'vehicle_event_count',\n",
    "        'vru_volume', 'vehicle_volume'\n",
    "    ]\n",
    "\n",
    "    # Only include columns that exist\n",
    "    export_cols = [c for c in export_cols if c in indices_eb_wf.columns]\n",
    "\n",
    "    indices_eb_wf[export_cols].to_csv(EXPORT_PATH, index=False)\n",
    "    print(f\"  ✓ Results exported to: {EXPORT_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "if CREATE_VISUALIZATIONS:\n",
    "    print(f\"\\n📈 Creating visualizations...\")\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        # 1. Time series plot\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "        # If analyzing specific intersection, show detailed view\n",
    "        if TARGET_INTERSECTION:\n",
    "            data = indices_eb_wf[indices_eb_wf['intersection']\n",
    "                                 == TARGET_INTERSECTION].sort_values('time_15min')\n",
    "        else:\n",
    "            # Show average across all intersections\n",
    "            data = indices_eb_wf.groupby('time_15min').agg({\n",
    "                'Combined_Index_EB': 'mean',\n",
    "                'VRU_Index_EB': 'mean',\n",
    "                'Vehicle_Index_EB': 'mean'\n",
    "            }).reset_index().sort_values('time_15min')\n",
    "\n",
    "        # Combined Index\n",
    "        axes[0].plot(data['time_15min'], data['Combined_Index_EB'],\n",
    "                     linewidth=2, color='purple')\n",
    "        axes[0].axhline(y=50, color='orange', linestyle='--',\n",
    "                        alpha=0.5, label='Medium Risk')\n",
    "        axes[0].axhline(y=70, color='red', linestyle='--',\n",
    "                        alpha=0.5, label='High Risk')\n",
    "        axes[0].set_ylabel('Combined Index')\n",
    "        axes[0].set_title(\n",
    "            f'Safety Indices Over Time - {TARGET_INTERSECTION or \"All Intersections (Average)\"}')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend()\n",
    "\n",
    "        # VRU Index\n",
    "        axes[1].plot(data['time_15min'], data['VRU_Index_EB'],\n",
    "                     linewidth=2, color='blue')\n",
    "        axes[1].axhline(y=50, color='orange', linestyle='--', alpha=0.5)\n",
    "        axes[1].axhline(y=70, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1].set_ylabel('VRU Index')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Vehicle Index\n",
    "        axes[2].plot(data['time_15min'], data['Vehicle_Index_EB'],\n",
    "                     linewidth=2, color='green')\n",
    "        axes[2].axhline(y=50, color='orange', linestyle='--', alpha=0.5)\n",
    "        axes[2].axhline(y=70, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[2].set_ylabel('Vehicle Index')\n",
    "        axes[2].set_xlabel('Time')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 2. Heatmap: Average index by hour and day of week\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        pivot = indices_eb_wf.groupby(['hour_of_day', 'day_of_week'])[\n",
    "            'Combined_Index_EB'].mean().reset_index()\n",
    "        heatmap_data = pivot.pivot(\n",
    "            index='hour_of_day', columns='day_of_week', values='Combined_Index_EB')\n",
    "\n",
    "        sns.heatmap(heatmap_data, cmap='RdYlGn_r', annot=True, fmt='.1f',\n",
    "                    xticklabels=['Mon', 'Tue', 'Wed',\n",
    "                                 'Thu', 'Fri', 'Sat', 'Sun'],\n",
    "                    yticklabels=range(24), cbar_kws={'label': 'Combined Index'})\n",
    "        plt.title('Average Safety Index by Hour and Day of Week')\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.ylabel('Hour of Day')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 3. Distribution histogram\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "        for idx, (col, ax, color) in enumerate(zip(\n",
    "            ['Combined_Index_EB', 'VRU_Index_EB', 'Vehicle_Index_EB'],\n",
    "            axes,\n",
    "            ['purple', 'blue', 'green']\n",
    "        )):\n",
    "            ax.hist(indices_eb_wf[col], bins=30,\n",
    "                    edgecolor='black', alpha=0.7, color=color)\n",
    "            mean_val = indices_eb_wf[col].mean()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--',\n",
    "                       linewidth=2, label=f'Mean: {mean_val:.1f}')\n",
    "            ax.set_xlabel('Index Value')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(col.replace('_EB', '').replace('_', ' '))\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"  ✓ Visualizations created\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(f\"  ⚠ Matplotlib not available - skipping visualizations\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Visualization error: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ WORKFLOW COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\n",
    "    f\"  1. Review the exported CSV: {EXPORT_PATH if EXPORT_RESULTS else '(export disabled)'}\")\n",
    "print(f\"  2. Analyze high-risk intervals for intervention opportunities\")\n",
    "print(f\"  3. Compare indices across intersections to prioritize resources\")\n",
    "print(f\"  4. Use for real-time monitoring or policy evaluation\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0pb95w4gl7i",
   "metadata": {},
   "source": [
    "## 🚀 Complete End-to-End Workflow\n",
    "\n",
    "Run this cell to execute the entire safety index computation pipeline with one click.\n",
    "\n",
    "**Configurable parameters:**\n",
    "\n",
    "- `DATE_RANGE_DAYS`: Number of days to analyze (default: 7)\n",
    "- `TARGET_INTERSECTION`: Specific intersection or `None` for all (default: None)\n",
    "- `EXPORT_PATH`: Where to save results CSV (default: auto-generated filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014xkldj59x0j",
   "metadata": {},
   "source": [
    "## Phase 7: Empirical Bayes Stabilization\n",
    "\n",
    "Apply Empirical Bayes adjustment to reduce noise in low-sample-size situations.\n",
    "\n",
    "**Formula:** `Adjusted_Index = λ × Raw_Index + (1-λ) × Baseline_Index`\n",
    "\n",
    "Where λ depends on sample size (more data → higher λ → trust raw index more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a24sfh9s7",
   "metadata": {},
   "source": [
    "## Phase 5: Normalization Constants\n",
    "\n",
    "Compute all normalization constants required for the Safety Index formulas.\n",
    "\n",
    "These constants scale raw metrics to [0, 1] range before combining into the final index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dys0z0tzlxm",
   "metadata": {},
   "source": [
    "## Phase 4: Master Feature Table Construction\n",
    "\n",
    "Join all feature sources into a single unified table aligned on 15-minute intervals.\n",
    "\n",
    "**This becomes the core data model in PostgreSQL for the production system.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urnn6km9vok",
   "metadata": {},
   "source": [
    "## Phase 3: Feature Engineering\n",
    "\n",
    "Extract and aggregate features from BSM, PSM, and safety event data at 15-minute intervals.\n",
    "\n",
    "**This becomes the core data processing pipeline in the FastAPI backend.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sanjfeusrfs",
   "metadata": {},
   "source": [
    "## Phase 2: Historical Baseline Construction\n",
    "\n",
    "Collect severity-weighted crash data to establish baseline risk scores for Empirical Bayes stabilization.\n",
    "\n",
    "**Note**: This prototype will become the `build_baseline()` function in the FastAPI backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7xzmlervelo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1.1: Check temporal coverage and data availability for each key table\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Function to safely check table coverage\n",
    "\n",
    "\n",
    "def check_table_coverage(schema, table, timestamp_col='publish_timestamp'):\n",
    "    \"\"\"\n",
    "    Check temporal coverage and record counts for a table.\n",
    "    Handles both bigint timestamps and actual timestamp columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try with publish_timestamp first\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT intersection) as num_intersections,\n",
    "            MIN({timestamp_col}) as earliest_ts,\n",
    "            MAX({timestamp_col}) as latest_ts\n",
    "        FROM {schema}.\"{table}\"\n",
    "        WHERE {timestamp_col} > 0 AND {timestamp_col} < 9999999999999999\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        result = cur.fetchone()\n",
    "\n",
    "        if result and result[0] > 0:\n",
    "            # Convert timestamps if they're bigints\n",
    "            try:\n",
    "                earliest = datetime.fromtimestamp(\n",
    "                    result[2] / 1000) if result[2] and result[2] > 999999999999 else result[2]\n",
    "                latest = datetime.fromtimestamp(\n",
    "                    result[3] / 1000) if result[3] and result[3] > 999999999999 else result[3]\n",
    "            except:\n",
    "                earliest = result[2]\n",
    "                latest = result[3]\n",
    "\n",
    "            return {\n",
    "                'schema': schema,\n",
    "                'table': table,\n",
    "                'total_records': result[0],\n",
    "                'num_intersections': result[1],\n",
    "                'earliest': earliest,\n",
    "                'latest': latest,\n",
    "                'status': 'OK'\n",
    "            }\n",
    "        else:\n",
    "            return {'schema': schema, 'table': table, 'status': 'EMPTY'}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'schema': schema, 'table': table, 'status': f'ERROR: {str(e)[:100]}'}\n",
    "\n",
    "\n",
    "# Check all key tables\n",
    "tables_to_check = [\n",
    "    ('alexandria', 'bsm', 'publish_timestamp'),\n",
    "    ('alexandria', 'psm', 'publish_timestamp'),\n",
    "    # Note: using time_at_site not publish_timestamp\n",
    "    ('alexandria', 'safety-event', 'time_at_site'),\n",
    "    ('alexandria', 'vehicle-count', 'publish_timestamp'),\n",
    "    ('alexandria', 'vru-count', 'publish_timestamp'),\n",
    "]\n",
    "\n",
    "coverage_results = []\n",
    "for schema, table, ts_col in tables_to_check:\n",
    "    print(f\"\\nChecking {schema}.{table}...\")\n",
    "    result = check_table_coverage(schema, table, ts_col)\n",
    "    coverage_results.append(result)\n",
    "\n",
    "    if result['status'] == 'OK':\n",
    "        print(f\"  ✓ Records: {result['total_records']:,}\")\n",
    "        print(f\"  ✓ Intersections: {result['num_intersections']}\")\n",
    "        print(f\"  ✓ Date range: {result['earliest']} to {result['latest']}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Status: {result['status']}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "coverage_df = pd.DataFrame(coverage_results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COVERAGE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(coverage_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ly2weq7ep8f",
   "metadata": {},
   "source": [
    "## Phase 1: Data Quality Assessment\n",
    "\n",
    "Before collecting data, we need to assess what's actually available and identify any quality issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fp7yfnertgs",
   "metadata": {},
   "source": [
    "# DATA COLLECTION FOR SAFETY INDEX\n",
    "\n",
    "This section implements the systematic data collection workflow for the Virginia Transportation Safety Index (VTSI).\n",
    "\n",
    "## Project Requirements\n",
    "\n",
    "Based on the checkpoint document, we need to compute three indices refreshed every 15 minutes:\n",
    "\n",
    "1. **VRU Safety Index**: Risk to pedestrians and cyclists\n",
    "2. **Vehicle Safety Index**: Risk of vehicle-vehicle collisions\n",
    "3. **Combined Safety Index**: Weighted combination of both\n",
    "\n",
    "## Data Collection Phases\n",
    "\n",
    "1. **Phase 1**: Data source verification and quality assessment\n",
    "2. **Phase 2**: Historical baseline construction (severity-weighted crashes)\n",
    "3. **Phase 3**: Feature engineering (BSM, PSM, safety events, counts)\n",
    "4. **Phase 4**: Safety index computation with normalization\n",
    "5. **Phase 5**: Validation and calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwkhhf8mcj6",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Notebook vs. Production Architecture\n",
    "\n",
    "### This Notebook's Purpose\n",
    "\n",
    "This notebook is for **development, prototyping, and demonstration** purposes. It allows us to:\n",
    "\n",
    "- Validate data collection queries and logic\n",
    "- Explore data quality issues\n",
    "- Test index computation formulas\n",
    "- Demonstrate the methodology to stakeholders\n",
    "\n",
    "### Production Architecture (Eventual Deployment)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Production System                        │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                               │\n",
    "│  ┌──────────────┐      ┌────────────────────────────────┐  │\n",
    "│  │ Cloud        │      │   FastAPI Backend              │  │\n",
    "│  │ Scheduler    │─────>│   (Google Cloud Run)           │  │\n",
    "│  │ (Every 15min)│      │                                 │  │\n",
    "│  └──────────────┘      │   - Data Collection Service    │  │\n",
    "│                        │   - Index Computation Service   │  │\n",
    "│                        │   - Feature Engineering         │  │\n",
    "│                        └──────────┬─────────────────────┘  │\n",
    "│                                   │                          │\n",
    "│                                   ↓                          │\n",
    "│                        ┌──────────────────────────┐         │\n",
    "│                        │  PostgreSQL + PostGIS    │         │\n",
    "│                        │  (Google Cloud SQL)      │         │\n",
    "│                        │                          │         │\n",
    "│                        │  - safety_index_features │         │\n",
    "│                        │  - safety_index_computed │         │\n",
    "│                        │  - intersection_baselines│         │\n",
    "│                        └──────────┬───────────────┘         │\n",
    "│                                   │                          │\n",
    "│                                   ↓                          │\n",
    "│                        ┌──────────────────────────┐         │\n",
    "│                        │  Streamlit Frontend      │         │\n",
    "│                        │  (Google Cloud Run)      │         │\n",
    "│                        │                          │         │\n",
    "│                        │  - Interactive Map       │         │\n",
    "│                        │  - Real-time Dashboard   │         │\n",
    "│                        │  - VRU/Vehicle Views     │         │\n",
    "│                        └──────────────────────────┘         │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### API Endpoints (To Be Implemented)\n",
    "\n",
    "- `POST /api/collect-data` - Triggered every 15 minutes to collect and process new data\n",
    "- `GET /api/safety-index?intersection=X&time=Y` - Query computed indices\n",
    "- `GET /api/features?intersection=X` - Get raw features for debugging\n",
    "- `GET /api/intersections` - List available intersections\n",
    "\n",
    "### Event-Driven Data Collection Flow\n",
    "\n",
    "1. **Cloud Scheduler** triggers `/api/collect-data` every 15 minutes\n",
    "2. **FastAPI service** queries Trino API for latest BSM, PSM, events, counts\n",
    "3. **Feature engineering** computes aggregates and derives metrics\n",
    "4. **Index computation** applies formulas and stores results in PostgreSQL\n",
    "5. **Frontend** queries PostgreSQL for visualization (not Trino directly)\n",
    "\n",
    "**Note**: The code developed in this notebook will be refactored into the FastAPI backend services.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
