{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Verification: VCC vs Local Storage\n",
    "\n",
    "This notebook verifies what intersections are available from VCC and compares with what we have stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check VCC API - Available Intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VCC API Configuration\n",
    "BASE_URL = \"https://vcc.vtti.vt.edu\"\n",
    "TOKEN_URL = f\"{BASE_URL}/api/auth/client\"\n",
    "CLIENT_ID = 'course-cs6604-student-djjay'\n",
    "CLIENT_SECRET = 'wHqQjvksKE6rYLYedkuIqewrFtEOpjHH'\n",
    "\n",
    "# Get access token\n",
    "def get_access_token():\n",
    "    data = {\n",
    "        'client_id': CLIENT_ID,\n",
    "        'client_secret': CLIENT_SECRET\n",
    "    }\n",
    "    \n",
    "    response = requests.post(TOKEN_URL, data=data, allow_redirects=False)\n",
    "    response.raise_for_status()\n",
    "    token_data = response.json()\n",
    "    access_token = token_data.get('access_token')\n",
    "    expires_in = token_data.get('expires_in')\n",
    "    \n",
    "    print(f\"‚úì Access token obtained (expires in {expires_in/60:.1f} minutes)\")\n",
    "    return access_token\n",
    "\n",
    "access_token = get_access_token()\n",
    "headers = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all MapData from VCC\n",
    "def get_all_mapdata():\n",
    "    url = f\"{BASE_URL}/api/mapdata/decoded\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    print(f\"‚úì Retrieved {len(data)} MapData messages from VCC\")\n",
    "    return data\n",
    "\n",
    "mapdata_list = get_all_mapdata()\n",
    "\n",
    "# Parse intersections\n",
    "vcc_intersections = []\n",
    "for md in mapdata_list:\n",
    "    if 'intersections' in md and len(md['intersections']) > 0:\n",
    "        intersection = md['intersections'][0]\n",
    "        vcc_intersections.append({\n",
    "            'Intersection ID': intersection['id']['id'],\n",
    "            'Revision': intersection['revision'],\n",
    "            'Latitude': intersection['refPoint']['lat'],\n",
    "            'Longitude': intersection['refPoint']['lon'],\n",
    "            'Lane Count': len(intersection.get('laneSet', [])),\n",
    "            'Lane Width (m)': intersection.get('laneWidth')\n",
    "        })\n",
    "\n",
    "df_vcc = pd.DataFrame(vcc_intersections)\n",
    "print(f\"\\nüìç VCC Intersections:\")\n",
    "display(df_vcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Our API - Currently Tracked Intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query our local API\n",
    "LOCAL_API_URL = \"http://localhost:8001/api/v1/safety/index/\"\n",
    "\n",
    "response = requests.get(LOCAL_API_URL)\n",
    "response.raise_for_status()\n",
    "local_data = response.json()\n",
    "\n",
    "print(f\"‚úì Retrieved {len(local_data)} intersections from local API\\n\")\n",
    "\n",
    "# Parse local intersections\n",
    "local_intersections = []\n",
    "for item in local_data:\n",
    "    local_intersections.append({\n",
    "        'Intersection ID': item['intersection_id'],\n",
    "        'Name': item['intersection_name'],\n",
    "        'Safety Index': item['safety_index'],\n",
    "        'Traffic Volume': item['traffic_volume'],\n",
    "        'Latitude': item['latitude'],\n",
    "        'Longitude': item['longitude']\n",
    "    })\n",
    "\n",
    "df_local = pd.DataFrame(local_intersections)\n",
    "print(f\"üìç Local API Intersections:\")\n",
    "display(df_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Parquet Storage - Historical Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parquet storage\n",
    "PARQUET_BASE = Path(\"../backend/data/parquet\")\n",
    "indices_path = PARQUET_BASE / \"indices\"\n",
    "raw_path = PARQUET_BASE / \"raw\"\n",
    "\n",
    "print(f\"üìÅ Parquet Storage Paths:\")\n",
    "print(f\"  Base: {PARQUET_BASE.absolute()}\")\n",
    "print(f\"  Indices: {indices_path.absolute()}\")\n",
    "print(f\"  Raw: {raw_path.absolute()}\")\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"\\nüìä Directory Status:\")\n",
    "print(f\"  Base exists: {PARQUET_BASE.exists()}\")\n",
    "print(f\"  Indices exists: {indices_path.exists()}\")\n",
    "print(f\"  Raw exists: {raw_path.exists()}\")\n",
    "\n",
    "# List indices files\n",
    "if indices_path.exists():\n",
    "    indices_files = sorted(indices_path.glob(\"indices_*.parquet\"))\n",
    "    print(f\"\\nüìÑ Indices Files ({len(indices_files)} total):\")\n",
    "    for f in indices_files[:10]:  # Show first 10\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\")\n",
    "    if len(indices_files) > 10:\n",
    "        print(f\"  ... and {len(indices_files) - 10} more files\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No indices directory found!\")\n",
    "\n",
    "# List raw BSM files\n",
    "if raw_path.exists():\n",
    "    bsm_path = raw_path / \"bsm\"\n",
    "    if bsm_path.exists():\n",
    "        bsm_files = sorted(bsm_path.glob(\"bsm_*.parquet\"))\n",
    "        print(f\"\\nüìÑ BSM Files ({len(bsm_files)} total):\")\n",
    "        for f in bsm_files[:5]:  # Show first 5\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  - {f.name} ({size_mb:.2f} MB)\")\n",
    "        if len(bsm_files) > 5:\n",
    "            print(f\"  ... and {len(bsm_files) - 5} more files\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No raw data directory found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Historical Data (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read most recent indices file if it exists\n",
    "if indices_path.exists():\n",
    "    indices_files = sorted(indices_path.glob(\"indices_*.parquet\"))\n",
    "    if indices_files:\n",
    "        latest_file = indices_files[-1]\n",
    "        print(f\"üìñ Reading latest indices file: {latest_file.name}\\n\")\n",
    "        \n",
    "        df_indices = pd.read_parquet(latest_file)\n",
    "        \n",
    "        print(f\"Shape: {df_indices.shape}\")\n",
    "        print(f\"Columns: {list(df_indices.columns)}\")\n",
    "        print(f\"\\nData types:\")\n",
    "        print(df_indices.dtypes)\n",
    "        \n",
    "        # Check unique intersections\n",
    "        if 'intersection' in df_indices.columns:\n",
    "            unique_intersections = df_indices['intersection'].unique()\n",
    "            print(f\"\\nüîç Unique intersections in data: {len(unique_intersections)}\")\n",
    "            print(f\"  Values: {sorted(unique_intersections)}\")\n",
    "            \n",
    "            # Count records per intersection\n",
    "            intersection_counts = df_indices['intersection'].value_counts()\n",
    "            print(f\"\\nüìà Records per intersection:\")\n",
    "            display(intersection_counts)\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nüìã Sample data (first 5 rows):\")\n",
    "        display(df_indices.head())\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No indices files found in {indices_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Indices directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì° VCC API:\")\n",
    "print(f\"  Available Intersections: {len(df_vcc)}\")\n",
    "print(f\"  Intersection IDs: {sorted(df_vcc['Intersection ID'].tolist())}\")\n",
    "\n",
    "print(f\"\\nüè† Local API:\")\n",
    "print(f\"  Tracked Intersections: {len(df_local)}\")\n",
    "if not df_local.empty:\n",
    "    print(f\"  Intersection IDs: {sorted(df_local['Intersection ID'].tolist())}\")\n",
    "\n",
    "print(f\"\\nüíæ Parquet Storage:\")\n",
    "if indices_path.exists():\n",
    "    indices_files = sorted(indices_path.glob(\"indices_*.parquet\"))\n",
    "    print(f\"  Historical Files: {len(indices_files)}\")\n",
    "    if indices_files:\n",
    "        # Read latest to check intersections\n",
    "        df_latest = pd.read_parquet(indices_files[-1])\n",
    "        if 'intersection' in df_latest.columns:\n",
    "            stored_intersections = df_latest['intersection'].unique()\n",
    "            print(f\"  Stored Intersections: {len(stored_intersections)}\")\n",
    "            print(f\"  Intersection IDs: {sorted(stored_intersections.tolist())}\")\n",
    "else:\n",
    "    print(f\"  Status: No storage directory found\")\n",
    "\n",
    "print(f\"\\nüîç Analysis:\")\n",
    "if len(df_vcc) > len(df_local):\n",
    "    missing_count = len(df_vcc) - len(df_local)\n",
    "    print(f\"  ‚ö†Ô∏è  Missing {missing_count} intersection(s) from local tracking\")\n",
    "    print(f\"  ‚ÑπÔ∏è  VCC has {len(df_vcc)} intersections, but we're only tracking {len(df_local)}\")\n",
    "elif len(df_vcc) == len(df_local):\n",
    "    print(f\"  ‚úì All VCC intersections are being tracked\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Local API has more intersections than VCC!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Data Collector Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json as json_module\n",
    "\n",
    "# Check data collector container logs\n",
    "print(\"üìã Data Collector Logs (last 50 lines):\\n\")\n",
    "result = subprocess.run(\n",
    "    ['docker', 'logs', 'trafficsafety-collector', '--tail', '50'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors:\")\n",
    "    print(result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
