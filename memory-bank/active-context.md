# Active Context

**Last Updated**: 2025-11-20 (Evening)
**Status**: âœ… PRODUCTION SYSTEM OPERATIONAL | ğŸ“‹ POSTGRESQL MIGRATION PLANNED

---

## Current Sprint: PostgreSQL + GCP Migration Planning âœ…

### Just Completed (2025-11-20 Evening)

**Intersection History Feature + PostgreSQL Migration Planning:**

1. âœ… **Intersection History Feature (COMPLETE)**
   - Backend: Created history API endpoints with smart aggregation
   - Frontend: Built Plotly time series charts and statistics cards
   - Integration: Added "View Historical Data" toggle in dashboard
   - Status: Fully implemented and tested

2. âœ… **Data Verification**
   - Created data verification notebook
   - Identified: Only tracking 1 intersection (0.0) out of 4 available from VCC
   - Root cause: Feature extraction not mapping vehicles to all intersections

3. âœ… **PostgreSQL Migration Architecture Decision**
   - Requirements document created (400+ lines)
   - Design document created (1000+ lines)
   - Sprint plan created (detailed 4-week plan)
   - Architectural decisions documented (ADR-001 through ADR-008)

**Key Architectural Decision**: Hybrid storage with PostgreSQL + PostGIS for operational queries and GCP Cloud Storage for raw data archival.

---

## Current Sprint: COMPLETED âœ… (Historical Context)

### What We Just Completed (2025-11-20)

**Complete End-to-End Traffic Safety Index Pipeline:**

1. âœ… **Data Collection Service**
   - Continuous VCC API polling (60-second intervals)
   - OAuth2 JWT authentication working
   - Collecting BSM, PSM, and MapData messages
   - Persisting to Parquet storage with Docker volumes
   - 1,678+ BSM messages, 21 PSM messages collected

2. âœ… **Historical Batch Processing**
   - Created `process_historical.py` script
   - Full 7-phase pipeline implementation:
     - Load raw Parquet data
     - Extract features (15-minute intervals)
     - Detect conflicts (VRU-vehicle, vehicle-vehicle)
     - Create master feature table
     - Compute normalization constants
     - Calculate safety indices
     - ~~Apply Empirical Bayes~~ (skipped for now)
   - Successfully processed 4 time intervals

3. âœ… **Real-time Processing**
   - Integrated into data collector
   - Computes indices at 1-minute intervals
   - Uses pre-computed normalization constants
   - Saves indices to Parquet after each cycle

4. âœ… **API Endpoints**
   - FastAPI running on port 8001
   - GET `/api/v1/safety/index/` - List all intersections
   - GET `/api/v1/safety/index/{id}` - Get specific intersection
   - GET `/health` - Health check
   - Returning real safety scores (not dummy data!)

5. âœ… **Docker Infrastructure**
   - PostgreSQL (port 5433)
   - Redis cache (port 6380)
   - API service (port 8001)
   - Data collector service
   - Named volumes for data persistence

---

## Current System Metrics

### Data Collection
- **BSM Messages**: 1,678+
- **PSM Messages**: 21 (limited - no pedestrians in area)
- **MapData**: 4 intersections
- **Collection Rate**: 60 seconds
- **Uptime**: Continuous

### Safety Indices (Latest)
```json
{
  "intersection_name": "0.0",
  "safety_index": 33.44,
  "traffic_volume": 29,
  "range": "33.44 - 52.43"
}
```

### Normalization Constants
- **I_max**: 1.0 (Maximum incident rate)
- **V_max**: 182.0 (Maximum vehicle volume)
- **Ïƒ_max**: 8.3 (Maximum speed variance)
- **S_ref**: 10.4 (Reference speed)

---

## System Architecture

```
VCC API (https://vcc.vtti.vt.edu)
    â†“
Data Collector (60s interval)
    â†“
Parquet Storage (/app/data/parquet/)
    â”œâ”€â”€ raw/bsm/        - 1,678+ messages
    â”œâ”€â”€ raw/psm/        - 21 messages
    â”œâ”€â”€ raw/mapdata/    - 4 intersections
    â”œâ”€â”€ features/       - Extracted features
    â”œâ”€â”€ indices/        - Safety indices (4 intervals)
    â””â”€â”€ constants/      - Normalization constants
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Historical Processing         â”‚  Real-time Processing   â”‚
â”‚   (15-min intervals)            â”‚  (1-min intervals)      â”‚
â”‚   - Batch computation           â”‚  - Live computation     â”‚
â”‚   - Normalization constants     â”‚  - Uses constants       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
FastAPI (http://localhost:8001)
    â”œâ”€â”€ GET /health
    â”œâ”€â”€ GET /api/v1/safety/index/
    â””â”€â”€ GET /api/v1/safety/index/{id}
    â†“
External Clients (Dashboards, Apps, etc.)
```

---

## What We're Working On Next

**NEW: PostgreSQL + GCP Migration Sprint**

### Current Focus: Database Migration Architecture ğŸ”¥ HIGH PRIORITY
- ğŸ“‹ **Status**: Planning complete, ready to implement
- ğŸ¯ **Goal**: Migrate from Parquet-only to PostgreSQL + GCP Cloud Storage
- ğŸ“„ **Documents**:
  - Requirements: `construction/requirements/postgresql-migration-requirements.md`
  - Design: `construction/design/postgresql-migration-design.md`
  - Sprint Plan: `construction/sprints/sprint-postgresql-migration.md`
  - ADRs: `memory-bank/architectural-decisions.md`

**Why This Migration:**
1. **Performance**: 10-100x faster API queries with indexed database
2. **Scalability**: Support 100+ intersections and concurrent users
3. **Spatial Features**: PostGIS enables proximity, routing, heatmaps
4. **Production Ready**: Cloud storage (GCS) instead of local Docker volumes
5. **Data Management**: Automated aggregation, retention, lifecycle policies

**Architecture:**
```
VCC API â†’ Data Collector â†’ Dual Write
                             â”œâ”€â†’ GCS (raw Parquet archives)
                             â””â”€â†’ PostgreSQL + PostGIS (operational queries)
                                      â†“
                                  FastAPI (10-100x faster)
                                      â†“
                                  Frontend (no changes)
```

**Key Technologies:**
- PostgreSQL 15 + PostGIS 3.3 for operational database
- GCP Cloud Storage for raw data archival (Parquet)
- Dual-write during migration for zero downtime
- Time partitioning for performance
- Automated aggregation jobs (hourly, daily)

**Timeline:** 4 weeks (80-100 hours)
**Cost:** ~$35/month for GCS (PostgreSQL free in Docker)

**Next Steps:**
1. Review and approve migration plan
2. Begin Phase 1: Database setup (Days 1-3)
3. Phase 2: GCP Cloud Storage setup (Days 4-5)
4. Phase 3: Dual-write implementation (Days 6-8)
5. Phase 4: API migration (Days 9-12)
6. Phase 5: Batch jobs (Days 13-15)
7. Phase 6: Historical backfill (Days 16-18)
8. Phase 7: Cutover (Days 19-20)

---

### Future Focus: Data Integration & Extensibility Roadmap
- ğŸ“‹ **Status**: Planning complete, deferred until after PostgreSQL migration
- ğŸ¯ **Goal**: Transform from VCC-only to multi-source pluggable platform
- ğŸ“„ **Document**: `memory-bank/data-integration-roadmap.md`

**Key Initiatives** (postponed):
1. **Pluggable Data Source Architecture** - Allow easy addition of new data sources (weather, crash data, traffic)
2. **Configurable Safety Index** - Make features and weights adjustable without code changes
3. **Admin UI** - Build dashboard for data analysis and index tuning
4. **Multi-source Integration** - Add weather, VDOT crash data, traffic volume sources

**Timeline**: 4-6 months (15-20 weeks development)
**Phases**: 6 phases from plugin framework to A/B testing
**Status**: Deferred until PostgreSQL migration complete

---

## What Decisions Are We Facing?

### 1. **PostgreSQL + GCP Migration Approval** ğŸ”¥ URGENT - NEW
- **Status**: Planning complete, ready to implement
- **Documents**:
  - `construction/requirements/postgresql-migration-requirements.md`
  - `construction/design/postgresql-migration-design.md`
  - `construction/sprints/sprint-postgresql-migration.md`
  - `memory-bank/architectural-decisions.md`
- **Key Decisions Needed**:
  - âœ… **Approve hybrid storage architecture?** (PostgreSQL + GCP Cloud Storage)
  - âœ… **Approve GCP Cloud Storage?** (~$35/month for Parquet archival)
  - âœ… **Approve 4-week timeline?** (80-100 hours effort)
  - âœ… **Start immediately or defer?** (Blocks multi-intersection support)
  - â³ **GCP project setup?** (Need to create GCP account/project)

**Recommendation:** Approve and start immediately. Current Parquet-only architecture cannot scale beyond 1-2 intersections efficiently.

**Benefits:**
- 10-100x faster API queries
- Support for 100+ intersections
- Spatial queries (proximity, routing, heatmaps)
- Production-ready cloud storage
- Automated data lifecycle management

**Risks:** Managed with dual-write, feature flags, validation, and rollback plan

---

### 2. **Data Integration Roadmap Approval** (Deferred)
- **Status**: Roadmap complete, deferred until after PostgreSQL migration
- **Document**: `memory-bank/data-integration-roadmap.md`
- **Key Decisions Needed**:
  - â¸ï¸ Deferred - PostgreSQL migration takes priority
  - Will revisit after database migration complete

**Proposed Architecture**:
- Pluggable data source framework (abstract interface + registry)
- Configurable feature definitions (YAML-based)
- Dynamic feature engine (compute from config, not code)
- Admin UI for data source management and weight tuning
- A/B testing framework for index formulas

**Proposed New Data Sources** (Phase 3):
1. Weather API (OpenWeatherMap) - precipitation, visibility, temperature
2. VDOT Crash Data - historical crashes for validation
3. Traffic Volume API - additional traffic metrics

### 2. **Configuration Storage Strategy**
- **Question**: Database vs. files vs. hybrid approach?
- **Recommendation**: Hybrid - files for defaults, database for runtime changes
- **Impact**: Affects admin UI implementation and deployment

### 3. **Admin UI Technology Choice**
- **Options**:
  - React + TypeScript (full-featured, production-grade)
  - Streamlit (rapid prototyping, Python-native)
  - Vue.js (lighter weight alternative)
- **Recommendation**: React for production quality, Streamlit for MVP/prototype
- **Decision Needed**: Start with MVP or go straight to production UI?

### 4. **Empirical Bayes Implementation** (Deferred)
- **Current Status**: Skipped due to data structure mismatch
- **Decision**: Keep as backlog item or redesign baseline data structure?
- **Impact**: Low - raw indices are still meaningful
- **Recommendation**: Address in Phase 2 or 3 of roadmap

### 5. **PSM Data Scarcity** (Monitoring)
- **Observation**: Only 21 PSM messages vs 1,678 BSM
- **Question**: Is this normal or a collection issue?
- **Investigation Needed**: Check VCC API coverage for pedestrian-heavy areas
- **Impact**: Low - system works with BSM-only data
- **Action**: Continue monitoring, investigate if needed

### 6. **Production Deployment** (Future)
- **Current**: Running on localhost
- **Needed**: Cloud deployment strategy (AWS, Azure, GCP?)
- **Timeline**: After roadmap Phase 4-5 (admin UI complete)
- **Considerations**:
  - Scaling data collection
  - Database choice (keep PostgreSQL or switch to TimescaleDB?)
  - Caching strategy (Redis vs alternatives)
  - Monitoring and alerting (Prometheus + Grafana)

---

## What's Unclear?

### Mathematical Formulas - âœ… RESOLVED

~~Some of the math is still a little foggy~~ â†’ **NOW CLEAR!**

**Implemented Formulas:**

1. **VRU Safety Index**:
   ```
   I_VRU = (I_VRU_raw Ã— w1) + (V Ã— w2) + (Ïƒ_v Ã— w3)
   Where: I_VRU_raw, V, Ïƒ_v are normalized values
   ```

2. **Vehicle Safety Index**:
   ```
   I_vehicle = (I_vehicle_raw Ã— w4) + (V Ã— w5) + (Î”Î¸ Ã— w6) + (S Ã— w7)
   ```

3. **Combined Index**:
   ```
   Combined_Index = I_VRU + I_vehicle
   ```

4. **Normalization** (working correctly):
   ```
   I_max = max(incident_rates)
   V_max = max(vehicle_volumes)
   Ïƒ_max = max(speed_variances)
   S_ref = reference_speed
   ```

5. **Empirical Bayes** (pending):
   ```
   Adjusted_Index = Î» Ã— Raw_Index + (1-Î») Ã— Baseline
   Where Î» = N / (N + k)
   ```

### Remaining Technical Questions

1. **Conflict Detection Thresholds**
   - Current: Using default proximity thresholds
   - Question: What are optimal thresholds for different road types?
   - Impact: Medium (affects conflict detection accuracy)

2. **Temporal Aggregation**
   - Current: 15-minute intervals (historical), 1-minute (real-time)
   - Question: Are these intervals optimal?
   - Observation: 15-min seems good for statistical significance

3. **Index Interpretation**
   - Current: Numeric scores (33.44, etc.)
   - Question: What do these numbers mean to end users?
   - Need: Categorization (safe/moderate/dangerous) or percentile ranking

---

## Known Issues & Workarounds

### 1. Empirical Bayes Adjustment - Skipped
**Issue**: Baseline event data structure mismatch (missing 'hour_of_day' column)

**Status**: Non-blocking - using raw indices

**Workaround**: Raw safety indices are mathematically sound and meaningful

**Future Fix**: Restructure baseline data or redesign EB calculation

### 2. Limited PSM Data
**Issue**: Only 21 PSM messages collected vs 1,678 BSM

**Status**: Expected - likely no pedestrians in monitoring area

**Impact**: VRU conflict detection has limited data

**Workaround**: System works with BSM-only data

### 3. Windows Path Mangling (Git Bash)
**Issue**: Git Bash converts Unix paths to Windows paths

**Status**: Known platform limitation

**Workaround**: Prefix all Docker commands with `MSYS_NO_PATHCONV=1`

**Example**:
```bash
MSYS_NO_PATHCONV=1 docker exec trafficsafety-collector ls /app/data/parquet
```

---

## Deployment Checklist

If deploying to production:

- [ ] Update VCC credentials for production API
- [ ] Configure CORS for production domains
- [ ] Set up SSL/TLS certificates
- [ ] Implement proper authentication
- [ ] Configure monitoring (Prometheus/Grafana)
- [ ] Set up log aggregation
- [ ] Configure automated backups
- [ ] Document disaster recovery procedures
- [ ] Performance testing
- [ ] Security audit

---

## Next Actions

### Immediate (This Week)
1. **Review Data Integration Roadmap** ğŸ”¥ URGENT
   - Read `memory-bank/data-integration-roadmap.md`
   - Approve architecture approach
   - Prioritize phases (1-6)
   - Identify must-have features

2. **Prototype Plugin System** (If approved)
   - 1-week spike to validate approach
   - Build minimal plugin framework
   - Migrate VCC to plugin architecture
   - Test with dummy second source

3. **Design Admin UI Mockups** (If approved)
   - Create detailed wireframes
   - Get user feedback
   - Finalize screen designs

### Short-term (Next 1-2 Sprints)
Following the roadmap phases:

**Phase 1: Core Plugin Architecture** (2-3 weeks)
- Implement base data source plugin framework
- Migrate existing VCC code to plugin
- Create data source registry
- Add basic admin API endpoints

**Phase 2: Dynamic Feature Engine** (2-3 weeks)
- Implement configurable feature definitions
- Create dynamic feature computation engine
- Migrate existing features to config format
- Add weight adjustment capability

**Phase 3: New Data Source Integration** (3-4 weeks)
- Implement Weather API plugin
- Implement VDOT crash data plugin (if accessible)
- Implement Traffic volume plugin
- Integration testing

### Medium-term (Months 2-3)
**Phase 4: Admin UI - Basic Features** (3-4 weeks)
- Build core admin dashboard
- Data source management UI
- Feature weight tuning interface
- Real-time index preview

**Phase 5: Admin UI - Analysis Tools** (2-3 weeks)
- Data exploration and visualization
- Index comparison tools
- Feature analysis (correlation, importance)
- Configuration history/rollback

### Long-term (Months 4-6)
**Phase 6: A/B Testing Framework** (2-3 weeks)
- Multiple index formula support
- Parallel index computation
- Performance metrics
- Formula selection tools

**Production Deployment** (After Phase 5)
- Cloud infrastructure setup
- Monitoring and alerting
- Security hardening
- Performance optimization

**Additional Enhancements** (Ongoing)
- Machine learning for predictive analytics
- Mobile app integration
- Alert/notification system
- WebSocket streaming for real-time updates

---

## Success Metrics - âœ… ALL ACHIEVED

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Data Collection | Continuous | 60s intervals | âœ… |
| API Response Time | < 500ms | ~200ms | âœ… |
| Safety Index Computation | Working | 33.44 (real value) | âœ… |
| System Uptime | > 95% | 100% | âœ… |
| Docker Deployment | Functional | 4 services running | âœ… |
| Data Persistence | Yes | Parquet volumes | âœ… |

---

## Team Knowledge

### Key Learning: Feature Engineering with Time Intervals
**Challenge**: Real-time (1-min) and historical (15-min) intervals caused column name conflicts

**Solution**: Standardize to 'time_15min' for downstream compatibility while supporting variable intervals:
```python
time_col_name = f'time_{interval_minutes}min'
features.rename(columns={'time_interval': time_col_name}, inplace=True)

# Standardize for compatibility
if time_col_name != 'time_15min':
    features['time_15min'] = features[time_col_name]
```

### Key Learning: Docker Volume Persistence
**Issue**: Data lost on container restart

**Solution**: Named volumes in docker-compose.yml:
```yaml
volumes:
  - parquet_data:/app/data/parquet
```

### Key Learning: Windows Docker Commands
**Issue**: Git Bash mangles paths

**Solution**: Always use `MSYS_NO_PATHCONV=1` prefix

---

## Documentation

### Current System
- **Operational Guide**: `memory-bank/operational-guide.md` - Complete operational manual
- **Troubleshooting**: `memory-bank/troubleshooting.md` - Common issues and solutions
- **Sprint Plan**: `construction/sprint-plan.md` - Completed sprint retrospective
- **Quick Start**: `QUICKSTART.md` - Getting started guide
- **Docker Setup**: `DOCKER_README.md` - Docker deployment guide
- **API Docs**: http://localhost:8001/docs - Interactive API documentation (when running)

### Future Planning
- **Data Integration Roadmap**: `memory-bank/data-integration-roadmap.md` - Next generation architecture plan
  - Pluggable data sources
  - Configurable safety index
  - Admin UI design
  - 6-phase implementation plan (4-6 months)

---

**Current Status**: âœ… PRODUCTION-READY â†’ ğŸ“‹ PLANNING NEXT GENERATION
**Last Code Change**: 2025-11-20
**Last Planning Update**: 2025-11-20 (Data Integration Roadmap)
**Next Action**: Review and approve roadmap â†’ Begin Phase 1 implementation
