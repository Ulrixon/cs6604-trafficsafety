# Sprint Plan: PostgreSQL + PostGIS Migration

**Sprint Goal:** Migrate from Parquet-only storage to hybrid PostgreSQL + GCP Cloud Storage architecture

**Duration:** 4 weeks (20 working days)
**Team:** 1-2 developers
**Estimated Effort:** 80-100 hours
**Priority:** High
**Status:** In Progress - Phase 3 Complete

---

## Current Progress

### Completed Phases
- âœ… **Phase 1: Database Setup** (Days 1-3) - All 6 tasks complete
- âœ… **Phase 2: GCP Cloud Storage Setup** (Days 4-5) - All 4 tasks complete
- âœ… **Phase 3: Dual-Write Implementation** (Days 6-8) - All 4 tasks complete

### Current Phase
- ðŸ”„ **Phase 4: API Migration** (Days 9-12) - Not started
  - Requires manual PostgreSQL data validation before proceeding

### Status Notes
- PostgreSQL dual-write implemented and tested
- GCS upload integrated with data collector
- Triple-write architecture: Local Parquet + PostgreSQL + GCS
- All feature flags disabled by default (backward compatible)
- Migration script ready for existing Parquet files
- Comprehensive documentation completed

---

## Sprint Overview

This sprint implements a production-grade data storage architecture with PostgreSQL for operational queries and GCP Cloud Storage for raw data archival. The migration maintains backward compatibility and zero downtime.

### Success Criteria
- [ ] All API endpoints respond â‰¥10x faster
- [ ] Historical data accessible for all past dates
- [x] GCP Cloud Storage integrated with lifecycle policies
- [ ] Automated aggregation jobs running
- [x] Zero data loss during migration (dual-write prevents loss)
- [x] Frontend requires no changes (backward compatible)

---

## Phase 1: Database Setup (Days 1-3, 20 hours) âœ… COMPLETE

### Task 1.1: PostgreSQL Infrastructure Setup âœ…
**Time:** 4 hours
**Status:** Complete
**Files:**
- `docker-compose.yml` (MODIFIED)
- `backend/db/init/01_init_schema.sql` (CREATED)
- `backend/db/init/02_seed_data.sql` (CREATED)

**Steps:**
1. Add PostGIS container to docker-compose.yml
   ```yaml
   db:
     image: postgis/postgis:15-3.3
     environment:
       POSTGRES_DB: trafficsafety
       POSTGRES_USER: postgres
       POSTGRES_PASSWORD: ${DB_PASSWORD}
     volumes:
       - postgres_data:/var/lib/postgresql/data
       - ./backend/db/init:/docker-entrypoint-initdb.d
     ports:
       - "5433:5432"
     healthcheck:
       test: ["CMD-SHELL", "pg_isready -U postgres -d trafficsafety"]
       interval: 10s
       timeout: 5s
       retries: 5
   ```

2. Create `.env` entries:
   ```bash
   DB_PASSWORD=your_secure_password
   DB_HOST=db
   DB_PORT=5432
   DB_NAME=trafficsafety
   DB_USER=api_user
   DB_PASSWORD_API=api_user_password
   ```

3. Start database: `docker-compose up -d db`
4. Verify PostGIS: `docker exec trafficsafety-db psql -U postgres -d trafficsafety -c "SELECT PostGIS_Version();"`

**Testing:**
- [x] Database container healthy
- [x] PostGIS extension loaded
- [x] Can connect from host

---

### Task 1.2: Create Database Schema âœ…
**Time:** 6 hours
**Status:** Complete
**Files:**
- `backend/db/init/01_init_schema.sql` (CREATED - 600+ lines)
- `backend/db/migrations/` (N/A - using init scripts)

**Schema Creation:**
```sql
-- Copy complete schema from design document
-- Includes:
-- - intersections table
-- - safety_indices_realtime (partitioned)
-- - safety_indices_hourly
-- - safety_indices_daily
-- - normalization_constants
-- - data_quality_log
-- - All indexes
-- - All triggers
-- - Partition management functions
```

**Steps:**
1. Implement schema from design document
2. Create initial partitions (7 days)
3. Set up auto-partition creation
4. Add seed data for test intersection

**Testing:**
- [ ] All tables created successfully
- [ ] Spatial indexes exist
- [ ] Can insert test data
- [ ] Partitions created correctly

---

### Task 1.3: Database Connection Layer
**Time:** 4 hours
**Files:**
- `backend/app/db/connection.py` (NEW - 80 lines)
- `backend/app/db/__init__.py` (NEW - 10 lines)
- `backend/requirements.txt` (MODIFY - add SQLAlchemy, psycopg2)

**Implementation:**
```python
# backend/app/db/connection.py
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool
from contextlib import contextmanager
from ..core.config import settings

DATABASE_URL = f"postgresql://{settings.DB_USER}:{settings.DB_PASSWORD}@{settings.DB_HOST}:{settings.DB_PORT}/{settings.DB_NAME}"

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=20,
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=settings.DEBUG
)

@contextmanager
def get_db():
    """Context manager for database connections."""
    conn = engine.connect()
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()
```

**Dependencies:**
```txt
# Add to backend/requirements.txt
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.9
geoalchemy2>=0.14.0  # For PostGIS support
```

**Testing:**
- [ ] Can create connection pool
- [ ] Context manager works
- [ ] Connection recycling works
- [ ] Error handling correct

---

### Task 1.4: Add PgAdmin (Development Tool)
**Time:** 1 hour
**Files:**
- `docker-compose.yml` (MODIFY)

```yaml
pgadmin:
  image: dpage/pgadmin4:latest
  environment:
    PGADMIN_DEFAULT_EMAIL: admin@trafficsafety.local
    PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
  ports:
    - "5050:80"
  networks:
    - trafficsafety-network
  profiles:
    - dev
```

**Testing:**
- [ ] Can access PgAdmin at http://localhost:5050
- [ ] Can connect to database
- [ ] Can browse schema

---

### Task 1.5: Configuration Updates
**Time:** 2 hours
**Files:**
- `backend/app/core/config.py` (MODIFY - add 15 lines)

**Add Settings:**
```python
class Settings(BaseSettings):
    # Existing settings...

    # Database configuration
    DB_HOST: str = Field("db", env="DB_HOST")
    DB_PORT: int = Field(5432, env="DB_PORT")
    DB_NAME: str = Field("trafficsafety", env="DB_NAME")
    DB_USER: str = Field("api_user", env="DB_USER")
    DB_PASSWORD: str = Field("", env="DB_PASSWORD")
    DB_POOL_SIZE: int = Field(10, env="DB_POOL_SIZE")
    DB_MAX_OVERFLOW: int = Field(20, env="DB_MAX_OVERFLOW")

    # Feature flags
    USE_POSTGRESQL: bool = Field(True, env="USE_POSTGRESQL")
    FALLBACK_TO_PARQUET: bool = Field(True, env="FALLBACK_TO_PARQUET")
```

**Testing:**
- [ ] Settings load from environment
- [ ] Defaults work correctly
- [ ] Feature flags toggle properly

---

### Task 1.6: Database Service Layer
**Time:** 3 hours
**Files:**
- `backend/app/services/db_service.py` (NEW - 150 lines)

**Implementation:**
```python
"""Database service layer for intersection queries."""
from datetime import datetime, date
from typing import List, Optional
from ..db.connection import get_db

def get_latest_safety_indices() -> List[dict]:
    """Get latest safety index for all intersections."""
    query = """
        SELECT DISTINCT ON (i.id)
            i.id as intersection_id,
            i.name as intersection_name,
            si.combined_index_eb as safety_index,
            si.traffic_volume,
            i.latitude,
            i.longitude,
            si.timestamp as last_updated
        FROM intersections i
        LEFT JOIN LATERAL (
            SELECT * FROM safety_indices_realtime
            WHERE intersection_id = i.id
            ORDER BY timestamp DESC
            LIMIT 1
        ) si ON true
        ORDER BY i.id, si.timestamp DESC NULLS LAST
    """
    with get_db() as conn:
        result = conn.execute(query)
        return [dict(row) for row in result.mappings()]

def insert_safety_index(data: dict) -> int:
    """Insert safety index record."""
    query = """
        INSERT INTO safety_indices_realtime (
            intersection_id, timestamp, combined_index,
            combined_index_eb, vru_index, vru_index_eb,
            vehicle_index, vehicle_index_eb, traffic_volume,
            vru_count, vehicle_event_count, vru_event_count,
            hour_of_day, day_of_week
        ) VALUES (
            :intersection_id, :timestamp, :combined_index,
            :combined_index_eb, :vru_index, :vru_index_eb,
            :vehicle_index, :vehicle_index_eb, :traffic_volume,
            :vru_count, :vehicle_event_count, :vru_event_count,
            :hour_of_day, :day_of_week
        )
        ON CONFLICT (intersection_id, timestamp) DO UPDATE SET
            combined_index_eb = EXCLUDED.combined_index_eb,
            traffic_volume = EXCLUDED.traffic_volume
        RETURNING id
    """
    with get_db() as conn:
        result = conn.execute(query, data)
        return result.fetchone()[0]
```

**Testing:**
- [ ] Can query latest indices
- [ ] Can insert new records
- [ ] Upsert works correctly
- [ ] Handles null values

---

## Phase 2: GCP Cloud Storage Setup (Days 4-5, 10 hours)

### Task 2.1: GCP Project and Bucket Setup
**Time:** 2 hours
**Manual Steps:**

1. Create GCP project: `trafficsafety-prod`
2. Create Cloud Storage bucket:
   - Name: `trafficsafety-prod-parquet`
   - Location: `us-east4` (multi-region)
   - Storage class: Standard
   - Public access: Prevented
   - Versioning: Enabled

3. Set up lifecycle policy:
```json
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
        "condition": {"age": 30, "matchesPrefix": ["raw/"]}
      },
      {
        "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
        "condition": {"age": 365, "matchesPrefix": ["raw/"]}
      },
      {
        "action": {"type": "Delete"},
        "condition": {"age": 730, "matchesPrefix": ["processed/"]}
      }
    ]
  }
}
```

4. Create service account:
   - Name: `trafficsafety-data-collector`
   - Roles: `Storage Object Creator`, `Storage Object Viewer`
   - Download JSON key â†’ save as `secrets/gcp-service-account.json`

**Testing:**
- [ ] Bucket accessible
- [ ] Lifecycle policy active
- [ ] Service account has correct permissions

---

### Task 2.2: GCS Client Implementation
**Time:** 4 hours
**Files:**
- `backend/app/services/gcs_storage.py` (NEW - 200 lines)
- `backend/requirements.txt` (MODIFY - add google-cloud-storage)

**Implementation:**
```python
"""GCP Cloud Storage service for Parquet files."""
from google.cloud import storage
from pathlib import Path
from datetime import date
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)

class GCSStorage:
    def __init__(self, bucket_name: str):
        self.client = storage.Client()
        self.bucket = self.client.bucket(bucket_name)

    def upload_bsm_batch(self, local_path: Path, target_date: date) -> str:
        """Upload BSM Parquet file to GCS."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        gcs_path = f"raw/bsm/{target_date.year}/{target_date.month:02d}/{target_date.day:02d}/bsm_{target_date}_{timestamp}.parquet"

        blob = self.bucket.blob(gcs_path)
        blob.upload_from_filename(str(local_path))

        logger.info(f"Uploaded BSM to gs://{self.bucket.name}/{gcs_path}")
        return f"gs://{self.bucket.name}/{gcs_path}"

    def list_indices_files(self, start_date: date, end_date: date) -> List[str]:
        """List indices files for date range."""
        prefix = "processed/indices/"
        blobs = self.bucket.list_blobs(prefix=prefix)

        files = []
        for blob in blobs:
            # Parse date from filename
            # Filter by date range
            files.append(blob.name)

        return files

    def download_parquet(self, gcs_path: str, local_path: Path):
        """Download Parquet file from GCS."""
        blob = self.bucket.blob(gcs_path)
        blob.download_to_filename(str(local_path))
```

**Dependencies:**
```txt
# Add to backend/requirements.txt
google-cloud-storage>=2.10.0
```

**Testing:**
- [ ] Can upload files
- [ ] Can list files
- [ ] Can download files
- [ ] Error handling works

---

### Task 2.3: Local-to-GCS Migration Script
**Time:** 3 hours
**Files:**
- `backend/scripts/migrate_parquet_to_gcs.py` (NEW - 150 lines)

**Script:**
```python
"""One-time migration of local Parquet files to GCS."""
from pathlib import Path
from app.services.gcs_storage import GCSStorage
import logging

def migrate_local_to_gcs():
    """Upload all local Parquet files to GCS."""
    gcs = GCSStorage("trafficsafety-prod-parquet")
    local_base = Path("backend/data/parquet")

    # Migrate raw BSM files
    for parquet_file in (local_base / "raw/bsm").rglob("*.parquet"):
        # Extract date from filename
        # Upload to GCS
        # Delete local file (optional)

    # Repeat for PSM, MapData, indices

    print("Migration complete!")

if __name__ == "__main__":
    migrate_local_to_gcs()
```

**Testing:**
- [ ] Dry-run mode works
- [ ] Files uploaded correctly
- [ ] Can resume if interrupted

---

### Task 2.4: Update Configuration
**Time:** 1 hour
**Files:**
- `backend/app/core/config.py` (MODIFY)
- `.env` (MODIFY)
- `docker-compose.yml` (MODIFY - add GCS secret mount)

**Configuration:**
```python
# Add to Settings
GCS_BUCKET: str = Field("trafficsafety-prod-parquet", env="GCS_BUCKET")
GCS_PROJECT_ID: str = Field("trafficsafety-prod", env="GCS_PROJECT_ID")
GOOGLE_APPLICATION_CREDENTIALS: str = Field(
    "/app/secrets/gcp-service-account.json",
    env="GOOGLE_APPLICATION_CREDENTIALS"
)
ENABLE_GCS_UPLOAD: bool = Field(True, env="ENABLE_GCS_UPLOAD")
```

**Docker Compose:**
```yaml
data-collector:
  # ... existing config
  volumes:
    - ./secrets:/app/secrets:ro  # Mount GCS credentials
  environment:
    - GOOGLE_APPLICATION_CREDENTIALS=/app/secrets/gcp-service-account.json
```

**Testing:**
- [ ] GCS credentials loaded
- [ ] Can initialize GCS client
- [ ] Feature flag works

---

## Phase 3: Dual-Write Implementation (Days 6-8, 16 hours)

### Task 3.1: Modify Data Collector for Dual-Write
**Time:** 6 hours
**Files:**
- `data_collector/data_collector.py` (MODIFY - 100 lines changed)
- `data_collector/requirements.txt` (MODIFY)

**Implementation:**
```python
"""Modified data collector with dual-write."""
from app.services.gcs_storage import GCSStorage
from app.services.db_service import insert_safety_index
import logging

logger = logging.getLogger(__name__)

def save_safety_indices(indices_df, target_date):
    """Save to both GCS and PostgreSQL."""

    # 1. Save to GCS (raw archive)
    if settings.ENABLE_GCS_UPLOAD:
        try:
            local_path = f"/tmp/indices_{target_date}.parquet"
            indices_df.to_parquet(local_path)
            gcs = GCSStorage(settings.GCS_BUCKET)
            gcs_path = gcs.upload_indices(Path(local_path), target_date)
            logger.info(f"âœ“ Saved to GCS: {gcs_path}")
        except Exception as e:
            logger.error(f"âœ— GCS upload failed: {e}")
            # Continue - don't block PostgreSQL write

    # 2. Save to PostgreSQL
    if settings.USE_POSTGRESQL:
        try:
            for _, row in indices_df.iterrows():
                data = {
                    'intersection_id': float(row['intersection']),
                    'timestamp': row['time_15min'],
                    'combined_index': row['Combined_Index'],
                    'combined_index_eb': row.get('Combined_Index_EB'),
                    # ... map all columns
                }
                insert_safety_index(data)
            logger.info(f"âœ“ Saved {len(indices_df)} records to PostgreSQL")
        except Exception as e:
            logger.error(f"âœ— PostgreSQL insert failed: {e}")
            # Log but don't crash
```

**Testing:**
- [ ] Dual-write succeeds
- [ ] GCS failure doesn't block DB write
- [ ] DB failure doesn't block GCS write
- [ ] Error logging works

---

### Task 3.2: Add Validation Layer
**Time:** 4 hours
**Files:**
- `backend/scripts/validate_dual_write.py` (NEW - 200 lines)

**Validation Script:**
```python
"""Validate data consistency between GCS and PostgreSQL."""
def validate_recent_data():
    """Compare last hour of data."""

    # 1. Load from PostgreSQL
    with get_db() as conn:
        db_count = conn.execute("""
            SELECT COUNT(*) FROM safety_indices_realtime
            WHERE timestamp > NOW() - INTERVAL '1 hour'
        """).scalar()

    # 2. Load from GCS (download latest file)
    gcs = GCSStorage(settings.GCS_BUCKET)
    # ... download and read Parquet

    # 3. Compare row counts
    assert gcs_count == db_count, f"Mismatch: GCS={gcs_count}, DB={db_count}"

    # 4. Spot-check random samples
    # ... compare actual values

    print("âœ“ Validation passed!")
```

**Testing:**
- [ ] Detects missing data
- [ ] Detects value mismatches
- [ ] Handles timezone correctly

---

### Task 3.3: Monitoring and Alerts
**Time:** 3 hours
**Files:**
- `backend/app/services/metrics.py` (NEW - 100 lines)
- `docker-compose.yml` (MODIFY - add Prometheus exporter)

**Metrics:**
```python
from prometheus_client import Counter, Histogram, Gauge

gcs_upload_success = Counter('gcs_upload_success_total', 'GCS uploads succeeded')
gcs_upload_failure = Counter('gcs_upload_failure_total', 'GCS uploads failed')
db_write_success = Counter('db_write_success_total', 'DB writes succeeded')
db_write_failure = Counter('db_write_failure_total', 'DB writes failed')
db_write_duration = Histogram('db_write_duration_seconds', 'DB write latency')
```

**Testing:**
- [ ] Metrics exported on :9090
- [ ] Can scrape with Prometheus
- [ ] Grafana dashboard works

---

### Task 3.4: Run Dual-Write for 48 Hours
**Time:** 3 hours (monitoring)
**Validation:**

Run these checks every 6 hours:
```bash
# Check GCS uploads
gsutil ls -l gs://trafficsafety-prod-parquet/raw/bsm/$(date +%Y/%m/%d)/ | wc -l

# Check PostgreSQL inserts
docker exec trafficsafety-db psql -U postgres -d trafficsafety -c "
SELECT COUNT(*) FROM safety_indices_realtime
WHERE timestamp > NOW() - INTERVAL '6 hours';
"

# Run validation script
python backend/scripts/validate_dual_write.py
```

**Testing:**
- [ ] 48 hours of stable operation
- [ ] No data loss detected
- [ ] Row counts match
- [ ] Sample values match

---

## Phase 4: API Migration (Days 9-12, 20 hours)

### Task 4.1: Migrate Intersection Service
**Time:** 4 hours
**Files:**
- `backend/app/services/intersection_service.py` (MODIFY - 80 lines changed)

**Before:**
```python
def get_all():
    df = parquet_storage.load_latest_indices()
    return [IntersectionRead(**row) for row in df.to_dict('records')]
```

**After:**
```python
def get_all():
    if settings.USE_POSTGRESQL:
        try:
            return db_service.get_latest_safety_indices()
        except Exception as e:
            logger.error(f"PostgreSQL query failed: {e}")
            if settings.FALLBACK_TO_PARQUET:
                logger.info("Falling back to Parquet")
                df = parquet_storage.load_latest_indices()
                return [IntersectionRead(**row) for row in df.to_dict('records')]
            raise
    else:
        # Legacy Parquet path
        df = parquet_storage.load_latest_indices()
        return [IntersectionRead(**row) for row in df.to_dict('records')]
```

**Testing:**
- [ ] PostgreSQL path works
- [ ] Fallback works
- [ ] Feature flag toggles correctly
- [ ] Response format unchanged

---

### Task 4.2: Migrate History Service
**Time:** 6 hours
**Files:**
- `backend/app/services/history_service.py` (MODIFY - 150 lines changed)

**Smart Query Routing:**
```python
def get_intersection_history(
    intersection_id: str,
    start_date: date,
    end_date: date,
    aggregation: Optional[str] = None
) -> IntersectionHistory:
    if not settings.USE_POSTGRESQL:
        # Legacy Parquet implementation
        return _get_history_from_parquet(...)

    # Determine which table to query
    days = (end_date - start_date).days

    if days <= 2:
        # Query realtime table
        query = """
            SELECT timestamp, combined_index_eb as safety_index,
                   vru_index_eb as vru_index, vehicle_index_eb as vehicle_index,
                   traffic_volume, hour_of_day, day_of_week
            FROM safety_indices_realtime
            WHERE intersection_id = :int_id
              AND timestamp BETWEEN :start AND :end
            ORDER BY timestamp
        """
        table = "realtime"
    elif days <= 90:
        # Query hourly table
        query = """
            SELECT hour_timestamp as timestamp, avg_si as safety_index,
                   NULL as vru_index, NULL as vehicle_index,
                   total_volume as traffic_volume,
                   EXTRACT(HOUR FROM hour_timestamp) as hour_of_day,
                   EXTRACT(DOW FROM hour_timestamp) as day_of_week
            FROM safety_indices_hourly
            WHERE intersection_id = :int_id
              AND hour_timestamp BETWEEN :start AND :end
            ORDER BY hour_timestamp
        """
        table = "hourly"
    else:
        # Query daily table
        query = """
            SELECT date as timestamp, avg_si as safety_index,
                   NULL as vru_index, NULL as vehicle_index,
                   total_volume as traffic_volume,
                   peak_hour as hour_of_day,
                   EXTRACT(DOW FROM date) as day_of_week
            FROM safety_indices_daily
            WHERE intersection_id = :int_id
              AND date BETWEEN :start AND :end
            ORDER BY date
        """
        table = "daily"

    with get_db() as conn:
        result = conn.execute(query, {
            'int_id': float(intersection_id),
            'start': start_date,
            'end': end_date
        })
        data_points = [IntersectionHistoryPoint(**row) for row in result.mappings()]

    return IntersectionHistory(
        intersection_id=intersection_id,
        data_points=data_points,
        # ... rest of response
    )
```

**Testing:**
- [ ] Realtime queries work (<2 days)
- [ ] Hourly queries work (2-90 days)
- [ ] Daily queries work (>90 days)
- [ ] Response format unchanged

---

### Task 4.3: Implement Spatial Endpoints
**Time:** 6 hours
**Files:**
- `backend/app/api/spatial.py` (NEW - 200 lines)
- `backend/app/main.py` (MODIFY - include router)

**Spatial Queries:**
```python
from fastapi import APIRouter, Query
from typing import List, Tuple

router = APIRouter(prefix="/api/v1/intersections", tags=["Spatial"])

@router.get("/nearby")
def get_nearby(
    lat: float = Query(..., ge=-90, le=90),
    lon: float = Query(..., ge=-180, le=180),
    radius: int = Query(5000, ge=100, le=50000, description="Radius in meters")
):
    """Find intersections within radius."""
    query = """
        SELECT id, name, latitude, longitude,
               ST_Distance(
                   geometry::geography,
                   ST_SetSRID(ST_MakePoint(:lon, :lat), 4326)::geography
               ) as distance_meters
        FROM intersections
        WHERE ST_DWithin(
            geometry::geography,
            ST_SetSRID(ST_MakePoint(:lon, :lat), 4326)::geography,
            :radius
        )
        ORDER BY distance_meters
    """
    with get_db() as conn:
        result = conn.execute(query, {'lat': lat, 'lon': lon, 'radius': radius})
        return [dict(row) for row in result.mappings()]
```

**Testing:**
- [ ] Nearby query works
- [ ] Route query works
- [ ] Area query works
- [ ] Spatial math correct

---

### Task 4.4: Performance Benchmarking
**Time:** 4 hours
**Files:**
- `backend/tests/performance/benchmark_queries.py` (NEW - 150 lines)

**Benchmark:**
```python
import time
from statistics import mean, median

def benchmark_get_all():
    """Compare Parquet vs PostgreSQL for get_all()."""

    # Warm up caches
    for _ in range(10):
        get_all()

    # Benchmark Parquet
    parquet_times = []
    for _ in range(100):
        start = time.time()
        get_all()  # with USE_POSTGRESQL=False
        parquet_times.append(time.time() - start)

    # Benchmark PostgreSQL
    postgres_times = []
    for _ in range(100):
        start = time.time()
        get_all()  # with USE_POSTGRESQL=True
        postgres_times.append(time.time() - start)

    print(f"Parquet - Mean: {mean(parquet_times)*1000:.2f}ms, Median: {median(parquet_times)*1000:.2f}ms")
    print(f"PostgreSQL - Mean: {mean(postgres_times)*1000:.2f}ms, Median: {median(postgres_times)*1000:.2f}ms")
    print(f"Speedup: {mean(parquet_times) / mean(postgres_times):.1f}x")
```

**Testing:**
- [ ] PostgreSQL â‰¥10x faster for get_all
- [ ] PostgreSQL â‰¥5x faster for history queries
- [ ] p95 latency acceptable

---

## Phase 5: Batch Jobs (Days 13-15, 16 hours)

### Task 5.1: Hourly Aggregation Job
**Time:** 5 hours
**Files:**
- `backend/jobs/hourly_aggregation.py` (NEW - 150 lines)
- `backend/jobs/crontab` (NEW)

**Job Implementation:**
```python
"""Hourly aggregation job."""
from datetime import datetime, timedelta
from app.db.connection import get_db
import logging

logger = logging.getLogger(__name__)

def aggregate_last_hour():
    """Aggregate safety indices for the previous hour."""
    current_hour = datetime.now().replace(minute=0, second=0, microsecond=0)
    prev_hour = current_hour - timedelta(hours=1)

    query = """
        INSERT INTO safety_indices_hourly (
            intersection_id, hour_timestamp,
            avg_si, min_si, max_si, std_si,
            total_volume, avg_volume, high_risk_minutes,
            data_quality_score
        )
        SELECT
            intersection_id,
            :hour_timestamp,
            AVG(combined_index_eb) as avg_si,
            MIN(combined_index_eb) as min_si,
            MAX(combined_index_eb) as max_si,
            STDDEV(combined_index_eb) as std_si,
            SUM(traffic_volume) as total_volume,
            AVG(traffic_volume) as avg_volume,
            COUNT(*) FILTER (WHERE combined_index_eb > 75) as high_risk_minutes,
            COUNT(*) / 60.0 as data_quality_score
        FROM safety_indices_realtime
        WHERE timestamp >= :start_time
          AND timestamp < :end_time
          AND combined_index_eb IS NOT NULL
        GROUP BY intersection_id
        ON CONFLICT (intersection_id, hour_timestamp) DO UPDATE SET
            avg_si = EXCLUDED.avg_si,
            min_si = EXCLUDED.min_si,
            max_si = EXCLUDED.max_si
    """

    with get_db() as conn:
        result = conn.execute(query, {
            'hour_timestamp': prev_hour,
            'start_time': prev_hour,
            'end_time': current_hour
        })
        rows_inserted = result.rowcount

    logger.info(f"âœ“ Aggregated {rows_inserted} hourly records for {prev_hour}")
    return rows_inserted

if __name__ == "__main__":
    aggregate_last_hour()
```

**Crontab:**
```cron
# Run at :05 past every hour
5 * * * * cd /app && python -m jobs.hourly_aggregation >> /var/log/hourly_agg.log 2>&1
```

**Testing:**
- [ ] Aggregation math correct
- [ ] Handles missing data
- [ ] Idempotent (can re-run)
- [ ] Logs success/failure

---

### Task 5.2: Daily Aggregation Job
**Time:** 4 hours
**Files:**
- `backend/jobs/daily_aggregation.py` (NEW - 120 lines)

Similar to hourly, but:
- Aggregates from hourly table
- Computes peak_hour and peak_si
- Runs at 00:05 daily

**Testing:**
- [ ] Aggregation correct
- [ ] Peak detection works
- [ ] Handles incomplete days

---

### Task 5.3: Retention Cleanup Job
**Time:** 3 hours
**Files:**
- `backend/jobs/retention_cleanup.py` (NEW - 100 lines)

**Implementation:**
```python
"""Data retention cleanup job."""
from datetime import datetime, timedelta

def cleanup_old_data():
    """Delete data beyond retention periods."""

    # Delete realtime data >48 hours
    cutoff_realtime = datetime.now() - timedelta(hours=48)

    # Drop old partitions
    with get_db() as conn:
        conn.execute("SELECT drop_old_realtime_partitions()")

    # Delete hourly data >90 days
    cutoff_hourly = datetime.now() - timedelta(days=90)
    query = """
        DELETE FROM safety_indices_hourly
        WHERE hour_timestamp < :cutoff
    """
    with get_db() as conn:
        result = conn.execute(query, {'cutoff': cutoff_hourly})
        logger.info(f"Deleted {result.rowcount} hourly records")

    # Vacuum tables
    with get_db() as conn:
        conn.execute("VACUUM ANALYZE safety_indices_realtime")
        conn.execute("VACUUM ANALYZE safety_indices_hourly")
```

**Testing:**
- [ ] Deletes old data correctly
- [ ] Vacuum succeeds
- [ ] Doesn't delete recent data

---

### Task 5.4: Deploy Batch Jobs
**Time:** 4 hours
**Files:**
- `docker-compose.yml` (MODIFY - add cron container)

**Cron Container:**
```yaml
batch-jobs:
  build:
    context: ./backend
  command: cron -f
  volumes:
    - ./backend:/app
    - ./backend/jobs/crontab:/etc/cron.d/trafficsafety
  environment:
    - DATABASE_URL=${DATABASE_URL}
  depends_on:
    - db
  networks:
    - trafficsafety-network
```

**Testing:**
- [ ] Jobs run on schedule
- [ ] Logs accessible
- [ ] Can manually trigger

---

## Phase 6: Historical Backfill (Days 16-18, 14 hours)

### Task 6.1: Backfill Script
**Time:** 6 hours
**Files:**
- `backend/scripts/backfill_historical_data.py` (NEW - 300 lines)

**Implementation:**
```python
"""Backfill historical Parquet data to PostgreSQL."""
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
from app.db.connection import get_db

def backfill_date_range(start_date, end_date):
    """Backfill Parquet data for date range."""

    current = start_date
    while current <= end_date:
        print(f"Processing {current}...")

        # 1. Download from GCS (or read local)
        gcs_path = f"processed/indices/indices_{current}.parquet"
        df = download_and_read_parquet(gcs_path)

        # 2. Bulk insert to PostgreSQL
        records = []
        for _, row in df.iterrows():
            records.append({
                'intersection_id': float(row['intersection']),
                'timestamp': row['time_15min'],
                'combined_index': row['Combined_Index'],
                # ... map all columns
            })

        # 3. Use COPY for fast bulk insert
        with get_db() as conn:
            conn.execute("""
                COPY safety_indices_realtime FROM STDIN WITH (FORMAT CSV)
            """, records)

        print(f"âœ“ Inserted {len(records)} records for {current}")
        current += timedelta(days=1)

if __name__ == "__main__":
    backfill_date_range(date(2025, 11, 1), date(2025, 11, 20))
```

**Testing:**
- [ ] Can backfill single day
- [ ] Can resume if interrupted
- [ ] Row counts match source
- [ ] No duplicate key errors

---

### Task 6.2: Create Aggregates from Backfill
**Time:** 4 hours

Run aggregation jobs for all historical dates:
```python
for date in date_range:
    # Run hourly aggregation for each hour
    for hour in range(24):
        aggregate_hour(date, hour)

    # Run daily aggregation
    aggregate_day(date)
```

**Testing:**
- [ ] Hourly aggregates created
- [ ] Daily aggregates created
- [ ] Statistics match

---

### Task 6.3: Validation Report
**Time:** 4 hours
**Files:**
- `backend/scripts/validation_report.py` (NEW - 200 lines)

**Validation Checks:**
```python
def generate_validation_report():
    """Compare Parquet vs PostgreSQL data."""

    report = {
        'total_records_parquet': 0,
        'total_records_postgres': 0,
        'missing_dates': [],
        'mismatched_stats': []
    }

    # 1. Count total records
    # 2. Check for missing dates
    # 3. Compare statistics per intersection
    # 4. Spot-check random samples

    # Generate HTML report
    with open('validation_report.html', 'w') as f:
        f.write(format_report(report))
```

**Testing:**
- [ ] Report shows no data loss
- [ ] Statistics within acceptable tolerance
- [ ] Missing data identified

---

## Phase 7: Cutover and Monitoring (Days 19-20, 10 hours)

### Task 7.1: Gradual Rollout
**Time:** 4 hours

**Rollout Steps:**
1. Day 19 AM: Enable PostgreSQL for 10% of requests (feature flag)
2. Day 19 PM: Increase to 50% if no errors
3. Day 20 AM: Increase to 100%
4. Day 20 PM: Disable Parquet fallback

**Monitoring:**
```bash
# Watch API latency
watch -n 5 'curl -s http://localhost:8001/api/v1/safety/index/ -w "\nTime: %{time_total}s\n"'

# Watch database connections
docker exec trafficsafety-db psql -U postgres -d trafficsafety -c "
SELECT count(*) as connections, state FROM pg_stat_activity GROUP BY state;
"

# Watch error logs
docker logs -f trafficsafety-api | grep ERROR
```

**Testing:**
- [ ] Error rate acceptable
- [ ] Latency improved
- [ ] No data inconsistencies

---

### Task 7.2: Performance Tuning
**Time:** 3 hours

Based on production metrics:
1. Tune database configuration
2. Add missing indexes
3. Optimize slow queries
4. Adjust connection pool

**Testing:**
- [ ] p95 latency <2s
- [ ] Database CPU <70%
- [ ] Connection pool not exhausted

---

### Task 7.3: Documentation Update
**Time:** 3 hours
**Files:**
- `docs/architecture.md` (UPDATE)
- `docs/operations.md` (NEW)
- `README.md` (UPDATE)

**Documentation:**
- Architecture diagrams
- Database schema
- Deployment guide
- Operations runbook
- Troubleshooting guide

**Testing:**
- [ ] Team can follow docs
- [ ] All commands work
- [ ] Diagrams accurate

---

## Risk Management

### High-Priority Risks

| Risk | Mitigation | Owner |
|------|------------|-------|
| Data loss during migration | Dual-write + validation | Dev Team |
| PostgreSQL slower than expected | Benchmarking before cutover | Dev Lead |
| GCS costs exceed budget | Lifecycle policies + monitoring | DevOps |
| Downtime during cutover | Feature flags + rollback plan | Dev Lead |

### Rollback Procedure

If critical issues detected:
1. Set `USE_POSTGRESQL=false` in config
2. Restart API containers
3. System reverts to Parquet queries
4. PostgreSQL writes continue (don't stop data collection)
5. Debug offline, retry cutover when fixed

---

## Success Metrics

### Performance Targets
- [ ] API latency: <2s p95 (target: <500ms)
- [ ] Database query time: <100ms p95
- [ ] Write throughput: >100 rows/second

### Reliability Targets
- [ ] Zero data loss during migration
- [ ] Uptime: >99% during sprint
- [ ] Error rate: <0.1%

### Business Targets
- [ ] All 4 VCC intersections tracked
- [ ] Historical data accessible
- [ ] Spatial queries functional

---

## Post-Sprint Retrospective

### Questions to Answer
1. What went well?
2. What could be improved?
3. Were time estimates accurate?
4. Any technical debt created?
5. What did we learn?

### Follow-Up Tasks
- [ ] Optimize slow queries
- [ ] Add more monitoring
- [ ] Document lessons learned
- [ ] Plan next features (ML models, advanced analytics)

---

**End of Sprint Plan**
